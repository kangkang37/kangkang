<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>GPT2模型原理的通俗理解</title>
    <link href="/2024/07/10/gpt01/"/>
    <url>/2024/07/10/gpt01/</url>
    
    <content type="html"><![CDATA[<h1 id="GPT2模型原理的通俗理解"><a href="#GPT2模型原理的通俗理解" class="headerlink" title="GPT2模型原理的通俗理解"></a>GPT2模型原理的通俗理解</h1><p>GPT2和只带有解码器decoder的transformer模型很像. 它有超大规模,是一个在海量数据集上基于transformer解码器训练的模型.  BERT模型则是通过transformer编码器模块构建的.  </p><p>通俗而言,gpt2就是根据现有句子,预测下一个单词会是什么. 它像传统的语言模型一样, 一次只输出一个单词token, 每次产生新单词后,该单词会被添加在之前生成的单词序列之后, 这个序列会成为模型下一步的新输入. 这种机制叫做<u>自回归(auto-regression)</u>, 也是令RNN效果超群的重要思想.  BERT没有使用自回归机制, BERT获得了结合单词前后的上下文信息的能力, 也能取得很好的效果.  </p><h2 id="浅析内部结构"><a href="#浅析内部结构" class="headerlink" title="浅析内部结构"></a>浅析内部结构</h2><p>下面看GPT2的内部结构, 它是如何工作的:<br>GPT-2最长可以处理1024个token的序列,每个token都会和它的前续路径一起‘流过’所有的decoder模块.<br>假设现在gpt2已经训练好了, 只需要提供一个预先定义好的起始单词(训练好的模型会使用endoftext作为起始单词,不妨称其为s), 然后让它自己生成文字.<br><img src="/2024/07/10/gpt01/gpt0101.png" alt="输入一个起始单词s让其自己生成下一个单词"><br>此时, 模型的输入只有一个单词, 经过层层处理, 最终得到一个输出向量. 向量对词汇表中每个单词计算一个概率, 选择概率最高的单词作为输出单词, 假设‘The’作为输出.<br>但这样有一个问题: 它可能会陷入推荐同一个单词的循环中, 比如一直重复输出概率最高的单词‘The’, 只有选择概率第2或者第3的单词才能跳出这个循环.  所以, gpt2会有一个叫做‘top-k’的参数, 模型从概率前k的单词中随机抽取下一个单词.<br><img src="/2024/07/10/gpt01/gpt0102.png" alt="生成单词The, 继续输入The预测下一个单词"></p><h2 id="深入理解内部原理"><a href="#深入理解内部原理" class="headerlink" title="深入理解内部原理"></a>深入理解内部原理</h2><h3 id="嵌入矩阵和位置编码"><a href="#嵌入矩阵和位置编码" class="headerlink" title="嵌入矩阵和位置编码"></a>嵌入矩阵和位置编码</h3><p>模型首先输入一个句子, 句子先tokenizer分词(以subword为基本离散单位)为一个个子词subword, 每个子词用词表中的index表示, 词表大小为50257.<br>然后通过嵌入矩阵, 找到每个子词对应的词向量. 嵌入矩阵的行数=词表长度50257, 嵌入矩阵的列数=embedding size, 即每个子词对应的词向量的长度. 最小的gpt-small模型会有1*768的长度表示一个子词.<br><img src="/2024/07/10/gpt01/gpt0103.png" alt="嵌入矩阵的结构, 各版本gpt对应的词向量长度"><br>嵌入矩阵的每一行都是一个词嵌入向量. 最开始, 需要从嵌入矩阵中找到起始单词s对应的词向量. 然后引入位置编码, 位置编码矩阵如下, 行数表示输入的context的长度(即一个句子分解为子词的个数), 列数同样为embedding size:<br><img src="/2024/07/10/gpt01/gpt0104.png" alt="位置编码矩阵的结构"></p><p>输入的单词经过嵌入矩阵和位置编码后, 进入transformer之前的步骤就完成了. 然后经过第一个transformer-decoder模块:<br>首先是自注意力层 masked self-attention, 然后传输给feed forward neural network.  第一个decoder模块完成后, 会把结果向量传给第二个decoder模块, 每个decoder模块的处理方式都是一样的, 但每个模块会维护自己的自注意力层和feed forward层的权重.  </p><h3 id="自注意力机制"><a href="#自注意力机制" class="headerlink" title="自注意力机制"></a>自注意力机制</h3><p>从之前的attention帖子<a href="/2024/07/05/attention/" title="详细理解attention的原理">详细理解attention的原理</a>, 我们了解到自注意力机制的公式: </p><script type="math/tex; mode=display">attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>自注意力机制就是: 对序列中每个单词, 都赋予一个相关度得分$softmax(\frac{QK^T}{\sqrt{d_k}})$, 然后对他们的向量表征$V$乘以这个相关度得分, 即对向量表征加权求和, 所加的这个权重是相关度得分.  </p><p>自注意力机制中的Query, Key和Value的含义: </p><ul><li>Query查询向量: 当前单词的查询向量被用来和其它单词的键向量相乘，从而得到其它词相对于当前词的注意力得分。只关心目前正在处理的单词的查询向量。</li><li>Key键向量: 键向量就像是序列中每个单词的标签，它使搜索相关单词时用来匹配的对象。</li><li>Value值向量: 值向量是单词真正的表征，当算出注意力得分后，使用值向量进行加权求和得到能代表当前位置上下文的向量。</li></ul><p><img src="/2024/07/10/gpt01/gpt0105.png" alt="自注意力机制比喻: 在档案柜中找文件"><br>一个简单粗暴的比喻是在档案柜中找文件。查询向量就像一张便利贴，上面写着你正在研究的课题。键向量像是档案柜中文件夹上贴的标签。当你找到和便利贴上所写相匹配的文件夹时，拿出它，文件夹里的东西便是值向量。</p><p>将单词的查询向量分别乘以每个文件夹的键向量，得到各个文件夹对应的注意力得分（这里的乘指的是向量点乘，乘积会通过 softmax 函数处理）。</p><p>将每个文件夹的值向量乘以其对应的注意力得分，然后求和，得到最终自注意力层的输出。<br><img src="/2024/07/10/gpt01/gpt0106.png" alt="值向量的加权求和, 权重是注意力得分"></p><h3 id="模型输出"><a href="#模型输出" class="headerlink" title="模型输出"></a>模型输出</h3><p>向量经过自注意力机制和feed forward层后,得到输出向量, 这个向量和嵌入矩阵相乘, 得到词汇表中每个单词的注意力得分. 因为嵌入矩阵$M$大小为$50257 \times 768$, 输出向量$X$的大小: $1\times 768$, $size (M \times X^T) = 50257 \times 1$.<br>选取得分最高的单词为输出结果(top-k=1时). 但更好的策略是选择得分较高的一部分单词,将他们的得分作为概率从整个单词表中抽样(得分高的单词更容易选中). 一般的, top-k=40.<br><img src="/2024/07/10/gpt01/gpt0107.png" alt="输出向量和嵌入矩阵相乘得到每个单词概率"><br>以上, 模型完成了一次迭代. 模型不断迭代直到生成一个完成序列, 序列得到1024或者生成终止符. </p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>BERT模型的原理结构<br>代码中的gpt2</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>中文博客园: <a href="https://www.cnblogs.com/zhongzhaoxie/p/13064404.html">GPT-2通俗理解</a><br>Jay Alammar博客: <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p>]]></content>
    
    
    <categories>
      
      <category>ML常见模型</category>
      
      <category>GPT</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GPT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HMA论文-存内计算方向</title>
    <link href="/2024/07/10/papers-paper-pim/"/>
    <url>/2024/07/10/papers-paper-pim/</url>
    
    <content type="html"><![CDATA[<h1 id="HMA论文"><a href="#HMA论文" class="headerlink" title="HMA论文"></a>HMA论文</h1><p>今天我们看这篇论文: Heterogeneous Memory Architecture Accommodating Processing-In-Memory on SoC For AIoT Applications. 链接: <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9712544">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9712544</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>由于其低计算延迟、大吞吐量和高能效等吸引人的特点，存内计算（PIM）技术是人工智能物联网（AIoT）应用最有前途的候选技术之一。然而，如何高效地将PIM与片上系统（SoC）架构结合使用却很少被讨论。在本文中，我们展示了一系列从硬件架构到算法的解决方案，以最大化PIM设计的优势。首先，我们提出了一种异构内存架构（HMA），通过高吞吐量的片上总线将现有SoC与PIM连接起来。然后，基于给定的HMA结构，我们还提出了一种HMA张量映射方法，用于划分张量并在PIM结构上部署通用矩阵乘法操作。HMA硬件和HMA张量映射方法都利用了成熟嵌入式CPU解决方案堆栈的可编程性，并最大化PIM技术的高效性。与最新的加速器解决方案相比，整个HMA系统可以节省416倍的功耗以及44.6%的设计面积。评估还表明，与最新基线和未经优化的PIM相比，我们的设计可以分别将TinyML应用的操作延迟减少430倍和11倍。<br>关键词—内存中处理，内存中计算，片上系统，可编程架构，硬件/软件接口<br><img src="/2024/07/10/papers-paper-pim/pim01.png" alt="图1: 传统SoC具有可编程机器学习加速器和通用CPU"></p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>人工智能和物联网（AIoT）的结合催生了各种特定领域的加速器，以实现低功耗的设备端智能[1]。AIoT设备在传感器附近进行本地数据分析，通常在10mW以下的电池或能量收集电源下运行。因此，AIoT技术需要包含通用CPU和可编程机器学习（ML）加速器的低成本小型片上系统（SoC），以兼顾可编程性和执行效率（图1）。本文旨在通过内存中处理（PIM）技术来增强这些嵌入式SoC，处理大量的通用矩阵乘法（GEMM）操作。</p><p>PIM设计旨在直接在内存阵列中进行计算操作。不同计算接口电路的PIM设计已经被展示[2], [3], [4], [5], [6]。此外，程序化架构也被探索过[7], [8], [9]。尽管在电路级别的PIM设计已经得到了很好的探索，如何将PIM接口引入SoC仍然不清晰。在上述最先进的PIM硬件设计中，PIM被视为一个变异的向量处理单元，形成加速器，即PIM与定制指令集架构（ISA）集成[7], [10]。然而，在SoC级别，由于需要额外的定制指令和与主处理器共享内存的分配，这种架构可能效率不高，导致编译问题复杂。因此，大多数情况下很难达到峰值效率。</p><p>为了解决这个问题，我们提出了一种异构内存架构（HMA），以适应现成的嵌入式SoC架构中的PIM。“异构内存”的概念在此被提出，表示这种架构在一个SoC上既包含PIM内存，也包含传统内存（只能存储和读取数据的内存）。在提出的HMA中，PIM内存与传统内存一样连接到片上的高吞吐量系统总线。HMA极大地简化了程序接口，我们还开发了HMA张量映射方法，作为将GEMM部署到提议架构的软硬件优化。</p><p>本文的主要贡献包括：</p><ul><li>我们提出了一种新颖的异构内存概念和架构，以PIM内存扩展内存空间。据我们所知，提出的HMA方案是第一个明确说明如何将PIM接口引入现成SoC的架构。</li><li>我们设计了HMA张量映射方法来划分张量并将GEMM任务部署到提议的硬件架构中。这种方法不仅为程序员提供了一种与硬件无关的方式来利用PIM硬件，还可以用作预设计规范估计。</li><li>我们创建了一个软硬件开发流程，使PIM能够使用现成的gcc编译器，弥合了硬件和软件工具链之间的差距，以在SoC中开发PIM。</li></ul><p>本文重点介绍了利用嵌入式非易失性存储技术制造的PIM，以利用其零待机功耗特性。我们对提出的HMA的评估基于电阻式随机存取存储器（RRAM）。</p><p>本文其余部分组织如下：第二节介绍PIM的背景。我们在第三节和第四节介绍了提出的架构和算法。评估结果在第五节展示，第六节对本文进行了总结。</p><h2 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2 Preliminaries"></a>2 Preliminaries</h2><h3 id="2-A-ASIC-Accelerators-in-SoC"><a href="#2-A-ASIC-Accelerators-in-SoC" class="headerlink" title="2.A ASIC Accelerators in SoC"></a>2.A ASIC Accelerators in SoC</h3><p>图1显示了在SoC中将应用专用集成电路（ASIC）加速器接口的现有方案。ASIC加速器是为特定任务设计的，具有应用特定的优化硬件实现。ASIC加速器或协处理器通常与主处理器并行工作，并与它们共享总线和内存。完全定制的ISA或扩展到公共标准（例如RISC-V）的ISA用作软硬件程序接口。存储指令的内存（“指令内存”）通常通过连接到系统总线从外部加载到加速器。在这样的SoC中，片上总线（例如AXI, AHB等[11]）不仅负责数据通信，还传输加速器执行的指令。这种内存共享和在同一总线上混合传输加速器指令的模式使得编译和调度变得非常复杂，进而引发设计工作量和开销。</p><h3 id="2-B-Processing-In-Memory"><a href="#2-B-Processing-In-Memory" class="headerlink" title="2.B Processing-In-Memory"></a>2.B Processing-In-Memory</h3><p><img src="/2024/07/10/papers-paper-pim/pim02.png" alt="图2: PIM的电路模块和机制"><br>PIM技术通过添加计算接口电路对内存电路进行了改造。如图2所示，PIM有两种工作模式：内存模式和计算模式。在内存模式下，可以通过读/写端口访问数据，从内存阵列中读取或写入数据；而在计算模式下，向量形式的输入数据“V”流入PIM内存。这些输入数据直接与存储在PIM内存单元中的数据交互【2】，输出结果可以锁存在输出端口“I”。PIM内存以模拟处理的方式计算GEMM，通过模数转换器（ADC）作为计算电路接口：I = VW，其中V是流入的输入，W是存储在PIM内存中的数据，I是计算结果。</p><p>将PIM内存阵列与指令解码器和数据路径控制器一起排列形成PIM加速器。最先进的PIM加速器根据PIM的新型编程模型定制指令【12】。然而，将软件程序编译和部署到PIM硬件上是一个非常复杂的问题【13】【14】，目前仍在研究和开发中。</p><h3 id="2-C-GEMM-optimization"><a href="#2-C-GEMM-optimization" class="headerlink" title="2.C GEMM optimization"></a>2.C GEMM optimization</h3><p>PIM架构可用于大多数AIoT应用，特别是机器学习（ML）和深度神经网络（DNN）。大规模矩阵乘法占总体操作的70%以上【15】，PIM硬件可以大大减少计算时间。在算法层面，矩阵乘法的时间复杂度可以通过Strassen算法和Winograd算法来降低【16】。在微架构层面，优化措施包括改善内存访问局部性和使用向量指令，以减少内存访问时间并提高缓存命中率。在本文中，我们采用硬件/软件协同设计方法，在PIM技术的帮助下加速GEMM。</p><h2 id="3-Heterogeneous-Memory-Architecture"><a href="#3-Heterogeneous-Memory-Architecture" class="headerlink" title="3 Heterogeneous Memory Architecture"></a>3 Heterogeneous Memory Architecture</h2><p>为了解决上述挑战，我们提出了异构内存架构（HMA），这是一种简约的设计，通过引入PIM内存来升级现有SoC设计，从而提升性能和效率。</p><h3 id="3-A-Overall-Structure"><a href="#3-A-Overall-Structure" class="headerlink" title="3.A Overall Structure"></a>3.A Overall Structure</h3><p>HMA的核心思想是将PIM内存以与传统内存相同的方式附加在SoC上，而不是像图2(b)所示那样构建独立的PIM内存加速器。图3展示了HMA结构在SoC上的整体工作原理。图2(a)展示了PIM内存中的硬件组件。PIM输入/输出缓冲区（通过静态随机存取存储器（SRAM）实现的片上高速内存，用于缓存PIM计算的输入/输出数据）和PIM内存通过互连接口安装在系统总线上。计算模式下的输入通过物理金属总线连接，使PIM内存可以直接访问PIM输入缓冲区中的数据。输出数据锁存器，加上额外的多路复用器/解码器和总线从属模块，形成了一个“虚拟输出缓冲区”，即通过系统总线可以寻址的输出数据块。额外的PIM输出缓冲区是完成GEMM操作所必需的，用于缓存中间部分和的结果。在这种硬件结构下，CPU指示PIM进行编程并执行内存中的GEMM计算。HMA的详细编程模型在第四节中描述。<br><img src="/2024/07/10/papers-paper-pim/pim03.png" alt="图3. 提出的HMA的层次结构框图, 异构内存由总线从属设备管理。"></p><p>图4展示了HMA内部地址分配的一个典型案例。整个地址空间可以分为几个块。我们以一个小型AIoT SoC设计为例。总地址宽度设为32位。每64MB组成一个块。块0和块7中的地址保留用于CPU配置、指令存储、内部外围设备等。块1覆盖片上SRAM和连接到低速总线的外部外围设备。PIM相关的缓冲区和PIM内存独占块3用于GEMM加速。在这个例子中，剩余的块4至6可用于未来扩展。请注意，不同形式的SoC可能根据其独特的总地址宽度和片上内存及PIM内存的规模，导致不同的分块方法。<br><img src="/2024/07/10/papers-paper-pim/pim04.png" alt="图4: 32 位地址宽度 SoC 上 HMA 中的典型地址空间分配。"></p><h3 id="3-B-Data-Transportation"><a href="#3-B-Data-Transportation" class="headerlink" title="3.B Data Transportation"></a>3.B Data Transportation</h3><p>HMA实现的一个关键前提是片上互连网络能够提供足够的带宽来支持与PIM相关的双向数据传输，即：(a) 情况A：从PIM输出缓冲区获取PIM计算结果；(b) 情况B：将输入向量/矩阵传输到PIM输入缓冲区。我们通过模拟AHB通信来验证这一点。寄存器传输级（RTL）代码在28nm工艺节点下进行综合和仿真，片上时钟为1GHz，总线宽度为128位。PIM相关信息汇总在表1【3】。<br><img src="/2024/07/10/papers-paper-pim/pim05.png" alt="表1"><br>结果如图5所示。Y轴是数据传输率。顶部X轴是突发增量模式下AHB总线的事务大小，底部X轴是PIM内存阵列的数量。我们模拟这种模式是因为向量/矩阵中的元素大多与线性分布的地址对齐。如表1所示，每个PIM内存阵列的大小为256kb。多个PIM内存阵列并行工作，使得输入/输出数据传输率线性增加。</p><p>图5(a)和(b)分别展示了两种情况的结果。它们分别对应于AHB管理器的读操作（情况A）和写操作（情况B）。结果表明，同时工作的PIM内存阵列越多，总线所需的数据传输率就越高。事务大小应提高到64字节，以涵盖高达3000MB/s的8个PIM内存阵列的高PIM输出吞吐量。通过这种方式，常用的AHB可以处理PIM的输入和输出，而不会出现额外的拥塞或互连缓冲。<br><img src="/2024/07/10/papers-paper-pim/pim06.png" alt="图5: 片上AHB总线的带宽和计算模式下PIM宏的吞吐量。"></p><h2 id="4-HMA-Tensor-Mapping-Approach"><a href="#4-HMA-Tensor-Mapping-Approach" class="headerlink" title="4 HMA Tensor Mapping Approach"></a>4 HMA Tensor Mapping Approach</h2><p>基于HMA硬件基础设施，我们提出了一种软硬件协同设计方法，即HMA张量映射方法，以提高计算效率并最大化HMA结构的优势。</p><h3 id="4-A-Approch-Background"><a href="#4-A-Approch-Background" class="headerlink" title="4.A Approch Background"></a>4.A Approch Background</h3><p>如前所述，深度神经网络需要进行大量高维矩阵运算，这些运算全部发生在我们的设计中的PIM内存中。在大多数情况下，操作数矩阵的大小大于单个PIM内存提供的单元大小。更具体地说，第五节中的示例性PIM内存在维度为I的情况下执行I = VW，其中I为1×16，V为1×64，W为64×16。而经典的简单两层感知器涉及一个784×100的权重矩阵，远大于64×16【17】。因此，存储在传统内存中的所有相关高维和大规模输入数据以及权重数据都需要进行分区并输入到PIM内存中以提高计算效率。</p><p>基于我们的方法，我们将PIM内存中的所有矩阵乘法直接编译为一系列普通的内存操作指令，相比之下，传统的PIM加速器将PIM中的GEMM归因于定制指令控制【10】。通过这一创新，PIM计算可以通过gcc工具链轻松编译，并基于这些指令在软件中进行优化。</p><p>表II中给出了PIM指令的内存版本的详细解释。指令中的(x,y)表示传统内存中矩阵左上角元素的地址偏移。</p><h3 id="4-B-Optimization-Method-of-HMA-GEMM-Execution"><a href="#4-B-Optimization-Method-of-HMA-GEMM-Execution" class="headerlink" title="4.B Optimization Method of HMA-GEMM Execution"></a>4.B Optimization Method of HMA-GEMM Execution</h3><p>对于大多数设计而言，可以使用多个PIM和HMA结构来增强系统性能。这些PIM内存应并行调度，以利用神经网络计算中PIM内存中的权重数据不需要频繁更改的事实。因此，如何分解这些操作取决于可访问的HMA架构数量。如果PIM内存数量足够多，可以覆盖整个矩阵权重，则权重数据可以保持不分割并部署在每个PIM内存中。然而，在大多数实际情况下，PIM和HMA结构不足以直接覆盖神经网络计算中的矩阵权重。由于这种硬件限制，权重数据无法在这些PIM中保持不变。与传统GEMM频繁交换权重数据不同，我们将操作拆分为多组乘法和加法。每个输入数据与已经存储在PIM矩阵中的某个权重相关联，需要一起计算，这样可以减少PIM内存中权重数据重新加载的频率。加载到同一行不同PIM输出缓冲区的计算结果相应叠加。当所有乘法结果准备就绪后，接下来进行加法操作。在最后一步，不同列的加法结果将横向拼接完成一次GEMM操作。</p><p>图6展示了优化前后的GEMM伪代码以及HMA张量映射方法的图模型。<br><img src="/2024/07/10/papers-paper-pim/pim07.png" alt="图6: 矩阵乘法的优化过程"><br>例如，在PIM结构中计算矩阵乘法C = A × B，其中操作数矩阵A和B的大小分别为100×400和400×80。PIM输入的大小为1×64，每次操作PIM内存的GEMM大小为64×16。为了简单起见，我们假设只有一个HMA结构。此操作可以通过HMA-PIM按以下步骤完成（如图7所示）：</p><ol><li>“LD.W (0,0)”和“SD.M”：从B矩阵的(0,0)位置获取数据并将其存储在HMA的PIM内存中；</li><li>“LD.I (0,0)”和“SD.V”：从A矩阵的(0,0)位置获取数据并将其存储在HMA的输入缓冲区中；</li><li>“LD.O”和“SD.R (0,0)”：从输出缓冲区获取结果并将其存储在传统内存中；</li><li>“LD.I (1,0);…;LD.I (99,0)”：重复此过程依次从A矩阵中取出数据，在PIM中计算；</li><li>“SD.R (1,0);…;SD.R (99,0)”：写回PIM的计算结果；</li><li>“LD.W (0,16)”：更新PIM内存中的数据并重复步骤1-5。<br><img src="/2024/07/10/papers-paper-pim/pim08.png" alt="图7: GEMM操作的内存操作编译序列可视化。"></li></ol><h3 id="4-C-Memory-Access-Frequency"><a href="#4-C-Memory-Access-Frequency" class="headerlink" title="4.C Memory Access Frequency"></a>4.C Memory Access Frequency</h3><p>频繁的内存访问会导致额外的开销，因此我们希望通过PIM来减少这种开销。本小节中将分析GEMM中的内存访问情况。</p><p>对于一个给定的矩阵乘法问题，计算C = A × B，可以表示为元素级表达式：</p><script type="math/tex; mode=display">C_{m,n} = \sum_{k=1}^K A_{m,k} B_{k,n} \ \ \ m,n,k \in N</script><p>其中矩阵A、B和C的大小分别为m×k、k×n和m×n。使用CPU循环的传统方法进行内存访问的总次数为：</p><script type="math/tex; mode=display">RT_{classic} = (2 + 1 + 1) \cdot m \cdot n \cdot k = 4 \cdot m \cdot n \cdot k</script><p>其中，m、n、k是累加求和的循环次数；2、1和1分别是访问C、A和B所需的频率，这涉及数据传输延迟。基于上述例子，内存访问次数为$4 × 100 × 400 × 80 = 1.28 × 10^7$。</p><p>如果使用PIM进行计算，内存访问频率RT_PIM可以描述如下：</p><script type="math/tex; mode=display"> RT_{PIM} = 6 \cdot parRowA \cdot parColA \cdot parColB</script><p>其中，$parRowA = m$，$parColA = k/length(input_{col})$，$parRowB = k/length(PIM_{row})$，$parColB = n/length(PIM_{col})$。parRowA、parColA、parRowB、parColB分别表示在PIM输入缓冲区和PIM内存中分区的矩阵A和B的数量。Inputcol是输入缓冲区的大小，而PIMrow和PIMcol是PIM内存的大小。</p><p>回到上述例子中，内存访问次数减少到：$ 6 \cdot 100 \cdot 400/64 \cdot 80/16 = 2.1 \times 10^4 $<br>（PIM内存带来了100倍的减少）。</p><p>通过使用多核和提出的拼接方法，PIM内存中的权重数据在最大程度上保持不变，内存访问频率进一步降低为：</p><script type="math/tex; mode=display">RT_{opt} = parRowB \cdot parColB \cdot (2+4\cdot parRowA)</script><p>然后，在上述相同的例子中，通过使用提出的拼接方法，内存访问次数可以进一步减少到：<br>$ \frac{400}{64} \times \frac{80}{16} \times (2 + 4 \times 100) = 1.407 \times 10^4 $<br>（由于提出的拼接方法，又减少了1.5倍）。</p><h2 id="5-Evaluation"><a href="#5-Evaluation" class="headerlink" title="5 Evaluation"></a>5 Evaluation</h2><h3 id="5-A-Simulation-Setup"><a href="#5-A-Simulation-Setup" class="headerlink" title="5.A Simulation Setup"></a>5.A Simulation Setup</h3><p>我们使用文献【3】中的最新设计作为目标PIM内存实现。每两个核心结合不同的数字显著性，以获得输入和权重的8位精度，形成4个PIM内存并行工作。每个等效的PIM内存在维度为V、W、I分别为1×64、64×16和1×16的情况下执行I = VW操作。根据表I，每次PIM矩阵操作在基本工作时钟频率为1GHz的情况下需要19个周期。为了提供足够的总线带宽，我们按照图5中的相同配置，将AHB总线的事务大小设置为64字节。针对AIoT应用场景，基线是最新的嵌入式TinyML处理器【18】。</p><h3 id="5-B-Compute-Latency"><a href="#5-B-Compute-Latency" class="headerlink" title="5.B Compute Latency"></a>5.B Compute Latency</h3><p>图8(a)显示了计算延迟的结果。x轴表示输入矩阵的维度，y轴表示相对于内存访问（包括指令解码和总线传输延迟）的单位时间的相对计算延迟，以对数刻度表示。在图8(b)的柱状图中，当输入维度从4096增加到5184时，加速率下降。因为传输的数据大小不足以填满整个PIM内存，因此空间利用率降低。与基线TinyML处理器在仅GEMM基准测试中的表现相比，PIMopt模型可以将性能提高约430倍。与优化前的PIM模型相比，提出的PIMopt可以将性能提高约11倍。<br><img src="/2024/07/10/papers-paper-pim/pim09.png" alt="图8: 不同尺寸的GEMM操作的计算延迟"></p><h3 id="5-C-Area-and-Power-Efficiency"><a href="#5-C-Area-and-Power-Efficiency" class="headerlink" title="5.C Area and Power Efficiency"></a>5.C Area and Power Efficiency</h3><p>我们使用Verilog HDL在28nm商业工艺设计套件中实现了AHB从属设备。比较结果，包括与PUMA【10】的比较，列在表III中。PUMA的结果根据【10】中的数据估算并缩放到28nm节点。由于HMA设计去除了占主导地位的片上网络基础设施，相同2Mb PIM内存在28nm工艺下的总体面积减少了44.6%。我们仅比较了连接到SoC接口的平均功耗，不包括PIM内存、指令内存和额外的输出缓冲区。HMA大大简化了互连网络设计，使2Mb PIM内存的外围电路活动功耗减少了416倍。</p><h3 id="5-D-DNN-Acceleration-Analysis"><a href="#5-D-DNN-Acceleration-Analysis" class="headerlink" title="5.D DNN Acceleration Analysis"></a>5.D DNN Acceleration Analysis</h3><p>为了研究HMA在实际DNN推理任务中的性能提升，我们选择了6个广泛使用的DNN基准：LeNet、AlexNet、VGG-16、VGG-19、ResNet-18和MobileNet【19】。结果如图9所示，其中y轴表示延迟的自然对数，x轴描述了每个基准的层数。在所有这些实际情况下，“pim optimized”在每层中的速度提高了11.0倍，无论是卷积层还是全连接层的维度。在这些神经网络中，MobileNet具有深而窄的神经网络结构——深度可分离卷积的形状导致了图9中显示的波动趋势。<br><img src="/2024/07/10/papers-paper-pim/pim10.png" alt="图9: DNN基准测试中的加速效果。"></p><h2 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6 Conclusion"></a>6 Conclusion</h2><p>在这项工作中，我们提出了一种异构内存架构，以提高PIM在传统小型嵌入式SoC上的效率。在此基础上，我们进一步提出了一种映射算法，以更好地利用PIM的加速能力。功耗和操作延迟在多种常见的AIoT应用下进行了全面探讨。该分析可以为PIM相关SoC设计在其早期设计阶段提供高层次软硬件协同设计的重要指导。</p>]]></content>
    
    
    <categories>
      
      <category>论文精读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文精读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SQuAD数据集的结构和代码</title>
    <link href="/2024/07/10/dataset-squad/"/>
    <url>/2024/07/10/dataset-squad/</url>
    
    <content type="html"><![CDATA[<h1 id="一文理解SQuAD数据集的结构"><a href="#一文理解SQuAD数据集的结构" class="headerlink" title="一文理解SQuAD数据集的结构"></a>一文理解SQuAD数据集的结构</h1><h2 id="SQuAD1-1版本"><a href="#SQuAD1-1版本" class="headerlink" title="SQuAD1.1版本:"></a>SQuAD1.1版本:</h2><p>SQuAD数据集全称Stanford Question Answering Dataset,是一个阅读理解数据集,是工作者在维基百科文章上提出的问题,每个问题的答案都是相应文章中的一段文本.  </p><p>SQuAD1.1版本的论文: <a href="https://arxiv.org/pdf/1606.05250">https://arxiv.org/pdf/1606.05250</a></p><p>SQuAD1.1的huggingface: <a href="https://huggingface.co/datasets/rajpurkar/squad">https://huggingface.co/datasets/rajpurkar/squad</a></p><h2 id="SQuAD2-0版本"><a href="#SQuAD2-0版本" class="headerlink" title="SQuAD2.0版本:"></a>SQuAD2.0版本:</h2><p>SQuAD2.0组合了SQuAD1.1中的10万个问题，并增加了超过5万个无法回答的问题，这些问题由众包工作者以对抗（adversarially）的方式设计，看起来与可回答的问题相似。<br>为了在SQuAD2.0数据集上表现出色。系统不仅必须在可能的情况下回答问题，还必须确定篇章数据何时不支持回答，并避免回答。</p><p>SQuAD2.0版本的论文: <a href="https://arxiv.org/abs/1806.03822">https://arxiv.org/abs/1806.03822</a></p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>下面通过代码分析SQuAD1.1的结构.  </p><p>SQuAD数据集分为训练集和验证集, 训练集包含87599条数据, 验证集包含10570条数据.<br>每条数据分为[‘id’, ‘title’, ‘context’, ‘question’, ‘answers’]五个部分, 其中id是该条数据的id编号, title是标题, context是一段文本, question是针对这段文本的问题, answer是字典类型,分为text和‘answer-start’两部分, ‘text’里面是回答, answer-start里面是这个回答开始的位置.</p><p>具体的,我们看下面的代码:<br>首先下载数据, 查看datasets的结构:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br>datasets = load_dataset(<span class="hljs-string">&#x27;squad&#x27;</span>)<br><span class="hljs-built_in">print</span>(datasets)<br><span class="hljs-comment"># DatasetDict(&#123;</span><br><span class="hljs-comment">#     train: Dataset(&#123;</span><br><span class="hljs-comment">#         features: [&#x27;id&#x27;, &#x27;title&#x27;, &#x27;context&#x27;, &#x27;question&#x27;, &#x27;answers&#x27;],</span><br><span class="hljs-comment">#         num_rows: 87599</span><br><span class="hljs-comment">#     &#125;)</span><br><span class="hljs-comment">#     validation: Dataset(&#123;</span><br><span class="hljs-comment">#         features: [&#x27;id&#x27;, &#x27;title&#x27;, &#x27;context&#x27;, &#x27;question&#x27;, &#x27;answers&#x27;],</span><br><span class="hljs-comment">#         num_rows: 10570</span><br><span class="hljs-comment">#     &#125;)</span><br><span class="hljs-comment"># &#125;)</span><br></code></pre></td></tr></table></figure></p><p>打印出训练集中第一条数据的结构:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(datasets[<span class="hljs-string">&#x27;train&#x27;</span>][<span class="hljs-number">0</span>])<br><br><span class="hljs-comment"># &#123;&#x27;id&#x27;: &#x27;5733be284776f41900661182&#x27;, </span><br><span class="hljs-comment"># &#x27;title&#x27;: &#x27;University_of_Notre_Dame&#x27;, </span><br><span class="hljs-comment"># &#x27;context&#x27;: &#x27;Architecturally, the school has a Catholic character. Atop the Main Building\&#x27;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &quot;Venite Ad Me Omnes&quot;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.&#x27;, </span><br><span class="hljs-comment"># &#x27;question&#x27;: &#x27;To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?&#x27;, </span><br><span class="hljs-comment"># &#x27;answers&#x27;: </span><br><span class="hljs-comment">#     &#123;&#x27;text&#x27;: [&#x27;Saint Bernadette Soubirous&#x27;], </span><br><span class="hljs-comment">#     &#x27;answer_start&#x27;: [515]&#125;</span><br><span class="hljs-comment"># &#125;</span><br></code></pre></td></tr></table></figure></p><p>Answer回答可能包含多个回答, 输出验证集中包含多个回答的数据:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> index, dct <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(datasets[<span class="hljs-string">&#x27;validation&#x27;</span>]):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(dct[<span class="hljs-string">&#x27;answers&#x27;</span>][<span class="hljs-string">&#x27;text&#x27;</span>])&gt;<span class="hljs-number">1</span>: <br>        <span class="hljs-built_in">print</span>(dct[<span class="hljs-string">&#x27;answers&#x27;</span>])<br></code></pre></td></tr></table></figure></p><h2 id="SQuAD实现GPT2的问答微调"><a href="#SQuAD实现GPT2的问答微调" class="headerlink" title="SQuAD实现GPT2的问答微调"></a>SQuAD实现GPT2的问答微调</h2><p>未完成</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考:"></a>参考:</h2><p><a href="https://ifwind.github.io/2021/08/30/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%884%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%97%AE%E7%AD%94/#bert%E5%AE%9E%E6%88%984%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%97%AE%E7%AD%94">Bert实战4——问答任务-抽取式问答</a></p><p><a href="https://dev.to/admantium/llm-fine-tuning-workshop-improve-question-answering-skills-1h18">LLM Fine-Tuning Workshop: Improve Question-Answering Skills</a></p>]]></content>
    
    
    <categories>
      
      <category>数据集</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据集</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Double-Win-Quant 论文精读</title>
    <link href="/2024/07/10/papers-paper04-double-win-quant/"/>
    <url>/2024/07/10/papers-paper04-double-win-quant/</url>
    
    <content type="html"><![CDATA[<h1 id="Double-Win-Quant-论文精读"><a href="#Double-Win-Quant-论文精读" class="headerlink" title="Double-Win Quant 论文精读"></a>Double-Win Quant 论文精读</h1><p>今天读这篇论文: <a href="https://proceedings.mlr.press/v139/fu21c/fu21c.pdf">Double-Win Quant: Aggressively Winning Robustness of Quantized Deep Neural Networks via Random Precision Training and Inference</a></p><p>官方代码在这里: <a href="https://github.com/RICE-EIC/Double-Win-Quant">GitHub Rice-Eic Double-Win-Quant</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>量化在使强大而复杂的深度神经网络（DNN）能够部署到资源受限的平台中具有很大潜力。然而，除非配备复杂的技术，否则量化的DNN容易受到对抗性攻击，这导致在DNN的效率和鲁棒性之间的困境。在这项工作中，我们展示了量化在DNN鲁棒性方面的新视角，主张量化可以大大提高DNN的鲁棒性，并提出了一个名为Double-Win Quant的框架，可以使量化的DNN在鲁棒性方面比全精度模型有大幅提升。具体来说，我们首次发现，当对抗训练模型以后训练方式量化到不同精度时，相关的对抗性攻击在不同精度之间传输效果很差。利用这一有趣的观察，我们进一步开发了Double-Win Quant，集成了随机精度推理和训练，以进一步减少并利用这种差的对抗转移性，实现DNN的鲁棒性和效率方面的“双赢”。大量实验和消融研究始终验证了Double-Win Quant在各种攻击/模型/数据集上的有效性和相对于最先进（SOTA）对抗训练方法的优势。我们的代码可在以下网址获取：<a href="https://github.com/RICE-EIC/Double-Win-Quant。">https://github.com/RICE-EIC/Double-Win-Quant。</a></p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>近期的DNN突破和物联网（IoT）设备的兴起引发了对DNN驱动的智能IoT设备的爆炸性需求（Liu等，2018；Wu等，2018）。然而，将DNN部署到IoT设备中仍然充满挑战。首先，强大的DNN通常成本高昂，而IoT设备经常面临严苛的资源限制。其次，尽管DNN易受对抗性攻击，但许多IoT应用需要严格的安全性。因此，提升DNN效率和鲁棒性的技术非常需要。</p><p>作为开发高效DNN的最有前途的技术之一，并且普遍适用于多种算法，量化DNN的鲁棒性已引起越来越多的关注。最初认为，量化的舍入效应可能有助于消除小的对抗性扰动，早期的工作（Galloway等，2017；Panda等，2019）确实表明，二进制网络（Galloway等，2017；Panda等，2019）或基于tanh的量化DNN（Rakin等，2018）甚至比其全精度对应物更具鲁棒性。后来，Gupta和Ajanthan（2020）和Lin等（2019）发现这些方法实际上遭受了模糊梯度问题（Athalye等，2018；Papernot等，2017），导致了一种虚假的鲁棒性感觉。Lin等（2019）进一步提高了社区对量化DNN低劣鲁棒性的认识，指出主要原因是误差放大效应，即对抗性扰动在通过DNN层时被放大。最近的开创性工作（Lin等，2019；Song等，2020；Shkolnik等，2020）试图压缩这种放大效应，以实现鲁棒和高效的DNN。</p><p>在这项工作中，我们提出了一个有趣的问题：“量化能否被恰当地利用以提高DNN的鲁棒性？”这受到以下最近发现的启发：（1）输入上的随机平滑或变换（Cohen等，2019；Li等，2018；Xie等，2017；Guo等，2017）可以防御DNN免受对抗性攻击；（2）权重扰动是输入扰动的良好补充（Wu等，2020），因为它们可以缩小鲁棒泛化差距，因为权重可以全局影响所有样本的损失。我们推测，量化噪声可以被利用来提供类似于权重和激活的扰动效果。具体来说，我们做出了以下贡献：</p><ul><li>我们提供了一个关于量化在DNN鲁棒性中角色的新视角，并主张如果适当利用量化，可以使DNN的鲁棒性比其全精度对应物显著增强，而不仅仅是提高量化模型的鲁棒性。</li><li>我们首次发现，即使对抗训练模型在后训练方式下直接量化到不同的精度，相关的对抗性攻击在不同精度之间的转移效果依然很差，即使用一种精度生成的对抗性攻击在攻击量化到其他精度的相同模型时通常成功率较低。</li><li>我们提出了一个简单但非常有效的框架，称为Double-Win Quant，它集成了随机精度推理（RPI）和随机精度训练（RPT），以实现DNN鲁棒性和效率方面的积极“双赢”。具体来说，RPI在运行时随机选择一个推理精度作为对抗训练模型的随机扰动，而RPT在训练中采用可切换的批量归一化，以进一步减少对抗性转移性差的影响，从而提高DNN的可实现鲁棒性。</li><li>大量实验和消融研究表明，我们的方法在四种常用的对抗性训练方法、四种DNN模型和三种数据集上普遍有效，例如在CIFAR-10上使用PGD-7训练WideResNet32时，在PGD-20攻击下实现了12.14%的更高鲁棒精度，同时计算成本减少了88.9%。此外，我们的方法在更具攻击性的扰动下显示出更大的改进。</li></ul><h2 id="2-Related-Works-and-Background"><a href="#2-Related-Works-and-Background" class="headerlink" title="2 Related Works and Background"></a>2 Related Works and Background</h2><p><strong>DNN量化</strong>。量化已成为开发高效DNN的主流技术之一，通过使用较低浮点精度（Wang等，2018；Sun等，2019）或定点精度（Zhu等，2016；Li等，2016；Jacob等，2018；Mishra &amp; Marr，2017；Mishra等，2017；Park等，2017；Zhou等，2016）来表示权重/激活/梯度。特别是，Jacob等（2018）提出了量化感知训练，以学习权重分布，最小化量化后的精度下降。随后，可训练量化器（Jung等，2019；Bhalgat等，2020；Esser等，2019；Park &amp; Yoo，2020）通过可训练的量化参数进一步提高了低精度量化DNN的精度。与此同时，混合精度量化方法（Wang等，2019；Xu等，2018；Elthakeb等，2020；Zhou等，2017）被提出，以分配不同层不同的精度。然而，由于DNN的误差放大效应，量化DNN被发现对对抗性攻击更为脆弱。因此，开发同时有利于DNN效率和鲁棒性的量化技术非常必要。</p><p><strong>对抗性攻击与防御</strong>。众所周知，DNN容易受到对抗性攻击（Goodfellow等，2014），即输入上的小扰动可以误导模型的决策。为了增强DNN的鲁棒性，许多防御方法（Guo等，2017；Buckman等，2018；Song等，2017；Xu等，2017；Liao等，2018；Metzen等，2017；Feinman等，2017；Li等，2018；Wu等，2020）也被提出，尽管其中许多方法后来被更强的攻击所击败。特别是，对抗性训练（Shafahi等，2019；Madry等，2017；Wong等，2019；Tramèr等，2017）目前是最有效的防御方法。具体来说，它通过生成不同攻击的对抗样本扩充训练集，从而使模型能够正确分类类似的未见对抗样本。在本工作中，我们重新思考量化在DNN鲁棒性中的角色，并利用它显著增强DNN的鲁棒性，超过其全精度对应物。</p><p><strong>鲁棒且高效的DNN</strong>。效率和鲁棒性对大多数DNN应用都至关重要，已有一些开创性的工作旨在实现两者。例如，（Ye等，2019；Sehwag等，2020；Guo等，2018；Rakin等，2019）通过剪枝DNN以派生子网络，这些子网络可以维持或提高鲁棒性，而（Hu等，2020）通过输入自适应推理平衡鲁棒性和效率。同时，由于量化在提升效率方面有前景，其他工作努力设计鲁棒的量化DNN。具体来说，（Galloway等，2017；Panda等，2019）提出了鲁棒的二值神经网络，但它们却遭受模糊梯度问题（Athalye等，2018；Papernot等，2017）。Rakin等（2018）采用了基于tanh的量化，但也如Lin等（2019）所观察到的那样，遭受模糊梯度问题。后来，Lin等（2019）发现量化网络由于误差放大效应更容易受到对抗性攻击。为了解决这一问题，Lin等（2019）和Shkolnik等（2020）在模型损失函数中添加了新的正则化项，而Song等（2020）通过反馈学习（Song等，2019）重新训练网络。此外，Panda（2020）研究了层级精度，Gui等（2019）构建了一个统一的公式来分别平衡和强化鲁棒性和紧凑性。然而，现有工作并未考虑利用量化来增强鲁棒性。我们的Double-Win Quant首次利用量化来提升DNN的鲁棒性，远远超过全精度对应物。</p><p><strong>量化DNN之间对抗样本的可迁移性</strong>。量化模型之间不同精度的对抗样本可迁移性已被（Bernhard等，2019；Gupta &amp; Ajanthan，2020）研究。特别是，Gupta和Ajanthan（2020）发现量化模型通常对由其全精度对应物生成的对抗样本具有鲁棒性，Bernhard等（2019）则发现全精度和量化模型之间以及不同比特宽度的量化模型之间的对抗样本可迁移性较差。基于前人的工作，我们进一步做出了新的贡献：（1）我们发现，即使是相同的对抗性预训练模型，如果在后训练方式下直接量化到不同精度，对抗样本在不同比特宽度的模型之间仍然传输不佳；（2）我们提出了两个简单且有效的技术（即RPI和RPT），基于（1）的观察，实现在DNN的鲁棒性和效率方面的双赢。</p><h2 id="3-The-Double-Win-Quant-Framework"><a href="#3-The-Double-Win-Quant-Framework" class="headerlink" title="3 The Double-Win Quant Framework"></a>3 The Double-Win Quant Framework</h2><p>在本节中，我们首先在3.1节介绍对抗性训练的预备知识，然后在3.2节呈现和分析我们Double-Win Quant（DWQ）的激励观察，最后分别在3.3和3.4节描述DWQ的集成技术，即随机精度推理（RPI）和随机精度训练（RPT）。</p><h3 id="3-1-Preliminaries-of-Adversarial-Training"><a href="#3-1-Preliminaries-of-Adversarial-Training" class="headerlink" title="3.1 Preliminaries of Adversarial Training"></a>3.1 Preliminaries of Adversarial Training</h3><p>众所周知，DNN容易受到对抗性攻击（Goodfellow等，2014），即在输入上施加小扰动$ \delta ( || \delta || \le \epsilon ) $可以误导DNN做出错误预测，其中$\epsilon$是限制扰动幅度的标量。为了增强DNN的对抗鲁棒性，目前对抗性训练是最强的防御方法（Athalye等，2018）。例如，在$l_{\infty}$攻击下（Goodfellow等，2014）通过最大化以下目标生成对抗扰动$\epsilon$：</p><script type="math/tex; mode=display">\max_{\|\delta\|_\infty \leq \epsilon} l(f_\theta(x + \delta), y)</script><p>其中，$\theta$表示DNN的权重，𝑥和𝑦分别表示输入和相应的标签，l是损失函数。</p><p>对抗性训练通过优化以下极小极大问题来提高模型的鲁棒性：</p><script type="math/tex; mode=display">\min_\theta \sum_i \max_{\|\delta\|_\infty \leq \epsilon} l(f_\theta(x_i + \delta), y_i)</script><p>不同的对抗性训练方法在于它们如何解决上述公式中的内部优化问题。具体来说，快速梯度符号法（FGSM）（Goodfellow等，2014）使用一步梯度的符号作为近似：</p><script type="math/tex; mode=display">\delta = \epsilon \cdot \text{sign}(\nabla_x l(f_\theta(x + \delta), y))</script><p>投影梯度下降法（PGD）（Madry等，2017）是FGSM的一个更强变种，通过多次迭代FGSM，每次使用小步长𝛼，公式为：</p><script type="math/tex; mode=display">\delta_{t+1} = \text{clip}_\epsilon \{\delta_t + \alpha \cdot \text{sign}(\nabla_{\delta_t} l(f_\theta(x + \delta_t), y))\}</script><p>FGSM-RS（Wong等，2019）通过引入随机初始化来增加对抗多样性：</p><script type="math/tex; mode=display">\delta = \text{Uniform}(-\epsilon, \epsilon)  \\\delta = \text{clip}_\epsilon \{\delta + \alpha \cdot \text{sign}(\nabla_\delta l(f_\theta(x + \delta), y))\}</script><p>其中，$clip_{\epsilon}$表示将输入限制在区间$[-\epsilon, \epsilon]$内的截断函数。</p><p>由于PGD攻击是最强的白盒攻击之一，我们在本工作中主要采用它来评估DNN的对抗鲁棒性。</p><h3 id="3-2-DWQ-Motivating-Observation"><a href="#3-2-DWQ-Motivating-Observation" class="headerlink" title="3.2 DWQ: Motivating Observation"></a>3.2 DWQ: Motivating Observation</h3><p>不同压缩模型之间的对抗攻击的可转移性已经被研究过（Matachana等，2020；Bernhard等，2019；Gupta &amp; Ajanthan，2020）。然而，如何利用这种可转移性来设计对抗攻击下的鲁棒DNN仍然是一个开放的问题。在本工作中，我们提出了一个问题：“对抗性训练模型在不同精度之间的对抗攻击的可转移性如何？”，考虑到预训练模型的精度可以瞬间切换（Jin等，2020）。我们发现，即使对抗性训练模型在训练后直接量化为不同的精度，这些对抗攻击在不同精度之间的可转移性仍然很差，无论其对抗性训练方法和训练精度如何。</p><p>实验设置。我们在不同的对抗性训练方法和训练精度下进行实验，以评估同一对抗性训练模型在不同精度下的对抗攻击的可转移性。我们在采用线性量化器（Jacob等，2018）的不同对抗性训练方法和训练精度下，使用PGD-20（20步PGD（Madry等，2017））攻击PreActResNet18（遵循（Wong等，2019）），其训练设置在4.1节介绍。在图1中，（a）∼（e）直接在训练后将模型量化为不同精度（权重和激活相同）以生成对抗样本和进行推理，（f）则利用不同量化模型在干净图像上生成的PGD-20攻击，模拟黑盒攻击。需要注意的是，实验（f）的目标是检查是否存在模糊梯度问题（Athalye等，2018），而不是像（a）∼（e）那样探索可转移性。</p><p>实验观察。可以得出四个观察结果：</p><ol><li>使用一种精度生成的对抗攻击在攻击同一对抗性训练模型时，量化到不同精度时成功率较低，尤其是在常采用的低精度下，这在不同的对抗性训练方法和量化模型的训练精度下是一致的；</li><li>白盒攻击下的平均鲁棒准确性（见图1（a）∼（e））始终高于使用相应对抗性训练方法的全精度模型，表明随机选择推理精度可以潜在地提供有效的防御。使用PGD-7/FGSM/FGSM-RS训练的PreActResNet18的全精度准确性分别为51.2%/41.5%/47.1%；</li><li>差的可转移性不是由于模糊梯度问题，因为模型在黑盒攻击下表现出比白盒攻击更好的鲁棒性（根据Athalye等，2018）；</li><li>在相同低精度下训练和攻击确实显著降低了鲁棒准确性，如图1（a）∼（e）的对角线所示，这与（Lin等，2019）的观察结果一致，原因是误差放大效应。</li></ol><p>分析和讨论。主要结论是，对于白盒攻击，用一种精度生成的对抗攻击在另一种精度下的可转移性较差。我们假设这种差的可转移性是因为对抗扰动被两种精度之间的量化噪声屏蔽，无法被基于梯度的攻击有效学习。具体来说，考虑一个线性量化器，对于激活值𝐴（权重相同），𝑘位量化值$𝐴_𝑞$可以表示为 $𝐴_𝑞 = 𝑆_𝑘 \lfloor  \frac{A}{S_k} \rceil $，其中$\lfloor \rceil$是取整操作，$ S_k = \frac{A_{max} - A_{min}}{2^k - 1} $是缩放因子。对于普通量化，基于梯度的攻击通过直接估计法（Bengio等，2013；Yin等，2019）可以有效学习量化效果，即$ \frac{\partial L}{\partial A} \approx \frac{\partial L}{\partial A_q} $，其中L是损失函数。然而，两种不同精度（m位和n位）之间的量化噪声$ S_m \lfloor \frac{A}{S_m} \rfloor - S_n \lfloor \frac{A}{S_n} \rfloor $ 不能被基于梯度的攻击有效学习，因此对抗扰动可以被埋藏在量化噪声中，导致攻击失败。</p><h3 id="3-3-Vanilla-DWQ-Random-Precision-Inference"><a href="#3-3-Vanilla-DWQ-Random-Precision-Inference" class="headerlink" title="3.3 Vanilla DWQ: Random Precision Inference"></a>3.3 Vanilla DWQ: Random Precision Inference</h3><p>这里我们介绍了整合了RPI（随机精度推理）的vanilla DWQ，它简单且普遍适用于不同的DNN。</p><p>方法论。给定一个经过对抗性训练的模型，RPI在推理期间随机选择一个精度集中的精度来量化模型的权重和激活。RPI的有效性基于以下两个事实：</p><ol><li>经过量化感知训练的模型在推理过程中被直接量化到不同精度时，其自然准确性（在干净图像上的表现）相对稳定（Jin等，2020；Guerra等，2020；Fu等，2021b），因此RPI生成的模型可以保持与静态精度模型相当的自然准确性；</li><li>随机选择推理精度可以大大降低对抗攻击的有效性，只要攻击不是在相同精度下生成的，这在不同的对抗性训练方法和训练精度下都得到了验证（见图1）。尽管对抗者可能会选择成功率较高的精度进行攻击，但RPI可以采用采样策略来倾向于选择通常更为鲁棒的精度。为了简便起见，本工作中我们认为对抗者和RPI都从相同的推理精度集中随机选择精度。RPI算法总结在算法1中。</li></ol><p><strong>实现</strong>。由于RPI的执行需要在推理过程中切换不同的精度，我们稍微修改了量化方案，灵感来自（Jin等，2020），以确保实现的简便性。具体来说，我们将权重$\theta$量化为 $\theta_q = \hat S_k min(\lfloor \frac{\theta}{\hat S_k} \rceil , 2^k - 1)$，其中 $  \hat S_k = \frac{\theta_{max} - \theta_{min}}{2^k}$。因此，在不同精度之间的切换只需要通过移位操作来裁剪最显著位（MSB），即$ \lfloor \frac{\theta}{\hat S_m} \rceil &gt;&gt; (m-1)$ 等于$ \lfloor \frac{\theta}{\hat S_n} \rceil $ ，其中 &gt;&gt; 是右移操作。因此，只需存储一个最高精度的量化模型副本。</p><p><strong>与先前工作的联系</strong>。RPI的精神与近期关于DNN扰动和鲁棒性的发现一致。特别地，已有研究表明，输入上的随机平滑或转换（Cohen等，2019；Li等，2018；Xie等，2017；Guo等，2017）可以帮助DNN增强对抗攻击的鲁棒性，（Wu等，2020）发现权重扰动可以作为输入扰动的良好补充，以缩小鲁棒性泛化差距，因为DNN的权重可以全局影响所有输入样本的损失。（He等，2019；Dhillon等，2018）也明确在模型的权重或激活中引入随机性和扰动。借鉴这些发现，我们推测RPI的有效性在于随机切换不同精度所产生的量化噪声自然地向权重和激活注入随机扰动，这可以抵消对抗特征的影响，从而增强模型的对抗鲁棒性。</p><p><strong>RPI的硬件支持</strong>。最先进的自适应精度加速器，如Bit Fusion（Sharma等，2018）和Stripes（Judd等，2016），致力于支持动态精度推理，这可以自然地支持RPI的执行。更多关于RPI的潜在硬件实现可以在（Camus等，2019）中找到。</p><h3 id="3-4-Enhanced-DWQ-Random-Precision-Training"><a href="#3-4-Enhanced-DWQ-Random-Precision-Training" class="headerlink" title="3.4 Enhanced DWQ: Random Precision Training"></a>3.4 Enhanced DWQ: Random Precision Training</h3><p>如上述小节所述，我们的vanilla DWQ仅通过操纵对抗训练模型的推理精度来提高其鲁棒性。在本小节中，我们介绍了增强版DWQ，它通过装备有可切换批量归一化（SBN）的RPT进一步加剧不同精度之间的低可迁移性，从而进一步增强DNN的对抗鲁棒性。</p><p><strong>动机</strong>。（Xie等，2020）采用了干净样本和对抗样本的双重BN，以提高自然准确性，这激发了我们分别处理干净和对抗输入统计数据的必要性。此外，（Jin等，2020；Guerra等，2020）建议为不同精度使用独立的BN，以实现对训练过的DNN的即时量化到不同位宽，保持与使用相应精度单独训练的相同DNN的自然准确性。这些工作激励并启发我们提出了增强版DWQ，因为为不同精度应用独立的BN来记录每个精度下生成的对抗样本的特定统计数据，可能会扩大不同精度之间的差距，即进一步增加对抗样本在不同精度之间的迁移难度。</p><p><strong>方法论</strong>。我们的增强版DWQ通过在每次迭代中从候选精度集中随机选择一个精度来生成对抗样本并使用所选精度更新模型，从头开始对模型进行对抗训练，同时为模型配备SBN以独立记录不同精度的统计数据。尽管存在其他训练方案，例如渐进精度（Fu等，2020）或动态精度（Fu等，2021a），但我们发现RPT在大幅提高量化模型鲁棒性方面已经足够有效，而不会增加训练复杂性，这在第4.4节中得到了验证。此外，我们在附录中可视化了增强版DWQ实现的对抗迁移性，可以观察到相较于vanilla DWQ，迁移性显著减少。</p><p>在PGD-7（Madry等，2017）训练基础上的RPT算法总结在算法2中，其他对抗训练方法（例如FGSM（Goodfellow等，2014）和FGSM-RS（Wong等，2019））的算法类似。请注意，RPI和RPT的一个优势是它们简单且在不同DNN模型、精度集和对抗训练方法中表现一致良好，无需在超参数选择上特别挑选，这在第4节中得到了验证。</p><h2 id="4-Experiment-Results"><a href="#4-Experiment-Results" class="headerlink" title="4 Experiment Results"></a>4 Experiment Results</h2><p>在本节中，我们首先在第4.1节介绍实验设置，然后在第4.2节对我们的DWQ与SOTA对抗训练方法进行基准测试。接下来，我们将在第4.3节和4.4节分别对DWQ的集成RPI和RPT技术进行全面的消融研究。</p><h3 id="4-1-Experiment-Setup"><a href="#4-1-Experiment-Setup" class="headerlink" title="4.1 Experiment Setup"></a>4.1 Experiment Setup</h3><p><strong>网络&amp;数据集</strong><br>我们在四个网络和三个数据集上评估了我们的DWQ，即在CIFAR-10/100上评估了PreActResNet18（参照Wong等人，2019），WideResNet32（参照Madry等人，2017和Shafahi等人，2019），和MobileNetV2，在ImageNet上评估了ResNet-50（参照Shafahi等人，2019和Wong等人，2019）。</p><p><strong>训练设置</strong>。我们考虑了四种SOTA对抗训练方法，包括FGSM（Goodfellow等人，2014），FGSM-RS（Wong等人，2019），PGD-7（Madry等人，2017），和Free（Shafahi等人，2019）。我们遵循他们原始论文中的对抗训练超参数设置，即在FGSM-RS训练中采用1.25𝜖的步长，在PGD-7训练中采用2的步长。为了公平起见，我们遵循(Madry等人，2017)的模型训练设置，不使用其他训练技巧。在CIFAR-10/100上，我们训练模型160个周期，批次大小为128，使用动量为0.9的SGD优化器，从初始学习率0.1开始，在第80和120个周期时衰减10倍。在ImageNet上，我们遵循ImageNet上SOTA量化工作的设置(Jung等人，2019；Bhalgat等人，2020；Esser等人，2019；Park和Yoo，2020)，从清晰图像上预训练的全精度模型开始。具体来说，我们对预训练的全精度ResNet-50进行60个周期的对抗训练，批次大小为256，使用动量为0.9的SGD优化器，从初始学习率0.01开始，在第30个周期时衰减10倍。</p><p><strong>攻击设置</strong>。我们主要考虑不同迭代次数/重启次数和扰动强度的PGD攻击(Madry等人，2017)，并评估DWQ在CW-L2/CW-Inf攻击(Carlini和Wagner，2017)，Auto-Attack(Croce和Hein，2020)，以及无梯度攻击Bandits(Ilyas等人，2018)下的整体鲁棒性。具体来说，对于CW-L2/CW-Inf攻击，我们采用AdverTorch（Ding等人，2019）中的实现，并遵循(Chen等人，2021；Rony等人，2019)中的设置；对于Auto-Attack（Croce和Hein，2020）和Bandits（Ilyas等人，2018），我们采用官方实现和原始论文中的默认设置。我们假设攻击者从与我们DWQ相同的推理精度集中随机选择精度进行攻击，这样可以保证公平，因为（1）任何超出DWQ推理精度集的攻击精度只会增加DWQ的鲁棒准确性，根据第3.2节的实验结果，和（2）虽然攻击者可能选择攻击成功率更高的精度，我们的DWQ也可以增加抽取更鲁棒精度的概率，以实现更强的防御，这里我们假设攻击者和DWQ都采用随机精度以简化说明。</p><p><strong>RPI和RPT的精度设置</strong>。DWQ涉及两种精度设置：（1）RPT的训练精度集和（2）RPI的推理精度集。在不挑选精度设置的情况下，我们基于效率-鲁棒性考虑确定精度。具体来说，如果没有特别说明，我们在采用RPT时，训练PreActResNet18和WideResNet32时使用4∼16位，训练MobileNetV2时使用4∼8位，并在对应的推理中使用相同的精度集。当仅启用RPI（即不使用RPT的DWQ）时，我们以定点4位对抗训练PreActResNet18和WideResNet32，以8位对抗训练MobileNetV2，并使用4∼8位的推理精度集。我们在第4.3和4.4节中验证了DWQ在不同推理和训练精度集选择下的一致性优势。此外，我们使用BitOPs（位操作）来衡量计算成本，参考SOTA量化DNN工作。</p><h3 id="4-2-DWQ-Benchmark-with-SOTA-Methods"><a href="#4-2-DWQ-Benchmark-with-SOTA-Methods" class="headerlink" title="4.2 DWQ: Benchmark with SOTA Methods"></a>4.2 DWQ: Benchmark with SOTA Methods</h3><p><strong>基准测试与SOTA对抗训练方法</strong><br>在这里，我们将我们的RPI和RPT技术与全精度训练的SOTA对抗训练方法进行基准测试，以验证其在提升模型鲁棒性和效率方面的“双赢”效果。</p><p><strong>CIFAR-10上的结果</strong>：如表1所示，（1）RPI和RPT在所有网络和对抗训练方法下的一致性提升了在不同设置下PGD攻击下的鲁棒准确率；（2）DWQ（即RPI + RPT）始终实现了最好的鲁棒性，同时保持了可比的自然准确率，远超全精度的SOTA对抗训练方法。具体来说，我们的DWQ在应用于PGD-7训练（最强的对抗训练方法之一）时，在PreActResNet18和WideResNet32上在PGD-20攻击下分别实现了13.98%/12.14%的更高鲁棒准确率，同时降低了88.9%的计算成本；（3）DWQ还在PGD-20攻击下在FGSM/FGSM-RS基础上提升了13.57%∼22.60%的鲁棒准确率。值得注意的是，尽管FGSM对抗训练在迭代攻击下容易失效（Kurakin等人，2016），我们的DWQ仍然显著提升了其鲁棒准确率22.6%。</p><p><strong>CIFAR-100上的结果</strong>：如表2所示，在CIFAR-100上的观察结果与CIFAR-10上的一致，表明我们的DWQ在更复杂任务中的可扩展性。具体来说，DWQ结合RPI和RPT在FGSM-RS/PGD-7训练的基础上在PGD-20攻击下分别在PreActResNet18和WideResNet32上实现了10.61%/13.77%和13.83%/9.39%的更高鲁棒准确率。</p><p><strong>ImageNet上的结果</strong>：如表3所示，我们可以观察到，我们增强的DWQ在自然准确率、鲁棒准确率和模型效率方面实现了三重胜利。具体来说，增强的DWQ在PGD-10攻击下在FGSM-RS（Wong等人，2019）和Free（Shafahi等人，2019）基础上分别实现了7.65%/10.11%的更高准确率，同时减少了88.9%的计算成本。这组实验进一步表明了我们DWQ框架在大规模数据集上的可扩展性和适用性。</p><p><strong>在紧凑型DNN上的基准测试</strong>：如（Madry等人，2017）所述，容量更高的模型在多步攻击下更具鲁棒性，因此防御像MobileNetV2（Sandler等人，2018）这样紧凑型模型更具挑战性和必要性。如表4所示，我们的DWQ在MobileNetV2上在FGSM-RS/PGD-7训练的基础上分别提升了17.05%/7.69%的鲁棒准确率，表明DWQ在紧凑型DNN中的适用性。我们还观察到，在FGSM-RS基础上的DWQ在鲁棒性和自然准确率之间实现了更好的平衡，而在PGD-7基础上的DWQ，因为PGD-7过于追求鲁棒性而忽视了MobileNetV2在干净图像上的量化易损性（Sheng等人，2018）。</p><p><strong>在更大扰动下的基准测试</strong>：我们进一步评估了DWQ在CIFAR-10上的PGD-7训练下在更大扰动下的可扩展性，如表5所示。令人感兴趣的是，DWQ在更强的对抗攻击下甚至实现了更大的鲁棒性提升。表5显示，DWQ在PGD-20攻击下实现了11.66%∼18.89%和15.68%∼23.26%的更高鲁棒准确率，扰动分别为𝜖=12和16。更大扰动下的鲁棒性提升验证了DWQ在更具挑战性环境中的适用性。</p><p><strong>在更多攻击下的基准测试</strong>：我们评估了增强的DWQ在PGD-7训练基础上对Auto-Attack（Croce和Hein，2020）、CW-L2/CW-Inf攻击（Carlini和Wagner，2017）和Bandits攻击（Ilyas等人，2018）的表现。如表6所示，增强的DWQ在不同攻击/模型/数据集/扰动下的一致性提升了鲁棒准确率，例如在Auto-Attack下+6.88%∼+9.12%、在Bandits攻击下+5.01%∼+24.48%，更令人惊讶的是在CW-Inf攻击下+9.97%∼+25.71%。这表明不同攻击/推理精度间的差转移性较差，因此DWQ仍然非常有效，而PGD-7训练的网络在更大扰动下的鲁棒性下降更多。这组实验验证了DWQ在不同攻击类型下实现的一致性鲁棒性。</p><p><strong>与SOTA鲁棒量化方法的基准测试</strong>：DWQ最相关的工作是（Lin等人，2019），该工作约束了逐层Lipschitz常数，并与DWQ正交。尽管（Lin等人，2019）压缩了量化对对抗鲁棒性的负面影响，DWQ利用量化噪声提升对抗鲁棒性，因此结合这两种方法可能会产生更鲁棒的DNN。与（Lin等人，2019）在CIFAR-10上PGD-20攻击下报告的最佳鲁棒准确率相比，我们的DWQ在同样的PGD-7训练网络上分别在𝜖=8和16下实现了14.6%和22.5%的更高鲁棒准确率。</p><p><strong>模糊梯度检查</strong>：我们还在PreActResNet18上评估了DWQ在（Athalye等人，2018）中所有其他模糊梯度标志下的表现，除了图1（f）中的黑箱攻击，我们发现（1）在CIFAR-10和CIFAR-100上，在1步/20步PGD攻击下的鲁棒准确率分别为78.37%/65.15%和52.25%/41.74%；（2）对于无限制PGD-20攻击，DWQ的鲁棒准确率接近于零；（3）在DWQ未发现对抗样本的情况下，PGD-20未找到对抗样本；（4）增加失真会导致鲁棒性下降，如表5所示。因此，DWQ不存在模糊梯度问题。</p><h3 id="4-3-DWQ-Ablation-Study-of-Vanilla-DWQ"><a href="#4-3-DWQ-Ablation-Study-of-Vanilla-DWQ" class="headerlink" title="4.3 DWQ: Ablation Study of Vanilla DWQ"></a>4.3 DWQ: Ablation Study of Vanilla DWQ</h3><p><strong>RPI在不同精度下的训练</strong>。图2(a)展示了在CIFAR-10上，RPI在PGD-20攻击下的三种不同精度的PGD-7训练网络所取得的自然和鲁棒准确率，采用了4∼8-bit的推理精度集。在不同的训练精度下，自然和鲁棒准确率保持稳定（在1.12%以内），表明RPI对量化模型具有普遍适用性。</p><p><strong>RPI采用不同的推理精度集</strong>。图2(b)展示了在CIFAR-10上，RPI在PGD-20攻击下的三种PGD-7/4-bit训练网络所取得的自然和鲁棒准确率，考虑了不同的推理精度集。我们可以观察到：（1）RPI采用不同的推理精度集始终改善了表1中普通PGD-7训练的鲁棒性；（2）没有RPT的帮助下，普通DWQ（即仅仅是RPI）稍微倾向于较小的精度范围和接近训练精度的精度，因为这两者导致了训练和推理精度之间较小的分布差异。因此，考虑到鲁棒性和效率，在第4.2节中我们采用了4∼8-bit的RPI。</p><p><strong>RPI仅应用于权重/激活</strong>。在所有其他实验中，我们采用相同的精度来处理权重和激活，以实现硬件友好的实现。在这里，我们进一步探讨了当RPI仅应用于权重或激活时，DWQ的动机观察和益处是否始终保持一致。图3中的实验表明，当固定权重或激活的精度时，对抗攻击的差转移性始终被观察到，这表明RPI仅应用于权重或激活时在增强模型鲁棒性方面仍然是有效的。</p><h3 id="4-4-DWQ-Ablation-Study-of-Enhanced-DWQ"><a href="#4-4-DWQ-Ablation-Study-of-Enhanced-DWQ" class="headerlink" title="4.4 DWQ: Ablation Study of Enhanced DWQ"></a>4.4 DWQ: Ablation Study of Enhanced DWQ</h3><p><strong>增强DWQ的不同精度范围</strong>。<br>图2(c)展示了在CIFAR-10上，三种使用增强DWQ（即RPI + RPT）训练的网络在不同训练精度集上的自然和鲁棒准确率（在PGD-20攻击下），以PGD-7训练为基础。我们可以看到：（1）虽然在不同设置下鲁棒准确率有所波动，DWQ结合RPT和RPI始终在鲁棒性方面超越SOTA的PGD-7训练或仅有RPI的DWQ（如表1所示）；（2）具有较高容量的模型如PreActResNet18和WideResNet32偏好较大的精度范围以减少命中对手精度的概率，而容量较低的模型如MobileNetV2则偏好较小的精度范围和相对较高的精度，因为它们对量化较为敏感（Sheng et al., 2018）。</p><p><strong>不同训练方案下的增强DWQ</strong>。我们还将增强的DWQ与另一种动态精度训练方法CPT（Fu et al., 2021a）结合，该方法在最低和最高精度之间循环切换，并在不同的循环周期下比较增强的DWQ和RPT或CPT。表7显示：（1）CPT在最强的PGD-7训练基础上可以达到与RPT相当的鲁棒性，但其循环周期需要微调；（2）CPT在较不强大的对抗训练方法基础上导致较大的训练不稳定性以及较低的自然和鲁棒准确率，我们推测这是由于SBN的统计数据与当前权重之间的不匹配，因为CPT仅在连续精度之间切换。相比之下，结合RPT的DWQ表现出一致的稳定性和有效性。</p><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h2><p>在这项工作中，我们展示了如果合理利用量化，可以显著增强量化DNN的鲁棒性，甚至超过其全精度模型的表现，而不仅仅是提高量化模型的鲁棒性。此外，我们提出了一个简单但有效的框架，称为Double-Win Quant，在DNN的鲁棒性和效率方面实现了激进的“双赢”。我们相信Double-Win Quant为设计鲁棒且高效的DNN开辟了新的视角。</p>]]></content>
    
    
    <categories>
      
      <category>论文精读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文精读</tag>
      
      <tag>模型量化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CPT Cyclic Precision Training 论文精读</title>
    <link href="/2024/07/10/papers-paper03-cpt/"/>
    <url>/2024/07/10/papers-paper03-cpt/</url>
    
    <content type="html"><![CDATA[<h1 id="CPT-cyclic-precision-training-论文精读"><a href="#CPT-cyclic-precision-training-论文精读" class="headerlink" title="CPT cyclic precision training 论文精读"></a>CPT cyclic precision training 论文精读</h1><p>今天精读这篇论文:<a href="https://arxiv.org/pdf/2101.09868">CPT: EFFICIENT DEEP NEURAL NETWORK TRAINING VIA CYCLIC PRECISION</a><br>官方代码在这里:<a href="https://github.com/GATECH-EIC/CPT">github-gatech-eic-cpt</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>低精度深度神经网络（DNN）训练由于减少精度是提高DNN训练时间/能量效率的最有效手段之一而受到了极大的关注。在本文中，我们尝试从一个新的角度探索低精度训练，这一角度受到最近在理解DNN训练方面的发现启发：我们推测DNN的精度在DNN训练期间可能具有类似于学习率的效果，并倡导在训练过程中使用动态精度以进一步提高DNN训练的时间/能量效率。具体来说，我们提出了循环精度训练（Cyclic Precision Training，CPT），在两个边界值之间循环变化精度，这两个边界值可以在前几轮训练中使用简单的精度范围测试来确定。对五个数据集和十一种模型进行的大量模拟和消融研究表明，CPT在各种模型/任务（包括分类和语言建模）中的有效性是一致的。此外，通过实验和可视化，我们表明CPT有助于（1）收敛到具有较低泛化误差的更宽的极小值，并且（2）减少训练方差，我们认为这为同时提高DNN训练的优化和效率开辟了一个新的设计维度。我们的代码可在以下地址获取：<a href="https://github.com/RICE-EIC/CPT">https://github.com/RICE-EIC/CPT</a></p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>现代深度神经网络（DNNs）的破纪录性能伴随着高昂的训练成本，因为需要大量的训练数据和参数，这限制了对众多应用程序高度需求的DNN驱动智能解决方案的发展（Liu等，2018；Wu等，2018）。例如，训练ResNet-50涉及10^18次浮点运算（FLOPs），在一个最先进的GPU上可能需要14天（You等，2020b）。同时，大规模的DNN训练成本引发了越来越多的财务和环境问题。例如，据估计，训练一个DNN的成本可能超过1万美元，并且排放的碳量高达一辆汽车的生命周期排放量。与此同时，最近DNN的进展激发了对智能边缘设备的巨大需求，其中许多设备需要在设备上进行现场学习，以确保在动态的现实环境中准确性，这些设备的资源有限，与高昂的训练成本之间存在不匹配（Wang等，2019b；Li等，2020；You等，2020a）。</p><p>为了应对上述挑战，广泛的研究工作致力于开发高效的DNN训练技术。其中，低精度训练引起了显著关注，因为它可以大幅提高训练时间/能量效率（Jacob等，2018；Wang等，2018a；Sun等，2019）。例如，GPU现在可以使用16位IEEE半精度浮点格式进行混合精度DNN训练（Micikevicius等，2017b）。尽管前景广阔，现有的低精度工作还没有充分利用理解DNN训练的最新发现。具体而言，现有工作大多在整个训练过程中固定模型精度，即采用静态量化策略，而最近的DNN训练优化工作建议在DNN训练过程中使用动态超参数。例如，（Li等，2019）显示，大的初始学习率有助于模型记住更易适应和更具普遍性的模式，这与从大学习率开始探索并逐渐减小以实现最终收敛的常见做法一致；（Smith，2017；Loshchilov &amp; Hutter，2016）通过采用循环学习率提高了DNN的分类准确性。</p><p>在本文中，我们倡导动态精度训练，并作出以下贡献：</p><ul><li>我们表明，DNN的精度似乎在DNN训练期间具有类似于学习率的效果，即低精度带有较大的量化噪声有助于DNN训练探索，而高精度带有更准确的更新有助于模型收敛，动态精度调度有助于DNN收敛到更好的极小值。这个发现为同时提高DNN训练的优化和效率开辟了一个设计维度。</li><li>我们提出了循环精度训练（Cyclic Precision Training, CPT），该方法在DNN的训练过程中采用循环精度调度，以推动DNN准确性和训练效率之间的可实现权衡。此外，我们表明，在训练的早期阶段使用简单的精度范围测试可以自动识别循环精度范围，这几乎没有计算开销。</li><li>在五个数据集和十一种模型上的广泛实验（包括分类和语言建模）验证了所提出的CPT技术在提高训练效率方面的一致有效性，同时达到或超过了可比的准确性。此外，我们提供了损失曲面的可视化，以更好地理解CPT的有效性，并讨论了其与最近对DNN训练优化理解的关系。</li></ul><h2 id="2-Related-Works"><a href="#2-Related-Works" class="headerlink" title="2 Related Works"></a>2 Related Works</h2><p><strong>量化的DNNs</strong>。DNN量化（Courbariaux等，2015；2016；Rastegari等，2016；Zhu等，2016；Li等，2016；Jacob等，2018；Mishra &amp; Marr，2017；Mishra等，2017；Park等，2017；Zhou等，2016）已基于目标精度-效率权衡进行了深入研究。例如，Jacob等（2018）提出了量化感知训练，以保持量化后的精度；Jung等（2019）；Bhalgat等（2020）；Esser等（2019）；Park &amp; Yoo（2020）努力使用可学习的量化器提高低精度DNN的精度。混合精度DNN量化（Wang等，2019a；Xu等，2018；Elthakeb等，2020；Zhou等，2017）为不同的层/滤波器分配不同的位宽。虽然这些工作都采用了静态量化策略，即分配的精度在量化后是固定的，但CPT在训练过程中采用了动态精度调度。</p><p><strong>低精度DNN训练</strong>。先锋性工作（Wang等，2018a；Banner等，2018；Micikevicius等，2017a；Gupta等，2015；Sun等，2019）表明DNN可以在降低精度的情况下进行训练。对于分布式学习，（Seide等，2014；De Sa等，2017；Wen等，2017；Bernstein等，2018）量化梯度以减少通信成本，而训练计算仍然采用全精度；对于集中式/设备上的学习，前向和后向计算中涉及的权重、激活、梯度和误差都采用降低精度。我们的CPT可以应用于这些低精度训练技术之上，所有这些技术在整个训练过程中都采用静态精度，以进一步提高训练效率。</p><p><strong>动态精度的DNNs</strong>。存在一些动态精度的工作，其目的是在全精度训练后推导出用于推理的量化DNN。具体来说，Zhuang等（2018）首先训练一个全精度模型以达到收敛，然后逐渐降低模型精度到目标值以获得更好的推理精度；Khoram &amp; Li（2018）也从一个全精度模型开始，然后逐渐学习每一层的精度以得出一个混合精度的对应模型；Yang &amp; Jin（2020）基于两个连续位宽的线性插值学习每一层/滤波器的分数精度，这会加倍计算并需要额外的微调步骤；Shen等（2020）提出在推理过程中以输入为依赖的方式调整每一层的精度，以平衡计算成本和精度。</p><h2 id="3-The-Proposed-CPT-Technique"><a href="#3-The-Proposed-CPT-Technique" class="headerlink" title="3 The Proposed CPT Technique"></a>3 The Proposed CPT Technique</h2><p>在本节中，我们首先在第3.1节中通过可视化示例介绍推动我们开发CPT的假设，然后在第3.2节中提出CPT概念，接着在第3.3节介绍精度范围测试（PRT）方法，其中PRT旨在自动化CPT的精度调度。</p><p><img src="/2024/07/10/papers-paper03-cpt/cpt01.png" alt="表1: 在 CIFAR-100 上训练的 ResNet-38/74 在第一阶段使用不同学习率和精度组合的测试准确性。注意，所有实验的最后两个阶段均以全精度和分别为 0.01 和 0.001 的学习率进行训练。"></p><h3 id="3-1-CPT-Motivation"><a href="#3-1-CPT-Motivation" class="headerlink" title="3.1 CPT: Motivation"></a>3.1 CPT: Motivation</h3><p><strong>假设1：DNN的精度与学习率有类似的效果</strong>。现有研究（Grandvalet等，1997；Neelakantan等，2015）理论上或经验上表明噪声有助于DNN训练，这促使我们重新思考量化在DNN训练中的作用。我们推测，低精度和高量化噪声有助于DNN训练的探索，类似于高学习率的效果，而高精度和更准确的更新有助于模型收敛，类似于低学习率。</p><p><strong>验证假设1</strong>。<br><u>设置</u>：为了经验性地验证我们的假设，我们在CIFAR-100数据集上训练ResNet-38/74共160个epoch，训练设置如第4.1节所述。特别是，我们将160个epoch的训练分为三个阶段：[0, 80]， [80, 120] 和 [120, 160]。在第一个训练阶段[0, 80]，我们为权重和激活采用不同的学习率和精度，而在剩下的两个阶段中使用全精度，学习率为[80, 120] epoch期间为0.01，[120, 160] epoch期间为0.001，以探索在第一个训练阶段中学习率和精度之间的关系。</p><p><u>结果</u>：如表1所示，我们可以观察到，当第一个训练阶段的学习率足够降低时，采用较低精度进行训练将比使用全精度训练获得更高的准确性。特别是，在标准初始学习率为0.1的情况下，全精度训练在ResNet-38/74上分别比4位精度的训练高出1.00%/0.70%的准确性；而当初始学习率降低时，这个准确性差距逐渐缩小并反转，例如，当初始学习率降到1e-2时，在[0, 80]epoch使用4位精度训练比全精度训练高出1.40%/1.57%的准确性。</p><p><u>见解</u>：这组实验表明（1）当初始学习率较低时，使用较低初始精度进行训练始终比使用全精度训练获得更好的准确性，表明降低精度引入了类似于高学习率的有利于探索的效果；（2）虽然低精度可以减轻由低学习率引起的准确性下降，但高学习率通常是最大化准确性的必要条件。</p><p><img src="/2024/07/10/papers-paper03-cpt/cpt02.png" alt="图1: 测试准确率ofResNet74 on CIFAR-100"><br><strong>假设2：动态精度有助于DNN泛化</strong>。最近在DNN训练中的发现促使我们更好地利用DNN精度，实现DNN准确性和效率的双赢。具体来说，有讨论指出（1）DNN在不同的训练阶段学习不同的模式，例如，Rahaman等（2019）和Xu等（2019）揭示了DNN训练首先学习低频成分，然后是高频特征，前者对扰动和噪声更为鲁棒；（2）动态学习率调度有助于改善DNN训练中的优化，例如，Li等（2019）指出，大的初始学习率有助于模型记住更易适应和更具普遍性的模式，而Smith（2017）和Loshchilov &amp; Hutter（2016）表明，循环学习率调度提高了DNN的分类准确性。这些工作激发我们假设，动态精度可能有助于DNN在优化景观中达到更好的最优解，特别是考虑到在假设1中验证的学习率和精度之间的相似效果。</p><p><strong>验证假设2</strong>。我们的假设2已通过各种实证观察得到一致确认。例如，最近的一项工作（Fu等，2020）提出在训练过程中逐步提高精度，我们按照他们的设置验证我们的假设。</p><p><u>设置</u>：我们在CIFAR-100上训练ResNet-74，训练设置与Wang等（2018b）相同，但在训练过程中量化权重、激活和梯度；对于逐步精度情况，我们在前80个epoch中将权重和激活的精度从3位均匀增加到8位，并采用静态8位梯度，而静态精度基线对所有权重/激活/梯度使用8位。</p><p><img src="/2024/07/10/papers-paper03-cpt/cpt03.png" alt="图2:在不同精度调度下训练的CIFAR-100上ResNet-74收敛后的损失景观可视化,其中更宽的轮廓和更大的间隔表示更好的局部最小值和更低的泛化误差."><br><u>结果</u>：图1显示，使用逐步精度调度训练的准确性略高于静态精度（+0.3%），同时前者可以降低训练成本。此外，我们在图2(b)中可视化了损失景观（按照Li等（2018）的方法）：有趣的是，逐步精度调度有助于收敛到具有更宽轮廓的更好局部极小值，这表明图2(a)中的静态8位基线具有较低的泛化误差（Li等，2018）。</p><p>Fu等（2020）中的逐步精度调度依赖于手动超参数调节。因此，一个自然的后续问题是：什么样的动态调度在不同任务/模型中既有效又易于实施？在本文中，我们表明简单的循环调度在提高训练效率的同时，始终有利于训练收敛。</p><p><img src="/2024/07/10/papers-paper03-cpt/cpt04.png" alt="图3：静态精度训练与循环精度训练（CPT）对比，其中CPT在训练过程中循环调度权重和激活的精度。"></p><h3 id="3-2-CPT-The-Key-Concept"><a href="#3-2-CPT-The-Key-Concept" class="headerlink" title="3.2 CPT: The Key Concept"></a>3.2 CPT: The Key Concept</h3><p>CPT 的关键概念受到 Li 等人 (2019) 的启发，后者表明较大的初始学习率有助于模型学习更具普遍性的模式。因此，我们假设较低的精度虽然会导致短期内较差的准确性，但由于其相关的较大量化噪声，实际上可能有助于 DNN 训练期间的探索；同时众所周知，较高的精度可以学习更高复杂度和细粒度的模式，这对于更好的收敛至关重要。两者结合可能会改善最终的准确性，因为它可能在 DNN 训练过程中更好地平衡粗粒度探索和细粒度优化，这就是 CPT 的理念。<br>具体而言，如图 3 所示，CPT 在两个边界之间循环变化精度，而不是在训练过程中固定精度，从而让模型以不同的粒度探索优化景观。</p><p>虽然 CPT 可以使用不同的循环调度方法实现，这里我们以余弦方式的实现作为示例：</p><script type="math/tex; mode=display">B_t^n = \lceil B_{min}^n + \frac{1}{2} (B_{max}^n - B_{min}^n)(1-cos(\frac{t \% T_n}{T_n} \pi)) \rfloor</script><p>其中$B_{min}^n$ $B_{max}^n$分别是第n个周期精度调度中的下限和上限精度，⌈⋅⌋ 和 $\%$ 分别表示取整运算和取余运算，$B_{tn}$ 是在第t个全局epoch中第n个周期的精度，周期长度为 $T_n$。注意，周期长度 $T_n$ 等于总训练epoch数除以周期总数 $N$，其中 $N$ 是CPT的超参数。例如，如果 $N = 2$，那么在训练过程中，使用CPT的DNN训练将经历两个循环精度调度周期。如第4.3节所示，我们发现采用不同数量的循环精度调度周期进行训练时，CPT的好处是保持不变的，即CPT对 $N$ 不敏感。精度调度的可视化示例见附录A。此外，我们发现使用不同的动态精度调度模式（即，不一定是公式(1)中的余弦调度）时，CPT通常是有效的。在本工作中，我们按照公式(1)实现CPT，并在第4.3节讨论了可能的变体。</p><p>我们在图1中可视化了CPT在ResNet-74上的训练曲线（使用CIFAR-100数据集），发现与静态固定精度的对应方法相比，CPT提高了0.91%的准确性，并减少了36.7%的所需训练BitOPs（位操作）。此外，图2 (c) 可视化了相应的损失景观，显示了CPT的有效性，即这种简单且自动化的精度调度导致了更好的收敛和更低的尖锐度。</p><p><img src="/2024/07/10/papers-paper03-cpt/cpt05.png" alt="图4：展示了 ResNet-152 和 MobileNetV2 在 CIFAR-100 上的精度范围测试，其中超过预设阈值的切换点用红色圆圈表示。"></p><h3 id="3-3-CPT-Precision-Range-Test"><a href="#3-3-CPT-Precision-Range-Test" class="headerlink" title="3.3 CPT: Precision Range Test"></a>3.3 CPT: Precision Range Test</h3><p>CPT 的概念足够简单，可以插入到任何模型或任务中以提高训练效率。一个剩下的问题是如何确定精度边界，即公式(1)中的 $B^i_{\text{min}}$ 和 $B^i_{\text{max}}$，我们发现可以在精度调度的第一个周期（即 $T_i = T_0$）中使用简单的PRT以几乎可忽略的计算成本自动决定。具体来说，PRT 从最低可能的精度（例如2位）开始，并逐渐增加精度，同时监控几个连续迭代中训练准确性幅度的变化；一旦这种训练准确性差异超过预设阈值，表明训练至少部分收敛，PRT 将确认下限精度已确定。虽然上限精度也可以类似地确定，但存在一种替代方法，建议简单采用CPT的静态精度对应方法的精度。其余周期使用相同的精度边界。</p><p>图4 可视化了在 CIFAR-100 上训练的 ResNet-152/MobileNetV2 的 PRT。我们可以看到，当模型在 ResNet-152 上经历显著的训练准确性提高时确定的最低精度边界是3位，而 MobileNetV2 是4位，这与常见观察一致，即 ResNet-152 对量化的鲁棒性比更紧凑的模型 MobileNetV2 更强。</p><h2 id="4-Experiment-Results"><a href="#4-Experiment-Results" class="headerlink" title="4 Experiment Results"></a>4 Experiment Results</h2><p>在本节中，我们将首先在第4.1节描述实验设置，在第4.2节展示在各种任务中基准测试SOTA训练方法的结果，然后在第4.3节进行CPT的全面消融研究。</p><h3 id="4-1-Experiment-Setup"><a href="#4-1-Experiment-Setup" class="headerlink" title="4.1 Experiment Setup"></a>4.1 Experiment Setup</h3><p><strong>模型、数据集和基线</strong>。我们考虑了十一种模型（包括八种基于ResNet的模型（He等，2016）、MobileNetV2（Sandler等，2018）、Transformer（Vaswani等，2017）和LSTM（Hochreiter &amp; Schmidhuber，1997））和五个任务（包括CIFAR-10/100（Krizhevsky等，2009）、ImageNet（Deng等，2009）、WikiText-103（Merity等，2016）和Penn Treebank（PTB）（Marcus等，1993））。具体来说，我们按照Wang等（2019b）的方法在CIFAR-10/100上实现MobileNetV2。</p><p><strong>基线</strong>：我们首先在三种SOTA静态低精度训练技术上对CPT进行了基准测试：SBM（Banner等，2018）、DoReFa（Zhou等，2016）和WAGEUBN（Yang等，2020），每种技术都采用了不同的量化器。由于SBM在报告的结果和我们的实验结果中都是最具竞争力的基线，因此我们在SBM的基础上应用了CPT，除非特别说明，所有的静态精度基线都采用SBM量化器。另一个基线是在静态精度训练的基础上采用的循环学习率（CLR）（Loshchilov &amp; Hutter，2016），我们遵循了Loshchilov &amp; Hutter（2016）中的最佳设置。</p><p><strong>训练设置</strong>：我们在所有实验中都遵循标准的训练设置。特别是，对于分类任务，我们分别按照Wang等（2018b）在CIFAR-10/100上的SOTA设置和He等（2016）在ImageNet实验中的设置进行；对于语言建模任务，我们遵循Baevski &amp; Auli（2018）在WikiText-103上使用Transformer的方法和Merity等（2017）在PTB上使用LSTM的方法。</p><p><strong>精度设置</strong>：所有实验中的低精度边界使用第3.3节中的PRT设置，上限精度与相应静态精度基线的精度相同。我们仅将CPT应用于权重和激活（统称为FW），而误差和梯度使用静态精度（统称为BW），后者是为了确保梯度的稳定性（Wang等，2018a）（更多讨论见附录C）。特别地，CPT从3位到8位的权重和激活，使用8位梯度，记为FW(3,8)/BW8。所有实验中的周期性精度循环总数，即第3.3节中的N，固定为32（见第4.3节中的消融研究）。</p><p><strong>硬件设置和指标</strong>：为了验证所提出的CPT的实际硬件效率，我们采用了标准的FPGA实现流程。具体来说，我们使用Vivado HLx设计流程在名为ZC706（Xilinx）的Xilinx开发板上实现基于FPGA的加速器。为了更好地评估训练成本，我们考虑了计算的GBitOPs（Giga bit operations）和在ZC706 FPGA板上的实际测量延迟。</p><h3 id="4-2-Benchmark-with-SOTA-Static-Precision-Training-Methods"><a href="#4-2-Benchmark-with-SOTA-Static-Precision-Training-Methods" class="headerlink" title="4.2 Benchmark with SOTA Static Precision Training Methods"></a>4.2 Benchmark with SOTA Static Precision Training Methods</h3><p><strong>CIFAR-10/100上的基准测试</strong>。基于SOTA量化器的基准测试：我们在训练ResNet-74/164和MobileNetV2于CIFAR-10/100时，使用三种SOTA静态低精度训练方法对CPT进行基准测试，如表2所总结的，涵盖了代表性低精度DNN训练中深度和紧凑的DNN。需要注意的是，精度提升是指在相同设置下CPT与最强基线之间的差异。表2显示，（1）我们的CPT在CIFAR-10/100的所有情况下，始终实现了更高的精度（+0.19%~+1.25%）和更低的训练成本（-21.0%~ -37.1%的计算成本和-14.7%~ -21.4%的延迟），即使是紧凑模型MobileNetV2，（2）在极低精度下，CPT在精度方面显著优于基线，这是低精度DNN训练中最有用的场景之一。特别是，CPT在4位和6位之间的周期性精度训练，相比于CIFAR-10上的静态6位训练，在ResNet-74/164上分别提高了1.25%和1.07%的精度，验证了CPT导致了更好的收敛。<br><img src="/2024/07/10/papers-paper03-cpt/cpt06.png" alt="图5：在 CIFAR-100 上训练 ResNet-38/74/110/152/164 和 MobileNetV2 时，使用静态精度、静态精度加CLR以及CPT方法的测试准确性与所需GBitOPs的关系。"><br>基于SBM之上的CLR基准测试：我们进一步在SBM之上用CLR（Loshchilov &amp; Hutter, 2016）对CPT进行基准测试（继承其在CIFAR-100上的最佳设置），如表2所示，SBM在SOTA量化器中表现最好，如图5所示，基于更多的DNN模型和精度。我们可以看到，（1）CPT仍然始终以更好的精度和效率权衡优于所有基线，（2）SBM之上的CLR对测试精度产生负面影响，我们推测这是由于低精度训练下梯度的不稳定性和对学习率的敏感性（Wang等，2018a所讨论），显示CPT比CLR更适用于低精度训练。<br><img src="/2024/07/10/papers-paper03-cpt/cpt07.png" alt="表2：CPT、DoReFa（Zhou et al., 2016）、WAGEUBN（Yang et al., 2020）和SBM（Banner et al., 2018）在CIFAR-10/100上训练ResNet-74/164和MobileNetV2模型的测试准确性、计算成本和延迟。"></p><p>损失景观可视化：为了更好地理解CPT实现的卓越性能，我们按照Li等（2018）的方法可视化了损失景观，如图6所示，涵盖了非紧凑和紧凑模型以及低精度设置（即6位和8位），这是低精度DNN训练的瓶颈。我们可以观察到，使用CPT训练的标准和紧凑DNN都经历了更宽的轮廓和更少的尖锐度，表明CPT有助于DNN训练优化收敛到更好的局部最优。<br><img src="/2024/07/10/papers-paper03-cpt/cpt08.png" alt="图6：在 CIFAR-100 上训练的 ResNet-110 和 MobileNetV2 的损失景观可视化。"></p><p><strong>ImageNet上的基准测试</strong>。为了验证CPT在更复杂任务和更大模型上的可扩展性，我们在ImageNet上的ResNet-18/34/50上使用SOTA静态精度训练方法SBM（Banner等，2018）进行基准测试，在这种情况下，低精度训练如4位的工作具有挑战性。如表3所示，我们可以观察到CPT仍然在计算成本减少的情况下（最多-30.4%）实现了可比的精度（-0.20%~+0.06%）。特别是，CPT在ResNet-50上表现良好，既提高了精度，又提高了训练效率，表明CPT在模型复杂性上的可扩展性，因此除了设备上的训练场景外，它还有潜在的应用于大规模训练的可能性。<br><img src="/2024/07/10/papers-paper03-cpt/cpt09.png" alt="表3：使用提出的CPT和SBM（Banner et al., 2018）在ImageNet上训练的ResNet-18/34/50的测试准确性和计算成本。"></p><p><strong>CPT提升精度</strong>：CPT的一个重要方面是其在提高训练优化方面的潜力。我们通过在ImageNet上使用CPT和静态全精度训练ResNet-18/34来说明CPT在提高最终精度方面的优势。如表4所示，CPT在ResNet-18/34上分别比其全精度对应模型在ImageNet上提高了0.91%/0.84%的精度，表明CPT不仅可以作为提高训练效率的一般技术，还可以提高最终精度。</p><p><strong>WikiText-103和PTB上的基准测试</strong>。我们还将CPT应用于语言建模任务（包括WikiText-103和PTB）（见表5），以表明CPT也适用于自然语言处理模型。表5显示，（1）CPT在精度（即困惑度——越低越好）和训练效率方面始终实现了双赢，（2）语言建模模型/任务对量化更敏感，尤其是在LSTM模型中，因为它总是适应较大的低精度边界，这与SOTA的观察一致（Hou等，2019）。</p><h3 id="4-3-Ablation-Studies-of-CPT"><a href="#4-3-Ablation-Studies-of-CPT" class="headerlink" title="4.3 Ablation Studies of CPT"></a>4.3 Ablation Studies of CPT</h3><p><strong>CPT 在不同精度范围内的表现</strong>。我们还评估了CPT在不同的上限精度范围内的表现，这些范围对应于不同的目标效率，以查看CPT是否仍然表现良好。图7展示了每次实验重复十次的箱线图。我们可以看到，（1）无论采用何种精度范围，CPT始终实现了双赢（+0.74% ∼ +2.03%的更高准确性和-18.3% ∼ -48.0%的计算成本减少），特别是在低精度场景下，（2）CPT甚至缩小了准确性方差，这更符合高效训练的实际目标。</p><p><strong>CPT 在不同精度调度周期数下的表现</strong>。为了评估CPT对采用的循环精度调度周期总数的敏感性，我们在不同的调度周期数下将CPT应用于ResNet-38/74和CIFAR-100。图8展示了基于十次重复实验的平均准确性。我们可以看到，（1）不同的周期总数选择在CIFAR-10/100上导致了可比的准确性（在0.5%以内）；（2）CPT在不同精度调度周期数下在达到的准确性上始终优于静态基线SBM。基于这个实验，我们为简化起见，将 $N = 32$ 设为固定值。</p><p><strong>CPT 在不同循环精度调度模式下的表现</strong>。除了余弦调度模式外，我们还使用其他循环精度调度模式评估了CPT，包括由Smith（2017）提出的三角形调度模式和作为学习率调度的余弦退火调度（Loshchilov &amp; Hutter，2016），所有这些模式都采用32个循环周期以保持公平。表6中的实验表明，（1）CPT在不同调度模式下始终有效；（2）在某些情况下，CPT使用其他两个调度模式甚至超越了余弦调度模式，但在紧凑模型上表现不佳。如何为给定模型和任务确定最佳的循环模式留作未来的研究工作。</p><p><img src="/2024/07/10/papers-paper03-cpt/cpt10.png" alt="表6：在 CIFAR-100 上对 ResNet-74/164 和 MobileNetV2 进行循环精度训练的不同精度调度下的 CPT。余弦（CPT）是目前 CPT 采用的调度方案。"></p><h2 id="5-Discussions-about-Future-Work"><a href="#5-Discussions-about-Future-Work" class="headerlink" title="5 Discussions about Future Work"></a>5 Discussions about Future Work</h2><p><strong>CPT 的理论视角</strong>。近年来，对理解和优化 DNN 训练的兴趣日益浓厚。例如，Li 等人（2019）显示，使用较大的初始学习率训练 DNN 有助于模型更快更好地记住更具普遍性的模式。最近，Zhu 等人（2020）显示，在凸性假设下，低精度 DNN 训练的收敛界由量化噪声和学习率的线性组合决定。关于 DNN 训练的这些发现似乎与我们 CPT 的有效性一致。</p><p><strong>CPT 的硬件支持</strong>。最近在支持动态精度的混合精度 DNN 加速器（Lee 等，2019；Kim 等，2020）中的进展有望支持我们的 CPT。我们将 CPT 的最佳加速器设计留作未来的研究工作。</p><h2 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6 Conclusion"></a>6 Conclusion</h2><p>我们假设，在 DNN 训练中，DNN 的精度具有与学习率类似的效果，即低精度和高量化噪声有助于 DNN 训练的探索，而高精度和更准确的更新有助于模型收敛，因此主张动态精度调度有助于 DNN 训练优化。然后，我们提出了 CPT 框架，该框架采用周期性精度调度进行低精度 DNN 训练，以提升任务准确性和训练效率的可实现帕累托前沿。大量实验和消融研究验证了 CPT 可以在训练期间降低计算成本，同时实现可比或更高的准确性。我们的未来工作将努力找到更多这种动态低精度训练的理论依据。</p>]]></content>
    
    
    <categories>
      
      <category>论文精读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文精读</tag>
      
      <tag>模型量化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LoRA论文精读</title>
    <link href="/2024/07/09/papers-paper02-lora/"/>
    <url>/2024/07/09/papers-paper02-lora/</url>
    
    <content type="html"><![CDATA[<h1 id="LoRA论文精读"><a href="#LoRA论文精读" class="headerlink" title="LoRA论文精读"></a>LoRA论文精读</h1><p>我们详细看看这篇论文：<a href="https://arxiv.org/pdf/2106.09685">Low-Rank Adaptation of Large Language Models</a></p><p>这篇论文是2021年十月由Microsoft发表的。</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>自然语言处理的重要范式包括在通用领域数据上进行大规模预训练，并适应特定任务或领域。随着我们预训练更大的模型，全量微调（重新训练所有模型参数）变得越来越不可行。以GPT-3 175B为例——部署每个具有175B参数的独立微调模型实例成本高昂。我们提出了低秩适应（Low-Rank Adaptation，简称LoRA），该方法冻结预训练模型的权重，并在Transformer架构的每一层中注入可训练的秩分解矩阵，从而大大减少下游任务的可训练参数数量。与使用Adam进行微调的GPT-3 175B相比，LoRA可以减少可训练参数数量达10,000倍，并将GPU内存需求减少3倍。尽管LoRA的可训练参数更少，但在RoBERTa、DeBERTa、GPT-2和GPT-3上的模型质量与微调相当或更好，同时具有更高的训练吞吐量，并且不像适配器那样没有额外的推理延迟。我们还提供了一项关于语言模型适应中秩缺乏的实证研究，这揭示了LoRA的有效性。我们发布了一个包，便于将LoRA集成到PyTorch模型中，并在<a href="https://github.com/microsoft/LoRA">github-lora</a>提供了我们的实现和RoBERTa、DeBERTa和GPT-2的模型检查点。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p><img src="/2024/07/09/papers-paper02-lora/lora01.png" alt="图1: 我们的重新参数化，只训练A和B"><br>自然语言处理的许多应用依赖于将一个大规模的预训练语言模型适应于多个下游应用。这样的适应通常通过微调来完成，这会更新预训练模型的所有参数。微调的主要缺点是新模型包含的参数数量与原始模型一样多。随着每隔几个月就训练出更大的模型，这从对GPT-2（Radford等）或RoBERTa large（Liu等，2019）来说的一个“小不便”变成了对拥有1750亿可训练参数的GPT-3（Brown等，2020）来说的一个关键部署挑战。</p><p>许多人试图通过只适应部分参数或为新任务学习外部模块来缓解这个问题。通过这种方式，我们只需要在预训练模型的基础上为每个任务存储和加载少量任务特定的参数，从而大大提高部署时的运行效率。然而，现有技术通常通过增加模型深度引入推理延迟（Houlsby等，2019；Rebuffi等，2017）或减少模型的可用序列长度（Li &amp; Liang，2021；Lester等，2021；Hambardzumyan等，2020；Liu等，2021）（第3节）。更重要的是，这些方法往往无法达到微调基线的性能，在效率和模型质量之间形成权衡。</p><p>我们受到Li等（2018a）和Aghajanyan等（2020）的启发，这些研究表明，学习到的过参数化模型实际上位于一个低内在维度上。我们假设在模型适应过程中权重的变化也具有低“内在秩”，这引导我们提出了低秩适应（LoRA）方法。LoRA允许我们通过优化适应过程中密集层变化的秩分解矩阵来间接训练神经网络中的一些密集层，同时保持预训练权重不变，如图1所示。以GPT-3 175B为例，我们展示了即使在全秩（即图1中的d）高达12,288时，一个非常低的秩（即图1中的r可以是1或2）也足够，使得LoRA在存储和计算上都很高效。</p><p>LoRA具有几个关键优势：</p><ul><li><strong>共享预训练模型</strong>：一个预训练模型可以用于构建许多针对不同任务的小型LoRA模块。我们可以冻结共享模型，并通过替换图1中的矩阵A和B来高效地切换任务，从而显著减少存储需求和任务切换的开销。</li><li><strong>提高训练效率</strong>：使用自适应优化器时，LoRA使训练更加高效，并将硬件门槛降低多达3倍，因为我们不需要计算大多数参数的梯度或维护优化器状态。相反，我们只优化注入的、更小的低秩矩阵。</li><li><strong>无推理延迟</strong>：我们的简单线性设计允许在部署时将可训练矩阵与冻结的权重合并，与完全微调模型相比，不引入任何推理延迟。</li><li><strong>方法兼容性</strong>：LoRA与许多现有方法是正交的，可以与许多方法结合使用，例如前缀调优。我们在附录E中提供了一个示例。</li></ul><p><strong>术语和惯例</strong><br>我们频繁引用Transformer架构，并使用其维度的常规术语。我们将Transformer层的输入和输出维度大小称为 $d_{\text{model}}$。我们使用$W_q$、$W_k$、$W_v$和$W_o$来指代自注意力模块中的查询、键、值和输出投影矩阵。$W$或$W_0$指预训练权重矩阵，$ \Delta W$是适应过程中累积的梯度更新。我们用$r$表示LoRA模块的秩。我们遵循（Vaswani等，2017；Brown等，2020）提出的惯例，使用Adam（Loshchilov &amp; Hutter, 2019；Kingma &amp; Ba, 2017）进行模型优化，并使用Transformer MLP前馈维度 $d_{\text{ffn}} = 4 \times d_{\text{model}}$。</p><h2 id="2-Problem-Statement"><a href="#2-Problem-Statement" class="headerlink" title="2 Problem Statement"></a>2 Problem Statement</h2><p>虽然我们的提议与训练目标无关，但我们专注于语言建模作为我们的动机用例。以下是语言建模问题的简要描述，特别是给定任务特定提示时条件概率的最大化。</p><p>假设我们有一个由参数$\Phi$表示的预训练自回归语言模型 $P_\Phi(y|x)$。例如，$P_\Phi(y|x)$ 可以是基于Transformer架构（Vaswani等，2017）的通用多任务学习器，如GPT（Radford等，b；Brown等，2020）。考虑将这个预训练模型适应于下游条件文本生成任务，例如摘要、机器阅读理解（MRC）和自然语言到SQL（NL2SQL）。每个下游任务由上下文-目标对的训练数据集表示：$Z = \{(x_i, y_i)\}_{i=1,..,N}$，其中 $x_i$ 和 $y_i$ 都是令牌序列。例如，在NL2SQL中，$x_i$ 是自然语言查询，$y_i$ 是其对应的SQL命令；对于摘要生成，$x_i$ 是文章内容，$y_i$ 是其摘要。</p><p>在全量微调过程中，模型被初始化为预训练权重 $\Phi_0$ 并通过反复沿着梯度更新为 $\Phi_0 + \Delta\Phi$ 来最大化条件语言建模目标：</p><script type="math/tex; mode=display">max_{\Phi} \sum_{(x,y) \in Z} \sum_{t=1}^{|y|} \log (P_{\Phi}(y_t | x, y_{ < t}))</script><p>全量微调的主要缺点之一是，对于每个下游任务，我们学习一组不同的参数 $\Delta\Phi$，其维度 $|\Delta\Phi|$ 等于 $|\Phi_0|$。因此，如果预训练模型很大（例如GPT-3，其 $|\Phi_0| \approx 175 十亿），存储和部署许多独立的微调模型实例可能会非常具有挑战性，甚至难以实现。</p><p>在本文中，我们采用了一种更高效的参数方法，其中任务特定的参数增量 $\Delta\Phi = \Delta\Phi(\Theta)$ 由一组更小的参数 $\Theta$ 编码，其维度 $|\Theta| \ll |\Phi_0|$。因此，寻找 $\Delta\Phi$ 的任务变成了对 $\Theta$ 进行优化：</p><script type="math/tex; mode=display">max_{\Theta} \sum_{(x,y) \in Z} \sum_{t=1}^{|y|} \log (P_{\Phi_{0} + \Delta \Theta (\Phi)}(y_t | x, y_{ < t}))</script><p>在随后的章节中，我们提出使用低秩表示(low-rank representation)来编码 $\Delta\Phi$，这在计算和内存方面都很高效。当预训练模型为 GPT-3 175B 时，可训练参数 $|\Theta|$ 可以小到 $|\Phi_0|$ 的 0.01%。</p><h2 id="3-Aren’t-Existing-Solutions-Good-Enough"><a href="#3-Aren’t-Existing-Solutions-Good-Enough" class="headerlink" title="3 Aren’t Existing Solutions Good Enough?"></a>3 Aren’t Existing Solutions Good Enough?</h2><p>我们要解决的问题并不新颖。自迁移学习诞生以来，已经有几十项工作致力于使模型适应更高效的参数和计算。请参见第6节了解一些著名的工作。以语言建模为例，在高效适应方面有两种突出的策略：添加适配器层(adapter layers)（Houlsby等，2019；Rebuffi等，2017；Pfeiffer等，2021；Rücklé等，2020）或优化某种形式的输入层激活（Li &amp; Liang，2021；Lester等，2021；Hambardzumyan等，2020；Liu等，2021）。然而，这两种策略都有其局限性，特别是在大规模和对延迟敏感的生产场景中。</p><p><strong>适配器层引入推理延迟</strong></p><p>适配器(Adapter)有许多变体。我们关注的是Houlsby等（2019）设计的每个Transformer块有两个适配器层的原始设计，以及Lin等（2020）设计的每个块只有一个适配器层但有额外LayerNorm（Ba等，2016）的新设计。虽然可以通过修剪层或利用多任务设置（Rücklé等，2020；Pfeiffer等，2021）来减少整体延迟，但没有直接的方法绕过适配器层中的额外计算。这似乎不是一个问题，因为适配器层设计成具有较少的参数（有时&lt;1% 的原始模型），通过有一个小瓶颈维度来限制它们可以添加的FLOPs。然而，大型神经网络依赖于硬件并行性来保持低延迟，而适配器层必须顺序处理。这在批量大小通常只有一个的在线推理设置中有所不同。在没有模型并行性的通用场景中，例如在单个GPU上运行GPT-2（Radford等）中型模型的推理时，即使使用非常小的瓶颈维度（表1），我们也会看到延迟显著增加。</p><p>当我们需要像Shoeybi等（2020）和Lepikhin等（2020）那样对模型进行分片时，这个问题会变得更糟，因为额外的深度需要更多的同步GPU操作，如AllReduce和Broadcast，除非我们将适配器参数冗余存储多次。</p><p><strong>直接优化提示是困难的</strong></p><p>另一个方向，如前缀调优（Li &amp; Liang，2021）所示，面临不同的挑战。我们观察到前缀调优难以优化，其性能在可训练参数中非单调变化，验证了原始论文中的类似观察。从根本上说，保留部分序列长度进行适应必然会减少可用于处理下游任务的序列长度，我们怀疑这使得调优提示的性能不如其他方法。我们将任务性能的研究推迟到第5节。</p><p><img src="/2024/07/09/papers-paper02-lora/lora02.png" alt="表1：在GPT-2 medium中一次前向传递的推理延迟，单位为毫秒，取100次试验的平均值。我们使用NVIDIA Quadro RTX8000。“|Θ|”表示适配器层中的可训练参数数量。AdapterL和AdapterH是两种适配器调优的变体，我们在第5.1节中描述。在在线、短序列长度的场景中，适配器层引入的推理延迟可能是显著的。完整的研究见附录B."></p><h2 id="4-Our-Method"><a href="#4-Our-Method" class="headerlink" title="4 Our Method"></a>4 Our Method</h2><p>我们描述了LoRA的简单设计及其实际收益。这里概述的原则适用于深度学习模型中的任何密集层，尽管我们在实验中仅专注于Transformer语言模型中的某些权重，作为激励用例。</p><h3 id="4-1-Low-Rank-Parameterized-Update-Matrices"><a href="#4-1-Low-Rank-Parameterized-Update-Matrices" class="headerlink" title="4.1 Low-Rank-Parameterized Update Matrices"></a>4.1 Low-Rank-Parameterized Update Matrices</h3><p>一个神经网络包含许多执行矩阵乘法的密集层。这些层中的权重矩阵通常具有全秩(full-rank)。Aghajanyan等（2020）表明，当适应特定任务时，预训练语言模型具有低“内在维度”，即使随机投影到较小的子空间中仍然可以有效学习。受此启发，我们假设在适应过程中权重的更新也具有低“内在秩”。对于预训练权重矩阵 $W_0 \in \mathbb{R}^{d \times k}$，我们通过表示其更新为低秩分解 $W_0 + \Delta W = W_0 + BA$ 来约束其更新，其中 $B \in \mathbb{R}^{d \times r}$、$A \in \mathbb{R}^{r \times k}$，且秩 $r \ll \min(d, k)$。在训练过程中，$W_0$ 是冻结的，不接收梯度更新，而 $A$ 和 $B$ 包含可训练参数。注意，$W_0$ 和 $\Delta W = BA$ 都与相同的输入相乘，它们各自的输出向量按坐标相加。对于 $h = W_0 x$，我们修改后的前向传递得到：</p><script type="math/tex; mode=display">h = W_0 x + \Delta W x = W_0 x + BA x</script><p>我们在图1中展示了我们的重新参数化。我们使用随机高斯初始化 $A$ 并将 $B$ 初始化为零，因此 $\Delta W = BA$ 在训练开始时为零。然后我们将 $\Delta W x$ 乘以 $\alpha / r$，其中 $\alpha$ 是 $r$ 中的一个常数。当使用Adam优化时，如果我们适当地缩放初始化，调整 $\alpha$ 大致相当于调整学习率。因此，我们简单地将 $\alpha$ 设置为我们尝试的第一个 $r$ 并且不调整它。这种缩放有助于减少在改变 $r$ 时重新调整超参数的需求（Yang &amp; Hu，2021）。</p><p><strong>全量微调的推广形式</strong>。更一般的微调形式允许训练预训练参数的子集。LoRA更进一步，不要求在适应过程中对权重矩阵的累积梯度更新具有全秩。这意味着，当将LoRA应用于所有权重矩阵并训练所有偏置时，通过将LoRA的秩 $r$ 设置为预训练权重矩阵的秩，我们大致可以恢复全量微调的表达能力。换句话说，随着可训练参数数量的增加，训练LoRA大致收敛于训练原始模型，而基于适配器的方法收敛于一个MLP，基于前缀的方法则收敛于无法处理长输入序列的模型。</p><p><strong>无额外推理延迟</strong>。在生产环境中部署时，我们可以显式计算并存储 $W = W_0 + BA$，并像往常一样进行推理。注意，$W_0$ 和 $BA$ 都在 $\mathbb{R}^{d \times k}$ 中。当我们需要切换到另一个下游任务时，可以通过减去 $BA$ 并添加不同的 $B’A’$ 来恢复 $W_0$，这是一个非常快速的操作，几乎没有内存开销。关键是，这保证了与构建微调模型相比，我们在推理过程中不会引入任何额外的延迟。</p><h3 id="4-2-Applying-LoRA-to-Transformer"><a href="#4-2-Applying-LoRA-to-Transformer" class="headerlink" title="4.2 Applying LoRA to Transformer"></a>4.2 Applying LoRA to Transformer</h3><p>原则上，我们可以将LoRA应用于神经网络中任意子集的权重矩阵，以减少可训练参数的数量。在Transformer架构中，自注意力模块有四个权重矩阵（$W_q$、$W_k$、$W_v$、$W_o$）和MLP模块中有两个权重矩阵。尽管输出维度通常被切片为注意力头，我们将 $W_q$（或 $W_k$、$W_v$）视为维度为 $d_{\text{model}} \times d_{\text{model}}$ 的单个矩阵。为了简化和提高参数效率，我们仅限于适应下游任务的注意力权重，并冻结MLP模块（因此在下游任务中不训练它们）。我们在第7.1节进一步研究了适应Transformer中不同类型注意力权重矩阵的效果。我们将MLP层、LayerNorm层和偏置适应的实证研究留待未来工作。</p><p><strong>实际收益和限制</strong></p><p>最显著的好处来自内存和存储使用量的减少。对于使用Adam训练的大型Transformer，如果 $r \ll d_{\text{model}}$，由于不需要存储冻结参数的优化器状态，我们将VRAM使用量减少了多达2/3。在GPT-3 175B上，我们将训练期间的VRAM消耗从1.2TB减少到350GB。使用 $r = 4$ 并且仅适应查询和值投影矩阵时，检查点大小减少了约10,000倍（从350GB到35MB）4。这使我们可以使用显著更少的GPU进行训练并避免I/O瓶颈。另一个好处是，在部署时仅通过交换LoRA权重而不是所有参数，可以以更低的成本在任务之间切换。这使我们能够创建许多定制模型，并能在存储预训练权重的机器上即时切换。我们还观察到，在GPT-3 175B上，与全量微调相比，训练速度提高了25%5，因为我们不需要计算绝大多数参数的梯度。</p><p>LoRA也有其局限性。例如，如果选择将 $A$ 和 $B$ 吸收进 $W$ 以消除额外的推理延迟，那么在单次前向传递中批处理具有不同 $A$ 和 $B$ 的任务输入并不简单。尽管在延迟不关键的情况下，仍然可以不合并权重，并动态选择批处理中的LoRA模块。</p><h2 id="5-Empirical-Experiments"><a href="#5-Empirical-Experiments" class="headerlink" title="5 Empirical Experiments"></a>5 Empirical Experiments</h2><p>我们在RoBERTa（Liu等，2019）、DeBERTa（He等，2021）和GPT-2（Radford等，b）上评估了LoRA在下游任务中的表现，然后扩展到GPT-3 175B（Brown等，2020）。我们的实验涵盖了从自然语言理解（NLU）到生成（NLG）的广泛任务。具体来说，我们在RoBERTa和DeBERTa上评估了GLUE（Wang等，2019）基准。我们遵循Li &amp; Liang（2021）的GPT-2设置进行直接比较，并为GPT-3的大规模实验添加了WikiSQL（Zhong等，2017）（自然语言到SQL查询）和SAMSum（Gliwa等，2019）（对话摘要）。有关我们使用的数据集的更多详细信息，请参见附录C。我们在所有实验中使用NVIDIA Tesla V100。</p><h3 id="5-1-Baselines"><a href="#5-1-Baselines" class="headerlink" title="5.1 Baselines"></a>5.1 Baselines</h3><p>为了广泛地与其他基线进行比较，我们复制了先前工作的设置，并尽可能重复使用他们报告的数字。然而，这意味着某些基线可能仅出现在某些实验中。</p><p><strong>微调（Fine-Tuning, FT）</strong> 是一种常见的适应方法。在微调过程中，模型被初始化为预训练的权重和偏置，所有模型参数都经历梯度更新。一种简单的变体是仅更新某些层，同时冻结其他层。我们包括了一种在先前工作（Li &amp; Liang，2021）中报告的基线，它仅适应GPT-2的最后两层（FTTop2）。</p><p><strong>仅训练偏置或BitFit</strong> 是一种基线方法，我们仅训练偏置向量，同时冻结其他所有参数。这个基线最近也被BitFit（Zaken等，2021）研究。</p><p><strong>前缀嵌入调优（PreEmbed）</strong> 在输入令牌之间插入特殊令牌。这些特殊令牌具有可训练的词嵌入，通常不在模型的词汇表中。放置这些令牌的位置会影响性能。我们专注于“前缀化”，即将这些令牌添加到提示的开头，以及“中缀化”，即将这些令牌添加到提示的中间；这两者都在Li &amp; Liang（2021）中讨论。我们用 $l_p$（前缀令牌的数量）和 $l_i$（中缀令牌的数量）表示。可训练参数的数量为 $|\Theta| = d_{\text{model}} \times (l_p + l_i)$。</p><p><strong>前缀层调优（PreLayer）</strong> 是前缀嵌入调优的扩展。我们不仅学习某些特殊令牌的词嵌入（或等效于嵌入层后的激活），还学习每个Transformer层后的激活。从前一层计算的激活简单地被可训练的激活替代。可训练参数的数量为 $|\Theta| = L \times d_{\text{model}} \times (l_p + l_i)$，其中 $L$ 是Transformer层的数量。</p><p><strong>适配器调优（Adapter tuning）</strong>，如Houlsby等（2019）提出的，在自注意力模块（和MLP模块）与后续残差连接之间插入适配器层。在适配器层中，有两个带有偏置的全连接层，中间有一个非线性层。我们称这种原始设计为AdapterH。最近，Lin等（2020）提出了一种更高效的设计，将适配器层仅应用于MLP模块之后和LayerNorm之后。我们称之为AdapterL。这与Pfeiffer等（2021）提出的另一个设计非常相似，我们称之为AdapterP。我们还包括另一个基线，称为AdapterDrop（Rücklé等，2020），它通过丢弃一些适配器层来提高效率（AdapterD）。我们尽可能引用先前工作的数字，以最大限度地增加我们比较的基线数量；它们在第一列带有星号（*）的行中。在所有情况下，我们有 $|\Theta| = \hat{L}_{\text{Adpt}} \times (2 \times d_{\text{model}} \times r + r + d_{\text{model}}) + 2 \times \hat{L}_{\text{LN}} \times d_{\text{model}}$，其中 $\hat{L}_{\text{Adpt}}$ 是适配器层的数量，$\hat{L}_{\text{LN}}$ 是可训练的LayerNorm数量（例如，在AdapterL中）。</p><p><strong>LoRA</strong> 在现有权重矩阵中并行添加可训练的秩分解矩阵对。如第4.2节所述，为了简化，我们在大多数实验中仅将LoRA应用于 $W_q$ 和 $W_v$。可训练参数的数量由秩 $r$ 和原始权重的形状决定：$|\Theta| = 2 \times \hat{L}_{\text{LoRA}} \times d_{\text{model}} \times r$，其中 $\hat{L}_{\text{LoRA}}$ 是我们应用LoRA的权重矩阵数量。</p><p><img src="/2024/07/09/papers-paper02-lora/lora03.png" alt="表2：RoBERTa base、RoBERTa large 和 DeBERTa XXL 在GLUE基准测试中使用不同适应方法的表现。我们报告了MNLI的整体（匹配和不匹配）准确性、CoLA的Matthew’s相关系数、STS-B的Pearson相关系数，以及其他任务的准确性。所有指标数值越高越好。* 表示先前工作中发布的数字。† 表示按照Houlsby等（2019）类似设置进行的运行以进行公平比较。"></p><h3 id="5-2-RoBERTa-Base-Large"><a href="#5-2-RoBERTa-Base-Large" class="headerlink" title="5.2 RoBERTa Base/Large"></a>5.2 RoBERTa Base/Large</h3><p>RoBERTa（Liu等，2019）优化了BERT（Devlin等，2019a）最初提出的预训练配方，并在没有引入更多可训练参数的情况下提升了后者的任务性能。尽管RoBERTa近年来在GLUE基准测试（Wang等，2019）等NLP排行榜上被更大的模型超越，但它在从业者中仍然是一个竞争力强且受欢迎的预训练模型。我们从HuggingFace Transformers库（Wolf等，2020）中获取预训练的RoBERTa base（125M）和RoBERTa large（355M），并评估不同高效适应方法在GLUE基准任务上的表现。我们还根据Houlsby等（2019）和Pfeiffer等（2021）的设置进行复现。为了确保公平比较，在与适配器进行比较时，我们对LoRA的评估做了两个重要的改变。首先，我们对所有任务使用相同的批量大小，并使用128的序列长度以匹配适配器基线。其次，我们在MRPC、RTE和STS-B任务中将模型初始化为预训练模型，而不是像微调基线那样初始化为已经适应MNLI的模型。遵循Houlsby等（2019）更严格设置的运行标记为†。结果如表2（前三部分）所示。有关使用的超参数的详细信息，请参见第D.1节。</p><h3 id="5-3-DeBERTa-XXL"><a href="#5-3-DeBERTa-XXL" class="headerlink" title="5.3 DeBERTa XXL"></a>5.3 DeBERTa XXL</h3><p>DeBERTa（He等，2021）是BERT的一个更新变体，经过大规模训练，在GLUE（Wang等，2019）和SuperGLUE（Wang等，2020）等基准测试中表现非常出色。我们评估了LoRA是否仍能在GLUE上匹敌完全微调的DeBERTa XXL（1.5B）的性能。结果如表2（底部部分）所示。有关使用的超参数的详细信息，请参见第D.2节。</p><p><img src="/2024/07/09/papers-paper02-lora/lora03.png" alt="表3：在E2E NLG挑战赛中使用不同适应方法的GPT-2 medium (M) 和 large (L)。对于所有指标，数值越高越好。LoRA在可比较或更少的可训练参数下优于多个基线。我们运行的实验显示了置信区间。*表示先前工作中发布的数字。"></p><h3 id="5-4-GPT-2-Medium-Large"><a href="#5-4-GPT-2-Medium-Large" class="headerlink" title="5.4 GPT-2 Medium/Large"></a>5.4 GPT-2 Medium/Large</h3><p>在展示了LoRA在NLU任务上可以作为全量微调的有力替代方案后，我们希望回答LoRA在NLG模型（如GPT-2 medium和large（Radford等，b））上是否仍然表现出色。为了进行直接比较，我们的设置尽可能接近Li &amp; Liang（2021）。由于篇幅限制，本节仅展示我们在E2E NLG挑战赛上的结果（表3）。WebNLG（Gardent等，2017）和DART（Nan等，2020）的结果见第F.1节。我们在第D.3节列出了使用的超参数。</p><p><img src="/2024/07/09/papers-paper02-lora/lora05.png" alt="表4：不同适应方法在GPT-3 175B上的表现。我们报告了WikiSQL的逻辑形式验证准确性、MultiNLI-matched的验证准确性，以及SAMSum上的Rouge-1/2/L。LoRA的表现优于之前的方法，包括全量微调。WikiSQL的结果波动在±0.5%左右，MNLI-m在±0.1%左右，SAMSum的三个指标分别在±0.2/±0.2/±0.1%左右。"></p><h3 id="5-5-Scaling-Up-to-GPT-3-175B"><a href="#5-5-Scaling-Up-to-GPT-3-175B" class="headerlink" title="5.5 Scaling Up to GPT-3 175B"></a>5.5 Scaling Up to GPT-3 175B</h3><p>作为对LoRA的最终压力测试，我们扩展到具有1750亿参数的GPT-3。由于训练成本高，我们只报告给定任务在随机种子上的典型标准差，而不是为每个条目提供一个。有关使用的超参数的详细信息，请参见第D.4节。</p><p>如表4所示，LoRA在所有三个数据集上匹配或超过了微调基线。请注意，并不是所有方法在拥有更多可训练参数时都会单调受益，如图2所示。当我们使用超过256个特殊令牌进行前缀嵌入调优或超过32个特殊令牌进行前缀层调优时，我们观察到性能显著下降。这与Li &amp; Liang（2021）中的类似观察结果相一致。虽然对这一现象的彻底调查超出了本文的范围，但我们怀疑更多的特殊令牌导致输入分布进一步偏离预训练数据分布。我们在第F.3节分别调查了不同适应方法在低数据环境中的表现。<br><img src="/2024/07/09/papers-paper02-lora/lora06.png" alt="图2：在WikiSQL和MNLI-matched上的GPT-3 175B验证准确性与多种适应方法的可训练参数数量的关系。LoRA显示出更好的可扩展性和任务性能。有关绘制数据点的更多详细信息，请参见第F.2节。"></p><h2 id="6-Related-Works"><a href="#6-Related-Works" class="headerlink" title="6 Related Works"></a>6 Related Works</h2><p><strong>Transformer语言模型</strong></p><p>Transformer（Vaswani等，2017）是一种广泛使用自注意力机制的序列到序列架构。Radford等（a）将其应用于自回归语言建模，使用一堆Transformer解码器。从那时起，基于Transformer的语言模型在NLP中占据主导地位，在许多任务中达到了最先进的水平。BERT（Devlin等，2019b）和GPT-2（Radford等，b）的出现开创了一种新范式——这两个大型Transformer语言模型在大量文本上训练，在通用领域数据预训练后再在任务特定数据上进行微调，相比直接在任务特定数据上训练提供了显著的性能提升。训练更大的Transformer通常会带来更好的性能，这仍然是一个活跃的研究方向。GPT-3（Brown等，2020）是迄今为止训练的最大的单一Transformer语言模型，具有1750亿参数。</p><p><strong>提示工程和微调</strong></p><p>尽管GPT-3 175B可以通过少量额外的训练示例来适应其行为，但结果严重依赖于输入提示（Brown等，2020）。这需要一种经验性的提示编写和格式化的艺术，以最大化模型在期望任务上的性能，这被称为提示工程或提示黑客。微调则是将预训练在通用领域的模型重新训练以适应特定任务（Devlin等，2019b；Radford等，a）。其变体包括仅学习部分参数（Devlin等，2019b；Collobert &amp; Weston，2008），但从业者通常会重新训练所有参数以最大化下游性能。然而，由于GPT-3 175B生成的检查点庞大以及高硬件门槛，按通常方式进行微调具有挑战性，因为其内存占用与预训练相同。</p><p><strong>参数高效适应</strong></p><p>许多人提出在神经网络的现有层之间插入适配器层（Houlsby等，2019；Rebuffi等，2017；Lin等，2020）。我们的方法使用类似的瓶颈结构对权重更新施加低秩约束。关键的功能区别在于，我们学习的权重可以在推理过程中与主权重合并，从而不引入任何延迟，而适配器层则不具备这一点（第3节）。适配器的一个当代扩展是COMPACTER（Mahabadi等，2021），其本质上使用克罗内克产品和一些预定的权重共享方案对适配器层进行参数化。同样，将LoRA与其他基于张量积的方法结合可能会提高其参数效率，这留待未来工作中探索。最近，许多人提出优化输入词嵌入代替微调，类似于提示工程的连续可微推广（Li &amp; Liang，2021；Lester等，2021；Hambardzumyan等，2020；Liu等，2021）。我们在实验部分包括了与Li &amp; Liang（2021）的比较。然而，这些工作只能通过在提示中使用更多的特殊令牌来扩展，而这些特殊令牌在学习位置嵌入时会占用可用的序列长度。</p><p><strong>深度学习中的低秩结构</strong></p><p>低秩结构在机器学习中非常常见。许多机器学习问题具有某种内在的低秩结构（Li等，2016；Cai等，2010；Li等，2018b；Grasedyck等，2013）。此外，众所周知，对于许多深度学习任务，尤其是那些具有严重过参数化的神经网络，学习到的神经网络在训练后会具有低秩性质（Oymak等，2019）。一些先前的工作甚至在训练原始神经网络时显式施加低秩约束（Sainath等，2013；Povey等，2018；Zhang等，2014；Jaderberg等，2014；Zhao等，2016；Khodak等，2021；Denil等，2014）；然而，据我们所知，这些工作中没有考虑到对冻结模型进行低秩更新以适应下游任务。在理论文献中，众所周知，当底层概念类具有某种低秩结构时，神经网络的性能优于其他经典学习方法，包括相应的（有限宽度）神经切线核（Allen-Zhu等，2019；Li &amp; Liang，2018）（Ghorbani等，2020；Allen-Zhu &amp; Li，2019；Allen-Zhu &amp; Li，2020a）。Allen-Zhu &amp; Li（2020b）的另一个理论结果表明，低秩适应在对抗性训练中可能有用。总之，我们认为，我们提出的低秩适应更新在文献中有充分的动机。</p><h2 id="7-Understanding-the-Low-Rank-updates"><a href="#7-Understanding-the-Low-Rank-updates" class="headerlink" title="7 Understanding the Low-Rank updates"></a>7 Understanding the Low-Rank updates</h2><p>鉴于LoRA的实证优势，我们希望进一步解释从下游任务中学习到的低秩适应的特性。请注意，低秩结构不仅降低了硬件门槛，使我们能够并行运行多个实验，还提供了更好的解释性，说明更新权重如何与预训练权重相关联。我们将研究重点放在GPT-3 175B上，在这里我们实现了最大程度的可训练参数减少（高达10,000倍），而不影响任务性能。</p><p>我们进行了一系列实证研究，以回答以下问题：</p><ol><li>在参数预算约束下，预训练Transformer中的哪些权重矩阵子集应该适应以最大化下游性能？</li><li>“最佳”适应矩阵 $\Delta W$ 是否真的秩不足？如果是，实践中使用的最佳秩是多少？</li><li>$\Delta W$ 与 $W$ 之间的联系是什么？$\Delta W$ 是否与 $W$ 高度相关？$\Delta W$ 相对于 $W$ 有多大？</li></ol><p>我们相信对问题（2）和（3）的回答能揭示使用预训练语言模型进行下游任务的基本原则，这是NLP中的一个关键话题。</p><h3 id="7-1-Which-Weight-Matrices-in-Transformer-should-we-apply-LoRa-to"><a href="#7-1-Which-Weight-Matrices-in-Transformer-should-we-apply-LoRa-to" class="headerlink" title="7.1 Which Weight Matrices in Transformer should we apply LoRa to"></a>7.1 Which Weight Matrices in Transformer should we apply LoRa to</h3><p>在有限的参数预算下，我们应该适应哪些类型的权重以通过LoRA在下游任务中获得最佳性能？如第4.2节所述，我们只考虑自注意力模块中的权重矩阵。在GPT-3 175B上，我们设定了1800万参数预算（如果以FP16存储，大约为35MB），这对应于在所有96层中，如果适应一种类型的注意力权重，则 $r = 8$，如果适应两种类型，则 $r = 4$。结果如表5所示。<br><img src="/2024/07/09/papers-paper02-lora/lora07.png" alt="表5：在相同数量的可训练参数下，将LoRA应用于GPT-3中不同类型的注意力权重后的WikiSQL和MultiNLI验证准确性。对W_q和W_v进行适应总体上表现最佳。我们发现对于给定的数据集，随机种子之间的标准差是一致的，并在第一列中报告。"><br>请注意，将所有参数放在 $\Delta W_q$ 或 $\Delta W_k$ 中会显著降低性能，而适应 $W_q$ 和 $W_v$ 则产生最佳结果。这表明，即使是秩为四的情况下，$\Delta W$ 中也包含足够的信息，因此适应更多的权重矩阵比用更大的秩适应单一类型的权重更可取。</p><h3 id="7-2-What-is-the-optimal-rank-for-LoRA"><a href="#7-2-What-is-the-optimal-rank-for-LoRA" class="headerlink" title="7.2 What is the optimal rank for LoRA"></a>7.2 What is the optimal rank for LoRA</h3><p>我们将注意力转向秩 $r$ 对模型性能的影响。我们适应 $\{W_q, W_v\}$、$\{W_q, W_k, W_v, W_c\}$ 以及仅适应 $W_q$ 进行比较。<br><img src="/2024/07/09/papers-paper02-lora/lora08.png" alt="表6：在WikiSQL和MultiNLI上的不同秩r的验证准确性。令我们惊讶的是，对于这些数据集，适应W_q和W_v的秩小至1就足够了，而单独训练W_q需要更大的r。我们在第H.2节中对GPT-2进行了类似的实验。"><br>表6显示，令人惊讶的是，即使在非常小的 $r$ 下，LoRA 也表现得非常有竞争力（尤其是 $\{W_q, W_v\}$ 比仅适应 $W_q$ 更为显著）。这表明更新矩阵 $\Delta W$ 可能具有非常小的“内在秩”。为了进一步支持这一发现，我们检查了不同 $r$ 的选择和不同随机种子所学习到的子空间的重叠情况。我们认为，增加 $r$ 并不会覆盖更多有意义的子空间，这表明低秩适应矩阵已经足够。</p><p><strong>不同 $r$ 的子空间相似性</strong>。给定 $A_{r=8}$ 和 $A_{r=64}$，它们是使用相同预训练模型学习到的秩为 $r=8$ 和 $r=64$ 的适应矩阵，我们进行奇异值分解，得到右奇异酉矩阵 $U_{A_{r=8}}$ 和 $U_{A_{r=64}}$。我们希望回答的问题是：$U_{A_{r=8}}$ 中前 $i$ 个奇异向量所跨越的子空间有多少包含在 $U_{A_{r=64}}$ 的前 $j$ 个奇异向量所跨越的子空间中（对于 $1 \le i \le 8$ 和 $1 \le j \le 64$）？我们用基于Grassmann距离的归一化子空间相似性来测量这个量（见附录G的正式讨论）：</p><script type="math/tex; mode=display">\phi(A_{r=8}, A_{r=64}, i, j) = \frac{||U_i^\top U_j||_F^2}{\min(i, j)} \in [0, 1]</script><p>其中 $U_i$ 表示 $U_{A_{r=8}}$ 中对应前 $i$ 个奇异向量的列。</p><p>$\phi(\cdot)$ 的范围是 $[0, 1]$，其中 1 表示子空间完全重叠，0 表示完全分离。图3展示了随着 $i$ 和 $j$ 的变化 $\phi$ 的变化。由于空间限制，我们仅查看第48层（共96层）中的情况，但结论对其他层也成立，如第H.1节所示。</p><p><img src="/2024/07/09/papers-paper02-lora/lora09.png" alt="图3：$A_{r=8}$ 和 $A_{r=64}$ 的列向量之间的子空间相似性，分别针对 $\Delta W_q$ 和 $\Delta W_v$。第三和第四幅图放大了前两幅图中的左下角三角区域。$r = 8$ 的顶级方向包含在 $r = 64$ 中，反之亦然。"><br>我们从图3中观察到一个重要现象。$A_{r=8}$ 和 $A_{r=64}$ 之间对应于顶级奇异向量的方向显著重叠，而其他方向则没有。具体来说，$A_{r=8}$ 的 $\Delta W_v$（或者 $\Delta W_q$）和 $A_{r=64}$ 的 $\Delta W_v$（或者 $\Delta W_q$）共享一个维度为1的子空间，其归一化相似度大于0.5，这解释了为什么在GPT-3的下游任务中，$r = 1$ 表现相当好。</p><p>由于 $A_{r=8}$ 和 $A_{r=64}$ 都使用相同的预训练模型进行学习，图3表明 $A_{r=8}$ 和 $A_{r=64}$ 的顶级奇异向量方向是最有用的，而其他方向可能主要包含在训练过程中积累的随机噪声。因此，适应矩阵确实可以具有非常低的秩。</p><p>不同随机种子的子空间相似性。我们进一步通过绘制两个随机种子运行之间 $r = 64$ 的归一化子空间相似性来确认这一点，如图4所示。$ \Delta W_q$ 似乎具有比 $ \Delta W_v$ 更高的“内在秩”，因为两个运行对于 $ \Delta W_q$ 学习到了更多的共同奇异值方向，这与我们在表6中的实证观察一致。作为对比，我们还绘制了两个随机高斯矩阵，它们彼此之间不共享任何共同的奇异值方向。</p><p><img src="/2024/07/09/papers-paper02-lora/lora10.png" alt="左图和中图：两个随机种子生成的 $A_{r=64}$ 的列向量之间的归一化子空间相似性，分别针对第48层的 $\Delta W_q$ 和 $\Delta W_v$。右图：两个随机高斯矩阵的列向量之间的相同热图。其他层的结果见第H.1节。"></p><h3 id="7-3-How-does-the-adaptation-matrix-Delta-W-compare-to-W"><a href="#7-3-How-does-the-adaptation-matrix-Delta-W-compare-to-W" class="headerlink" title="7.3 How does the adaptation matrix $\Delta W$ compare to W"></a>7.3 How does the adaptation matrix $\Delta W$ compare to W</h3><p>我们进一步研究了 $\Delta W$ 和 $W$ 之间的关系。特别是，$\Delta W$ 是否与 $W$ 高度相关？（或者从数学上讲，$\Delta W$ 是否主要包含在 $W$ 的顶级奇异方向中？）此外，$\Delta W$ 相对于 $W$ 中对应的方向有多“大”？这可以揭示适应预训练语言模型的基本机制。</p><p>为了解答这些问题，我们通过计算 $U^\top W V^\top$ 将 $W$ 投影到 $\Delta W$ 的 $r$ 维子空间中，其中 $U/V$ 是 $\Delta W$ 的左/右奇异向量矩阵。然后，我们比较 $|U^\top W V^\top|_F$ 和 $|W|_F$ 之间的Frobenius范数。作为对比，我们还通过用 $W$ 的前 $r$ 个奇异向量或一个随机矩阵替换 $U, V$ 来计算 $|U^\top W V^\top|_F$。<br><img src="/2024/07/09/papers-paper02-lora/lora11.png" alt="表7：$U^TW_qV^T$的Frobenius范数，其中$U$和$V$分别是 (1)$\Delta W_q$、(2) $W_q$或 (3) 随机矩阵,的左/右前r个奇异向量方向。权重矩阵取自GPT-3的第48层。"><br>从表7中我们得出以下结论。首先，$\Delta W$ 与 $W$ 的相关性比随机矩阵更强，这表明 $\Delta W$ 放大了 $W$ 中已经存在的一些特征。其次，$\Delta W$ 并不是重复 $W$ 的顶级奇异方向，而是放大了 $W$ 中没有强调的方向。第三，放大因子相当大：对于 $r = 4$ 的情况，约为21.5（即 $\approx 6.91/0.32$）。见第H.4节了解为什么 $r = 64$ 的放大因子较小。我们还在第H.3节提供了一个可视化，展示了随着我们包括更多 $W_q$ 的顶级奇异方向，相关性如何变化。这表明低秩适应矩阵可能放大了在一般预训练模型中已学习但未强调的特定下游任务的重要特征。</p><h2 id="8-Conclusion-and-future-work"><a href="#8-Conclusion-and-future-work" class="headerlink" title="8 Conclusion and future work"></a>8 Conclusion and future work</h2><p>微调庞大的语言模型在硬件需求和为不同任务托管独立实例的存储/切换成本方面是极其昂贵的。我们提出了LoRA，这是一种高效的适应策略，既不引入推理延迟，也不减少输入序列长度，同时保持高模型质量。重要的是，它允许在作为服务部署时，通过共享绝大多数模型参数来实现快速任务切换。虽然我们专注于Transformer语言模型，但所提出的原则普遍适用于任何具有密集层的神经网络。</p><p>未来工作有很多方向。1）LoRA可以与其他高效的适应方法结合，可能提供正交的改进。2）微调或LoRA背后的机制尚不清楚——预训练过程中学习到的特征如何转化为在下游任务中表现良好？我们相信，LoRA比全量微调更容易回答这个问题。3）我们主要依靠启发式方法选择应用LoRA的权重矩阵。有更有原则的方法来做这件事吗？4）最后，$\Delta W$ 的秩不足表明 $W$ 也可能存在秩不足，这也可以成为未来工作的灵感来源。</p><h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><p>LoRA论文：<a href="https://arxiv.org/pdf/2106.09685">Low-Rank Adaptation of Large Language Models</a></p><p>给GPT2添加LoRA模块的代码实现：<a href="https://github.com/tsmatz/finetune_llm_with_lora/blob/master/02-finetune-gpt2-with-lora.ipynb">github代码实现</a></p>]]></content>
    
    
    <categories>
      
      <category>论文精读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文精读</tag>
      
      <tag>LoRA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型量化技术的原理和代码实现</title>
    <link href="/2024/07/09/quant/"/>
    <url>/2024/07/09/quant/</url>
    
    <content type="html"><![CDATA[<h1 id="大模型量化技术的原理"><a href="#大模型量化技术的原理" class="headerlink" title="大模型量化技术的原理"></a>大模型量化技术的原理</h1><p>大模型量化，简单而言，就是对大模型中的参数（比如权重参数）转换数据类型，比如从16位浮点型转为8位整型，转换后数据只占用一般存储空间且推理加快，但模型性能损失不大。</p><p>目前主要有两个权重量化技术：</p><ol><li>PTQ (Post-Training Quantization)训练后量化：先训练好模型，再把模型权重转为较低精度，而无需任何重新训练。PTQ方法易于实施，但是会导致潜在性能下降。  </li><li>QAT (Quantization-Aware Training)量化感知训练：在预训练或者微调阶段结合了权重转换过程，提高模型性能。但是QAT的计算成本高，且需要有代表性的训练数据。</li></ol><h2 id="数据类型和存储"><a href="#数据类型和存储" class="headerlink" title="数据类型和存储"></a>数据类型和存储</h2><p>计算机上存储数据有这么几种类型，不同类型数据需要不同的存储空间。深度学习中主要用浮点型，下面看看浮点型的数据结构.</p><p>浮点型数据使用n位来存储数值，比如float32使用32bit存储一个数，float16使用16bit存储一个数。具体的，这n位分为三部分：</p><ol><li>符号sign：符号位表示这个数是正数或者负数，占用1bit，0表示正数，1表示负数；</li><li>指数Exponent：指数位一般占用8bit，表示基数（二进制中通常是2）的幂，指数可以是正数或者负数，让数字很大或者很小；</li><li>有效数/尾数 Significand/Mantissa：剩余位存储有效数，也称为尾数。</li></ol><p>浮点数的这种设计让其有不同精度能覆盖广泛的值，公式如下：</p><script type="math/tex; mode=display">(-1)^{sign} \times base^{exponent} \times significand</script><p>下面给出三个例子方便理解：<br>float32：1位表示符号，8bit表示指数，剩余23bit表示有效数；<br>float16: 1bit符号，5bit指数，10bit有效数；<br>bfloat16: 1bit符号，8bit指数，7bit有效数，和float16相比扩大了范围但降低精度；<br><img src="/2024/07/09/quant/quant01.png" alt="float32/float16/bfloat16的数据例子"></p><h2 id="量化方法"><a href="#量化方法" class="headerlink" title="量化方法"></a>量化方法</h2><h3 id="MinMax量化"><a href="#MinMax量化" class="headerlink" title="MinMax量化"></a>MinMax量化</h3><p>MinMax量化属于线性量化，也称为均匀量化。MinMax量化分为对称量化和非对称量化两种。公式如下(这个公式参考的LLM-QAT论文，但是为什么round取整之后还要乘以缩放因子$\alpha$？这个公式1好像是把quantization和dequantization两个过程结合了)：</p><script type="math/tex; mode=display">X_Q^i = \alpha \hat X_Q^i = \alpha \lfloor \frac{X_R^i - \beta }{\alpha } \rceil + \beta \tag{1}</script><p>其中，$X_Q$和$X_R$分别表示量化后变量和全精度变量。$i$表示张量中的第$i$个元素。$\alpha$表示放缩因子，$\beta$是零点值。<br>对于对称量化：</p><script type="math/tex; mode=display">\alpha = \frac{max(|X_R|)}{2^{N-1} -1}, \ \ \beta =0</script><p>对于非对称量化：</p><script type="math/tex; mode=display">\alpha = \frac{max(X_R) - min(X_R)}{2^N -1}, \ \ \beta = min(X_R)</script><p>具体的，我们以float32量化为int8为例，此时全精度变量$X_R$的每个元素是float32型，量化后的$X_{quant}$的每个元素是int8型，$X_{dequant}$表示反量化得到的值 它不等于全精度变量$X_R$有一定误差，$N=8$，$X^i$表示变量X的第i个元素，括号$\lfloor \rceil$表示round取整，可以是上取整也可以是下取整。<br>堆成量化和反量化公式如下：</p><script type="math/tex; mode=display">X_{quant}^i = \lfloor \frac{X_R^i - \beta}{\alpha} \rceil = \lfloor \frac{127 \times X_R^i }{ max(|X_R|) } \rceil  \\X_{dequant}^i = \alpha X_{quant}^i +\beta = \frac{max(|X_R|)}{127} \times X_{quant}^i</script><p>下面是pytorch实现的$N=8$的对称量化：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br> <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">absmax_quantize</span>(<span class="hljs-params">X</span>):<br>    <span class="hljs-comment"># Calculate scale</span><br>    scale = <span class="hljs-number">127</span> / torch.<span class="hljs-built_in">max</span>(torch.<span class="hljs-built_in">abs</span>(X))<br> <br>    <span class="hljs-comment"># Quantize</span><br>    X_quant = (scale * X).<span class="hljs-built_in">round</span>()<br> <br>    <span class="hljs-comment"># Dequantize</span><br>    X_dequant = X_quant / scale<br> <br>    <span class="hljs-keyword">return</span> X_quant.to(torch.int8), X_dequant<br></code></pre></td></tr></table></figure></p><p>$N=8$的非对称量化：</p><script type="math/tex; mode=display">X_{quant}^i = \lfloor \frac{X_R^i-\beta}{\alpha} \rceil  = \lfloor \frac{255(X_R^i - min(X_R))}{max(X_R) - min(X_R)} \rceil \\X_{dequant}^i = \alpha X_{quant}^i + \beta = \frac{max(X_R)-min(X_R)}{255} X_{quant}^i + min(X_R)</script><p>pytorch代码实现：（这个代码的公式和上述公式略有不同）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">zeropoint_quantize</span>(<span class="hljs-params">X</span>):<br>    <span class="hljs-comment"># Calculate value range (denominator)</span><br>    x_range = torch.<span class="hljs-built_in">max</span>(X) - torch.<span class="hljs-built_in">min</span>(X)<br>    x_range = <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> x_range == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> x_range<br> <br>    <span class="hljs-comment"># Calculate scale</span><br>    scale = <span class="hljs-number">255</span> / x_range<br> <br>    <span class="hljs-comment"># Shift by zero-point</span><br>    zeropoint = (-scale * torch.<span class="hljs-built_in">min</span>(X) - <span class="hljs-number">128</span>).<span class="hljs-built_in">round</span>()<br> <br>    <span class="hljs-comment"># Scale and round the inputs</span><br>    X_quant = torch.clip((X * scale + zeropoint).<span class="hljs-built_in">round</span>(), -<span class="hljs-number">128</span>, <span class="hljs-number">127</span>)<br> <br>    <span class="hljs-comment"># Dequantize</span><br>    X_dequant = (X_quant - zeropoint) / scale<br> <br>    <span class="hljs-keyword">return</span> X_quant.to(torch.int8), X_dequant<br></code></pre></td></tr></table></figure></p><h2 id="待办："><a href="#待办：" class="headerlink" title="待办："></a>待办：</h2><p>机器学习中数据类型有哪些 除了最常见的float32/float16/int8之外。</p><p>量化方法MinMax的pytorch实现</p><p>量化除了MinMax还有哪些</p><h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><p><a href="https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c">英文原版实现LLM的QAT量化</a></p><p><a href="https://blog.csdn.net/shebao3333/article/details/134476651">中文版实现LLM的QAT量化</a></p>]]></content>
    
    
    <categories>
      
      <category>ML基础知识</category>
      
      <category>模型量化</category>
      
    </categories>
    
    
    <tags>
      
      <tag>模型量化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLM-QAT论文精读</title>
    <link href="/2024/07/09/papers-paper01-LLM-QAT/"/>
    <url>/2024/07/09/papers-paper01-LLM-QAT/</url>
    
    <content type="html"><![CDATA[<h1 id="LLM-QAT论文精读"><a href="#LLM-QAT论文精读" class="headerlink" title="LLM-QAT论文精读"></a>LLM-QAT论文精读</h1><p>今天我们精读这篇论文：<a href="https://arxiv.org/pdf/2305.17888">LLM-QAT: Data-Free Quantization Aware Training for Large Language Models</a></p><p>这篇论文实现了对通用大模型的量化，大模型的量化是指：用更少的信息表示数据，同时不损失太多准确性。通常的做法是将模型的一些参数（如权重）转换并存储为更少比特的数据类型。例如，将权重参数由16位浮点数转为8位整数，转换后的数字将会只需要一半的存储空间，同时因为精度降低，计算被简化，推理速度也会加快。</p><p>具体的，关于大模型量化的基础原理可以看这篇博客<a href="/2024/07/09/quant/" title="大模型量化技术的原理和代码实现">“大模型量化技术的原理和代码实现”</a>。</p><h2 id="论文全文翻译"><a href="#论文全文翻译" class="headerlink" title="论文全文翻译"></a>论文全文翻译</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>几种post-training quantization方法已经应用于大型语言模型（LLM），并且在8位精度下表现良好。我们发现这些方法在更低的位精度下会失效，因此我们研究了量化感知训练quantization aware training for LLMs（LLM-QAT），以进一步推进量化水平。<br>我们提出了一种无需数据的蒸馏方法(data-free distillation method)，该方法利用预训练模型生成的输出，从而更好地保留了原始输出分布，<u>并且可以独立于其训练数据对任何生成模型进行量化，类似于训练后量化方法</u>。<br>除了量化权重和激活值外，我们还量化了KV缓存，这对于在当前模型规模下提高吞吐量和支持长序列依赖性至关重要。我们在7B、13B和30B大小的LLaMA模型上进行了实验，量化水平最低达4位。我们观察到，在低位设置下，相较于无需训练的方法，我们的量化方法有显著的改进。</p><h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h3><p>继GPT-3（Brown等，2020）之后，OPT（Zhang等，2022）、PALM（Chowdhery等，2022）、BLOOM（Scao等，2022）、Chinchilla（Hoffmann等，2022）和LLaMA（Touvron等，2023）等多个大型语言模型（LLM）家族已经证明，增加模型规模能够提升模型能力。因此，拥有数百亿甚至数千亿参数的语言模型已经成为当今AI领域的常态。尽管LLM的兴起令人振奋，但由于其巨大的计算成本和环境足迹，如何将这些模型服务于数十亿用户仍面临重大挑战。</p><p>幸运的是，越来越多的研究致力于准确量化大型语言模型（LLM），多项近期工作（Xiao等，2022；Yao等，2022）专注于权重和激活的8位训练后量化，并在几乎没有精度损失的情况下取得了成功。然而，一个拥有650亿参数的LLaMA模型，仅其权重就占用了65GB的GPU内存。此外，用于存储注意力层激活值的键值（KV）缓存可以轻易达到数十GB，并且在当今常见的长序列长度应用中成为吞吐量瓶颈。上述工作并没有考虑同时对KV缓存、权重和激活进行量化。不幸的是，最先进的训练后量化方法在超过8位时质量显著下降。对于更高的量化水平，我们发现有必要使用量化感知训练（QAT）。</p><p>据我们所知，LLM的量化感知训练（QAT）尚未被研究。这可以理解，主要有两个原因。首先，LLM的训练在技术上非常困难且资源密集。其次，QAT需要训练数据，而对于LLM来说，这些数据难以获得。预训练数据的庞大规模和多样性本身就是一个障碍。预处理可能成本高昂，更糟的是，由于法律限制，有些数据可能根本无法获得。越来越普遍的做法是分阶段训练LLM，涉及指令调优和强化学习（Ouyang等，2022），这在QAT期间很难复制。<u>在这项工作中，我们通过使用LLM自身生成的数据进行知识蒸馏(knowledge distillation)，绕过了这个问题</u>。这种简单的解决方案，我们称之为<strong>无数据知识蒸馏</strong>，适用于任何生成模型，而不管原始训练数据是否可用。我们展示了这种方法更能保留原始模型的输出分布，甚至比在原始训练集的大子集上训练更好。此外，我们只需使用一小部分（10万）采样数据就能成功蒸馏量化模型，从而保持合理的计算成本。我们所有的实验都在单个8-GPU训练节点上进行。</p><p>因此，<u>我们能够将7B、13B和30B的LLaMA模型的权重和KV缓存量化到4位</u>。在这方面，我们的方法在质量上相对于训练后量化显示出显著提升。值得注意的是，采用QAT的大模型在性能上优于使用16位浮点表示的小模型，尽管它们的模型规模相似。<u>此外，我们成功地将激活值量化到6位精度，超越了现有方法的可能性</u>。有关我们实验结果的全面分析和详细研究，请参见第3节。</p><p><u>总而言之，我们展示了QAT在LLM中的首次应用，结果是首个准确的4位量化LLM。我们还展示了同时对权重、激活和KV缓存进行量化</u>，这对于缓解长序列生成的吞吐量瓶颈至关重要。所有这一切都是通过一种新颖的<strong>无数据蒸馏方法(data-free distillation method)</strong>实现的，使得QAT在大型预训练生成模型中变得切实可行。</p><h3 id="2-Method"><a href="#2-Method" class="headerlink" title="2 Method"></a>2 Method</h3><p>使用量化感知训练（QAT）对大型语言模型（LLM）进行量化是一项复杂的任务，面临两个关键方面的挑战。首先，LLM经过预训练，以在零样本泛化中表现出色，保持这一能力在量化后至关重要。因此，<u>选择合适的微调数据集非常重要</u>。如果QAT数据在领域上过于狭窄或与原始预训练分布显著不同，这可能会损害模型性能。另一方面，由于LLM训练的规模和复杂性，很难完全复制原始训练设置。在第2.1节中，<u>我们介绍了无数据量化感知训练（QAT），该方法使用下一个令牌数据生成QAT数据</u>。与使用原始预训练数据的子集相比，这种方法表现出更优异的性能。其次，LLM具有独特的权重和激活分布，表现为显著的离群值，这使得它们不同于较小的模型。因此，适用于小模型的最先进的量化裁剪方法无法直接应用于LLM。在第2.2节中，我们确定了<u>适合LLM的量化器(quantizers)</u>。</p><h4 id="2-1-Data-free-Distillation"><a href="#2-1-Data-free-Distillation" class="headerlink" title="2.1 Data-free Distillation"></a>2.1 Data-free Distillation</h4><p><img src="/2024/07/09/papers-paper01-LLM-QAT/LLM-QAT01.png" alt="图1:LLM-QAT概述。我们通过从预训练模型中生成下一个令牌数据，这些数据从top-k候选中采样。然后，我们使用生成的数据作为输入，并以教师模型的预测作为标签来指导量化模型的微调。"><br>为了在有限的微调数据量下尽可能接近预训练数据的分布，我们提出了<u>从原始预训练模型生成下一个令牌数据的方法</u>。如图1(a)所示，我们从词汇表中随机选择第一个令牌&lt;’start’&gt;，并让预训练模型生成下一个令牌&lt;’out1’&gt;，然后将生成的令牌附加到开始令牌上以生成新的输出&lt;’out2’&gt;。我们重复这一迭代过程，直到我们达到句子结束令牌或最大生成长度。</p><p>我们在下一个令牌生成中测试了三种不同的采样策略。最直接的方法是选择前1名候选者作为下一个令牌。然而，这种生成的句子缺乏多样性，并且会周期性地重复几个令牌。为了解决这个问题，我们使用预训练模型的SoftMax输出作为概率，从分布中随机采样下一个令牌。这种采样策略生成了更多样化的句子，并大大提高了微调学生模型的准确性。此外，我们发现最初的几个令牌在确定预测趋势方面起着至关重要的作用。因此，这些令牌需要有较高的置信度。在我们的生成过程中，我们采用了一种混合采样策略，确定性地选择前3~5个令牌的顶级预测，然后随机采样剩余的令牌。在第3.3.1节中，我们对比了不同生成数据和真实数据的详细<u>消融研究(ablation study)</u>。</p><h4 id="2-2-Quantization-Aware-Training"><a href="#2-2-Quantization-Aware-Training" class="headerlink" title="2.2 Quantization-Aware Training"></a>2.2 Quantization-Aware Training</h4><h5 id="2-2-1-Preliminaries"><a href="#2-2-1-Preliminaries" class="headerlink" title="2.2.1 Preliminaries"></a>2.2.1 Preliminaries</h5><p>在这项工作中，我们研究了线性量化(linear quantization)，即均匀量化(uniform quantization)。线性量化可以根据实值是否被裁剪分为两类：MinMax量化，它保留了所有的数值范围；以及基于裁剪的量化(clipping-based quantization)。</p><p>在MinMax量化中，量化过程可以表述如下:</p><script type="math/tex; mode=display">X_Q^i = \alpha \hat X_Q^i = \alpha \lfloor \frac{X_R^i - \beta }{\alpha } \rceil + \beta \tag{1}</script><p>其中，$X_Q$和$X_R$分别表示量化后变量和全精度变量。$i$表示张量中的第$i$个元素。$\alpha$表示放缩因子，$\beta$是零点值。对于对称量化：</p><script type="math/tex; mode=display">\alpha = \frac{max(|X_R|)}{2^{N-1} -1}, \ \ \beta =0</script><p>对于非对称量化：</p><script type="math/tex; mode=display">\alpha = \frac{max(X_R) - min(X_R)}{2^N -1}, \ \ \beta = min(X_R)</script><p>与MinMax量化相比，裁剪离群值可以帮助提高精度，并将更多的位分配给中间值。因此，许多近期的研究（Shen等，2020a；Zhang等，2020）采用基于裁剪的量化(clipping-based quantization)用于基于Transformer的语言模型。量化过程可以表述如下：</p><script type="math/tex; mode=display">X_Q^i = \alpha \hat X_Q^i = \alpha \lfloor Clip(\frac{X_R^i - \beta}{\alpha},0,1) \rceil + \beta \tag{2}</script><p>缩放因子$\alpha$和零点值$\beta$可以通过统计方法计算或通过梯度学习得到。</p><h5 id="2-2-2-Quantization-for-Large-Languare-Models"><a href="#2-2-2-Quantization-for-Large-Languare-Models" class="headerlink" title="2.2.2 Quantization for Large Languare Models"></a>2.2.2 Quantization for Large Languare Models</h5><p><img src="/2024/07/09/papers-paper01-LLM-QAT/LLM-QAT02.png" alt="图2：LLM-QAT中量化Transformer的概述。我们对全连接线性层中的所有权重和输入激活进行量化。如果有规定，KV缓存也会进行量化。"><br><strong>Quantization function</strong><br>我们在图2中展示了量化后的Transformer模型。与（Dettmers等，2022；Xiao等，2022）的研究发现一致，我们也观察到了大型语言模型（LLM）的权重和激活中存在显著的离群值。这些离群值对量化过程有显著影响，因为它们会增加量化步长，同时降低中间值的精度。然而，在量化过程中裁剪这些离群值会对LLM性能造成不利影响。在训练的初期，任何基于裁剪的方法都会导致异常高的困惑度分数（即，&gt; 10000），从而造成信息的大量丢失，难以通过微调恢复。因此，我们选择保留这些离群值。此外，我们发现，在带有门控线性单元（GLU）的模型中，激活值和权重大多呈对称分布。基于我们的分析和经验观察，我们选择对权重和激活值使用对称的MinMax量化：</p><script type="math/tex; mode=display">X_Q^i = \alpha \lfloor \frac{X_R^i}{\alpha} \rceil , \ \ \alpha = \frac{max(|X_R|)}{2^{N-1}-1} \tag{3}</script><p>这里，$(X_Q)$表示量化后的权重或激活值，$(X_R)$表示实值权重或激活值。为了确保高效的量化，我们采用<strong>如图3(a)</strong>所示的每个令牌激活量化和每个通道权重量化。对于不同量化器选择的全面评估，请参见第3.3.2节中的消融研究。</p><p><img src="/2024/07/09/papers-paper01-LLM-QAT/LLM-QAT03.png" alt="图3:(a)每个通道的权重量化和每个令牌的激活量化；(b)用于KV缓存的每个令牌量化。KV缓存通过附加当前的键和值来更新。因此，我们对键和值都采用每个令牌量化。"></p><p><strong>键值缓存的量化感知训练</strong><br>除了权重和激活量化外，大型语言模型（LLM）中的键值缓存（KV缓存）也消耗了不可忽视的内存。然而，只有少数以往的工作关注了LLM中的KV缓存量化，这些方法主要限于训练后量化（Sheng等，2023）。在我们的研究中，我们展示了一种类似于激活量化的量化感知训练方法，可以用于量化KV缓存。<strong>如图3所示</strong>，由于键和值是逐个令牌生成的，我们在公式3中采用了每个令牌的量化。在生成过程中，当前的键和值被量化，并且它们的相应缩放因子被存储。在QAT的训练过程中，我们对键和值的整个激活张量应用量化，如图2所示。通过将量化函数整合到梯度计算中，我们确保了使用量化后的键值对进行有效的训练。</p><p><strong>知识蒸馏</strong><br>我们使用基于交叉熵的logits蒸馏方法，从全精度预训练教师网络 训练量化后的学生网络：</p><script type="math/tex; mode=display">L_{CE} = - \frac{1}{n} \sum_c \sum_{i=1}^n p_c^{\tau}(X_i) \log (p_c^S(X_i)) \tag{4}</script><p>其中，$i$表示当前批次中的第$i$个样本，总共有$n$个句子。$c$表示类别数，在我们的情况下，它等于词汇表的大小。$\tau$和$S$分别是教师网络和学生网络。</p><p>正如第2.1节所讨论的，在数据生成过程中，从分布中采样下一个令牌比总是选择前1名候选者更为重要。通过这样做，下一个令牌不一定代表训练学生模型的最佳标签，因为采样引入了固有的噪声。因此，我们建议使用预训练模型的预测作为软标签，这为指导学生模型的训练提供了更具信息量的目标。我们在第3.3.3节中提出了详细的消融研究，以深入探讨这一方法的具体细节。</p><h3 id="3-Experiments-表格数据看论文"><a href="#3-Experiments-表格数据看论文" class="headerlink" title="3 Experiments (表格数据看论文)"></a>3 Experiments (表格数据看论文)</h3><p>我们通过在LLaMA-7B/13B/30B模型上进行实验并在各种任务上展示结果来评估我们方法的有效性。具体而言，我们报告了在常识推理任务上的零样本性能，如BoolQ（Clark等，2019）、PIQA（Bisk等，2020）、SIQA（Sap等，2019）、HellaSwag（Zellers等，2019）、WinoGrande（Sakaguchi等，2021）、ARC（Clark等，2018）和OBQA（Mihaylov等，2018）。我们还评估了在TriviaQA（Joshi等，2017）和MMLU（Hendrycks等，2020）数据集上的少样本性能，以及在WikiText2（Merity等，2016）和C4（Raffel等，2020）数据集上的困惑度评分。</p><h4 id="3-1-Experimental-Settings"><a href="#3-1-Experimental-Settings" class="headerlink" title="3.1 Experimental Settings"></a>3.1 Experimental Settings</h4><p>在我们的量化网络训练过程中，我们用预训练模型初始化模型，并将其用作知识蒸馏的教师模型。为了优化模型，我们使用<u>AdamW（Loshchilov &amp; Hutter, 2017）优化器</u>，并设定权重衰减为零。每个GPU分配的批量大小为1，学习率设置为2e-5，采用<u>余弦学习率衰减策略(cosine learning-rate decay strategy)</u>。对于数据生成，我们使用LLaMA-7B模型，生成序列的最大长度设置为1024。</p><h4 id="3-2-Main-Results"><a href="#3-2-Main-Results" class="headerlink" title="3.2 Main Results"></a>3.2 Main Results</h4><p>我们考虑了三种训练后量化（PTQ）方法，分别是round-to-nearest（RTN）、GPT-Q（Frantar等，2022）和SmoothQuant（Xiao等，2022）作为基线。在不同的设置下比较它们，其中权重、激活值和KV缓存值被量化到不同的水平（表示为W-A-KV）。不同的PTQ方法在不同的设置下表现良好，我们将我们的方法与每个设置中最佳的PTQ结果进行比较。</p><p>表1、表2和附录中的表7分别给出了在常识推理任务的零样本任务、Wiki2和C4的困惑度评估以及MMLU和TriviaQA基准测试中的少样本精确匹配上，我们提出的QAT方法与SOTA PTQ方法的比较。困惑度评估验证了量化模型是否能够在其训练域的多样本上保留模型的输出分布。零样本和少样本评估衡量模型在下游任务中的能力是否得以保留。</p><p>每个表中的趋势类似。所有方法在8位设置下在所有模型大小中表现良好。这甚至在KV缓存与权重和激活值一起被量化到8位时也是如此。然而，当这些三个值中的任何一个被量化到低于8位时，PTQ方法会导致精度损失，而LLM-QAT表现得更好。例如，在8-8-4设置中，30B LLM-QAT实现了69.7的平均零样本准确率，而SmoothQuant为50.7（表1，第68-69行）。在4-8-8设置中，差异较小，但LLM-QAT仍比最佳的PTQ方法（在这种情况下为RTN）高1.4点（第55, 57行）。在4-8-4设置中，权重和KV缓存都被量化到4位时，所有PTQ方法产生的结果都较差，而LLM-QAT实现了69.9，仅比全精度模型低1.5点。LLM-QAT在6位激活量化中也表现得相当好。虽然由于缺乏硬件支持，这种设置目前可能不实用，但它是LLM子8位计算的一个有前景的数据点。不幸的是，4位激活量化在我们尝试的设置中效果不佳（见第3.4节）。</p><p>对从业者来说，一个重要的问题是是否使用全精度的小模型，还是使用具有类似推理成本的更大量化模型。虽然确切的权衡可能因几个因素而有所不同，但我们可以根据我们的结果提出几项建议。首先，8位量化应优于较小的全精度模型，对于这种情况，PTQ方法已足够。一个8-8-8 30B量化模型优于一个具有类似大小的13B模型，实际上应具有更低的延迟和更高的吞吐量。这同样适用于8位13B模型与16位7B模型的比较。此外，使用LLM-QAT量化的4位模型应优于具有类似大小的8位模型。例如，一个4-8-4 LLM-QAT 30B优于一个8位LLaMA-13B，而一个4-8-8 LLM-QAT 13B优于一个8位LLaMA-7B。因此，我们推荐4位LLM-QAT模型以获得最佳的效率-精度权衡。</p><h4 id="3-3-Ablation"><a href="#3-3-Ablation" class="headerlink" title="3.3 Ablation"></a>3.3 Ablation</h4><p>我们在第3.3.1节、第3.3.2节和第3.3.3节分别进行关于数据选择、量化方法和知识蒸馏方法的消融研究。我们报告了WikiText2（Merity等，2016）/C4（Raffel等，2020）数据集上的困惑度评分以及零样本常识推理任务的性能。</p><h5 id="3-3-1-Data-Choice"><a href="#3-3-1-Data-Choice" class="headerlink" title="3.3.1 Data Choice"></a>3.3.1 Data Choice</h5><p>在表3中，我们观察到由维基百科文本构建的WikiText（Merity等，2016）并未涵盖预训练期间使用的所有信息。因此，仅在WikiText上微调的模型倾向于在该特定数据集上过拟合，并且难以很好地泛化到其他数据集。另一方面，Crawled Corpus（C4）数据集（Raffel等，2020）包括从网络收集的数百GB的干净英文文本。在C4上微调模型，在WikiText数据集上评估时获得了合理的迁移准确性。然而，当进行零样本推理任务时，其准确性表现较差。</p><p>与现有数据相比，在生成数据上微调的模型表现出更优越的泛化能力，特别是在零样本任务中。此外，通过从分布中采样生成的数据相比不采样生成的数据展示出更大的多样性。这种增强的多样性在所有任务中显著提高了性能。</p><h5 id="3-3-2-Quantization-Function"><a href="#3-3-2-Quantization-Function" class="headerlink" title="3.3.2 Quantization Function"></a>3.3.2 Quantization Function</h5><p>我们在表4中比较了无裁剪量化方法与基于裁剪的方法。按照之前工作的实践（Liu等，2022b，2023），我们使用StatsQ（Liu等，2022a），这是一个统计计算的缩放因子用于基于裁剪的权重量化，以及LSQ（Esser等，2019），一个用于基于裁剪的激活量化的可学习缩放因子。然而，我们的发现表明，这两种最先进的基于裁剪的量化方法并未超越MinMax非裁剪方法所取得的性能。这一观察结果加强了保留离群值对大型语言模型性能至关重要的论点。</p><p>此外，我们观察到对于LLaMA模型，激活值和权重主要表现出对称分布，这使得使用对称量化器成为最佳选择。然而，重要的是要注意，这一结论可能不适用于其他大型语言模型，特别是那些包含GeLU层的模型。</p><h5 id="3-3-3-Knoledge-Distillation"><a href="#3-3-3-Knoledge-Distillation" class="headerlink" title="3.3.3 Knoledge Distillation"></a>3.3.3 Knoledge Distillation</h5><p>表5显示，不同的知识蒸馏方法对微调模型的最终准确性有显著影响。值得注意的是，仅使用下一个令牌作为标签是次优的，因为在生成过程中从候选分布中采样引入了固有的随机性和噪声。相比之下，使用教师模型的完整logit分布预测的logit蒸馏，比基于标签的训练方法导致微调模型的性能更优。有趣的是，我们观察到，加入注意力蒸馏或隐藏层蒸馏实际上会降低性能。因此，我们在所有实验中都仅使用logit蒸馏。</p><h4 id="3-4-Compatibility-with-SmoothQuant"><a href="#3-4-Compatibility-with-SmoothQuant" class="headerlink" title="3.4 Compatibility with SmoothQuant"></a>3.4 Compatibility with SmoothQuant</h4><p>我们的方法也兼容SmoothQuant（Xiao等，2022）提出的权重激活重缩放技术。表6显示，将SmoothQuant整合到4位权重4位激活（W4A4）量化中可以进一步提高准确性。然而，在激活位数大于权重位数的情况下（即W4A8），添加SmoothQuant并不会带来任何改进，甚至可能会损害性能。</p><h3 id="4-Related-Works"><a href="#4-Related-Works" class="headerlink" title="4 Related Works"></a>4 Related Works</h3><p><strong>量化</strong><br>神经网络量化已被证明是压缩模型大小和减少存储消耗的有效工具。经典的量化方法，如MinMax量化（Jacob等，2018；Krishnamoorthi，2018）、学习步长量化（Esser等，2019）、PACT（Choi等，2018）、N2UQ（Liu等，2022a）等，主要是为卷积神经网络开发的。虽然一些近期工作探索了语言模型的压缩，但它们主要集中在较小的模型上（Zafrir等，2019；Fan等，2020；Shen等，2020b；Zadeh等，2020；Bai等，2021；Qin等，2021；Liu等，2022b），如BERT（Devlin等，2019）或BART（Lewis等，2019）。对于大型语言模型（LLM），现有的量化方法主要限于训练后量化（Xiao等，2022；Yao等，2022；Frantar等，2022），这是由于缺乏可获得的训练数据或在整个预训练数据集上微调所需的资源过于庞大。据我们所知，之前的工作中没有研究LLM的量化感知训练的具体挑战。</p><p><strong>数据生成</strong><br>QAT的数据生成仍然是一个相对未被充分研究的领域。尽管在视觉领域有一些工作使用预训练教师模型生成的数据微调学生网络（Yin等，2020；Liu等，2022c；Cai等，2020），这些方法主要集中在图像数据上。它们涉及基于从标签计算的梯度更新噪声输入，并通过累积这些梯度重建图像。相比之下，我们提出的方法引入了语言领域的下一个令牌数据生成。这种方法更自然，并证明对微调量化语言模型有效。</p><h3 id="5-Conclusion-and-Limitations"><a href="#5-Conclusion-and-Limitations" class="headerlink" title="5 Conclusion and Limitations"></a>5 Conclusion and Limitations</h3><p>我们提出了无需数据的量化感知训练（QAT）方法用于大型语言模型（LLM），并展示了使用该技术可以实现精确的4位量化。鉴于这种对训练数据无关的蒸馏方法的普遍性以及LLM部署成本的不断增加，我们预计该方法将具有广泛的适用性。例如，该方法也可以用于在多个阶段训练的模型，如指令调优或强化学习（Ouyang等，2022）。我们将这一研究留待未来工作进行。由于4位量化没有现成的硬件支持，我们没有将硬件实现包含在本工作中。然而，我们正在与合作伙伴合作，以在不久的将来实现这一点。虽然我们的方法在4位权重、4位KV缓存和8位激活量化方面表现良好，但对于4位激活量化来说仍然不够充分，这一情况需要进一步研究。</p><h2 id="重点和创新"><a href="#重点和创新" class="headerlink" title="重点和创新"></a>重点和创新</h2><p>这篇论文是对large language model LLM的量化研究。使用的QAT quantization-aware training量化感知训练方法。具体的，提出了一种无数据蒸馏方法(data-free distillation method)，该方法通过使用LLM自身生成的数据进行知识蒸馏，可以适用于任何模型。</p><p>并且除了量化权重和激活值之外，还量化了KV缓存。在LLaMA模型上进行了实验，量化最低达4位，还实现了6位量化。</p><p>为什么提出无数据蒸馏方法，因为对LLM进行QAT量化感知训练有几个难点：<br>其一，对LLM选择合适的微调数据很重要。如果微调数据与原始预训练数据的分布不符合，或者微调数据过于狭窄，会导致微调反而损害模型性能；<br>其二，由于LLM预训练的复杂性和规模之大，我们很难完全复制原始的训练设置；<br>其三，LLM有独特的权重和激活分布，它们不同于小型模型，因此适用于小模型的量化裁剪方法可能不适用于LLM。</p><p>基于这几个难点，本论文提出了无数据的蒸馏方法：<br>该方法使用原始预训练模型生成下一个token数据，以此得到QAT数据。使用混合采样，对最开始的3～5个token采用top预测采样，对后面的token采用基于输出概率的随机采样。（具体看2.1节）</p><p>量化方程：<br>线性量化分为MinMax量化和基于裁剪的量化，MinMax量化又分为对称量化和非对称量化。<br>本论文<u>对权重和激活值使用对称的MinMax量化</u>，之所以没有用裁剪量化，是因为LLM的权重和激活值中存在显著的离群值，裁剪这些离群值会对模型性能造成不利影响。在训练初期，任何基于裁剪的方法都会导致大量信息丢失，因此我们保留这些离群值。  </p><p>此外对KVcache的量化，也使用公式3的MinMax对称量化，当前key和value被量化，相应的缩放因子被存储。</p><h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><p>关于LLM量化的基础知识：<br><a href="/2024/07/09/quant/" title="大模型量化技术的原理和代码实现">“大模型量化技术的原理和代码实现”</a></p><p>什么是老师学生模型：</p><p>什么是知识蒸馏：</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>论文：<a href="https://arxiv.org/pdf/2305.17888">LLM-QAT: Data-Free Quantization Aware Training for Large Language Models</a></p>]]></content>
    
    
    <categories>
      
      <category>论文精读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文精读</tag>
      
      <tag>模型量化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>优化器介绍II——动量和自适应学习率</title>
    <link href="/2024/07/08/optimizer02/"/>
    <url>/2024/07/08/optimizer02/</url>
    
    <content type="html"><![CDATA[<h1 id="优化器介绍II——动量-amp-自适应学习率"><a href="#优化器介绍II——动量-amp-自适应学习率" class="headerlink" title="优化器介绍II——动量&amp;自适应学习率"></a>优化器介绍II——动量&amp;自适应学习率</h1><p>在优化器<a href="/2024/07/05/optimizer/" title="优化器介绍I——BGD&#x2F;SGD&#x2F;MBGD">篇章1</a> 中，BGD/SGD/MBGD三种梯度下降方法的学习率是不变了，是提前设置好的超参数。这时就面临一个问题，如何设置初始学习率？因为使用不同的batch size时学习率最好有所变化。还有学习率在训练时不能自主调节吗（自适应学习率）？<br>下面介绍的这几种优化器，会使用动态调节的学习率。</p><h2 id="什么是动量"><a href="#什么是动量" class="headerlink" title="什么是动量"></a>什么是动量</h2><h3 id="一阶矩-二阶矩-一阶动量-二阶动量"><a href="#一阶矩-二阶矩-一阶动量-二阶动量" class="headerlink" title="一阶矩/二阶矩/一阶动量/二阶动量"></a>一阶矩/二阶矩/一阶动量/二阶动量</h3><p>首先简单介绍什么是一阶矩/二阶矩，一阶动量/二阶动量。我后面介绍的数学定义都是通俗理解的，为了方便看懂，不是严谨定义。<br>对一个向量$x=[x_1,x_2,..x_n]$，向量x的长度是$1 \times n$，x的均值和方差分别是：</p><script type="math/tex; mode=display">m=\frac{x_1+x_2+...+x_n}{n} \\var = \frac{ (x_1-m)^2 + (x_2 - m)^2 + ...+(x_n-m)^2}{n}</script><p><strong>一阶矩：</strong>就是期望，就是平均值，对向量x的一阶矩就是对x求期望:</p><script type="math/tex; mode=display">E(x) = \frac{x_1 + x_2 + .. + x_n}{n}</script><p><strong>一阶中心距：</strong>就是对每个值$x_i$减去均值m再求期望:</p><script type="math/tex; mode=display">E(x-m) = \frac{(x_1-m) + (x_2-m)+...+(x_n-m)}{n}=0</script><p><strong>二阶矩：</strong>也是二阶非中心矩，对x的平方求期望,也是x的未中心化的方差:</p><script type="math/tex; mode=display">E(x^2) = \frac{x_1^2 + x_2^2 + ... +x_n^2 }{n}</script><p><strong>二阶中心矩：</strong>对变量x和均值m的差的平方求均值,二阶中心矩也叫做方差，它告诉我们一个随机变量x在它的均值附近波动的大小，方差越大波动越大:</p><script type="math/tex; mode=display">E((x-m)^2) = \frac{ (x_1-m)^2 + (x_2-m)^2 +...+(x_n-m)^2}{n} = var(x)</script><p>现在有一个梯度张量$g=[g_1,g_2,..g_T]$, $g_t$是t时刻的梯度向量。<br>一阶动量：过去各个时刻的梯度的线性组合：</p><script type="math/tex; mode=display">\sum_{i=1}^T \alpha_i \times g_i = \alpha_1 g_1 + \alpha_2 g_2 +... + \alpha_T g_T</script><p>二阶动量：过去各个时刻的梯度的平方的线性组合：</p><script type="math/tex; mode=display">\sum_{i=1}^T \alpha_i \times g_i^2 = \alpha_1 g_1^2 + \alpha_2 g_2^2 +... + \alpha_T g_T^2</script><p>梯度的一阶矩：就是梯度g的期望: $E(g)$. 梯度的二阶矩，就是梯度的平方的期望，也是梯度的未中心化方差：$E(g^2)$.  </p><h3 id="鞍点"><a href="#鞍点" class="headerlink" title="鞍点"></a>鞍点</h3><p>鞍点是一个非局部极值点的驻点，简单而言，鞍点在一个切面上是最小值，在一个切面上是最大值，图示如下：<br><img src="/2024/07/08/optimizer02/optimizer0201.png" alt="z=x^2-y^2的鞍点在(0,0)处"></p><p>传统的随机梯度方法没有添加动量信息，为了解决SGD的山谷震荡和鞍点停滞问题，提出在参数更新时不仅仅有梯度，还有动量。假设现在有个重量几乎为0的小球，在山坡上滚下来，假设它没有惯性，只有速度没有加速度，通过山坡的切面来求速度：<br><u>山谷震荡</u>：在山谷中，小球沿着山谷往下滚动，速度是切面梯度，两侧是山壁，则小球会在左右山壁来回碰撞，来回震荡的滚下来；<br><u>鞍点停滞</u>：小球来到鞍点时，因为没有惯性没有加速度，来到鞍点的瞬间切面梯度=0所以速度=0，则小球会在鞍点停下来；<br>上面这个情况显然不符合物理规律 因为没有惯性，如果把小球换成有重量的铁球，他有惯性，则铁球沿着山坡往下滚动时不容易被山壁弹来弹去，并且来到鞍点也会因为惯性继续向前。铁球更容易逃离鞍点找到全局最低点，而没有惯性的小纸球容易被困在鞍点或局部最优点。<br><img src="/2024/07/08/optimizer02/optimizer0202.png" alt="小球滚下的轨迹图，左图不带惯性，右图带惯性"></p><h3 id="添加动量的梯度更新"><a href="#添加动量的梯度更新" class="headerlink" title="添加动量的梯度更新"></a>添加动量的梯度更新</h3><p>上面这个场景形象解释了梯度更新.<br>传统的梯度更新是不带惯性的小纸球从山上滚下：当前时刻t的损失函数的梯度$g_t$=速度，学习率$\alpha$=时间，参数$\theta_{t+1}$=距离，看做一个距离公式:</p><script type="math/tex; mode=display">\theta_{t+1} = \theta_{t} - \alpha \times g_t \\距离_{new} = 距离_{old} - 时间 \times 速度</script><p>添加了动量的是带惯性的铁球，添加了动量momentum的SGD叫做SGD with momentum，相当于添加了惯性. 这项$\beta v_{t-1}$就是动量或者惯性，公式如下：</p><script type="math/tex; mode=display">\theta_{t+1} = \theta_t - v_t, \ v_t = \beta v_{t-1} + \alpha g_t \\距离_{t+1} = 距离_{t} - 速度_t, \ 速度_t = 速度_{t-1} + 时间\times 加速度_{t}</script><p>添加动量的特点：</p><ol><li>下降初期，本来只有$g_t$梯度，现在添加了上一时刻的步伐$v_{t-1}$，$g_t$和$v_{t-1}$的下降方向一致，能加速参数更新。</li><li>下降中后期，在局部最小值来回震荡，此时的$g_t$很小，容易陷入鞍点/局部最小点，加上动量使得更新幅度增大，能跳出陷阱。</li><li>在梯度改变方向时（$v_{t-1}$和$g_t$的方向不一致），动量能减少更新（就像初速度向右，加速度向左，新速度会向右慢慢变小，然后向左加速，不会突然向左运动），抑制震荡。</li><li>总之，动量项能加速参数更新，抑制震荡，加快收敛。</li></ol><h2 id="AdaGrad优化器"><a href="#AdaGrad优化器" class="headerlink" title="AdaGrad优化器"></a>AdaGrad优化器</h2><p>下面看几个常见的添加了动量的优化器。<br>首先是AdaGrad优化器，它有自适应的学习率，能让不同参数有不同学习率。t和t+1是当前时刻和下一时刻，i是模型的第i个参数，$\eta$是初始的学习率，$\frac{\eta}{\sqrt{\sum_{k=0}^t g_{k,i}^2 + \epsilon }}$是自适应学习率：  </p><script type="math/tex; mode=display">\theta_{t+1,i} = \theta_{t,i} - \frac{\eta}{\sqrt{\sum_{k=0}^t g_{k,i}^2 + \epsilon }} g_{t,i}</script><p>其中，这一项$- \frac{1}{\sqrt{\sum_{k=0}^t g_{k,i}^2 + \epsilon }}$也称为正则项，或者约束项regularize，$\epsilon$保证分母不为0。</p><p>AdaGrad优点：</p><ol><li>适合处理稀疏梯度，在数据分布稀疏的场景，对稀疏参数用大的学习率，对非稀疏参数用小lr。历史梯度的平方和$\sum_{k=0}^t g_{k,i}^2$来表示参数的梯度的稀疏性。<ul><li>如果$\sum_{k=0}^t g_{k,i}^2$很小,表示这个参数$\theta_i$稀疏不频繁出现，被更新的频率低，故用大步长更新它。</li><li>反之，如果$\theta_i$的$\sum_{k=0}^t g_{k,i}^2$大,表示其出现频繁，更新频繁，就用小步长更新它。</li></ul></li><li>分母的求和实现了退火过程，随着时间推移，lr越来越小。保证算法的收敛。</li></ol><p>缺点：</p><ol><li>需要事先人工设置一个全局学习率$\eta$；</li><li>如果设置$\eta$过大，会让正则项很敏感，对梯度调节很大。</li><li>中后期，由于正则项的分母对梯度的累加，让梯度趋于0 训练提前结束。</li></ol><h2 id="RMSProp优化器"><a href="#RMSProp优化器" class="headerlink" title="RMSProp优化器"></a>RMSProp优化器</h2><p>全称Root Mean Square Propagation，公式如下，其中$\eta$是初始化学习率：</p><script type="math/tex; mode=display">\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{v_t}+\epsilon} g_t \\v_t = \gamma v_{t-1} + (1-\gamma)g_t^2</script><p>这个优化器为什么叫RMS，均方根？均方根就是变量x的平方的均值再开方。<br>我们注意到AdaGrad的约束项的分母是梯度的平方和开根号，而RMSProp的约束项的分母$- \frac{1}{\sqrt{v_t}+\epsilon}$，展开看看：</p><script type="math/tex; mode=display">\begin{align}v_t &= \gamma v_{t-1} + (1-\gamma)g_t^2 \\&= \gamma (\gamma v_{t-2} + (1-\gamma) g_{t-1}^2) + (1-\gamma)g_t^2 \\&= ... \\&= (1-\gamma) (g_t^2 + \gamma g_{t-1}^2 + \gamma^2 g_{t-2}^2 +...+\gamma^{t-1} g_1^2 + \gamma ^t)\end{align}</script><p>故RMSProp的约束项的分母实际上是梯度平方的<u>指数移动平均数</u>的开根号。故这个优化器叫做RMS。（这里指数移动平均数，就是指数加权平均，就是指数衰减平均）。</p><p>RMSProp的优点：  </p><ul><li>克服了AdaGrad的梯度急剧下降的问题，有优秀的学习率自适应能力。</li><li>在不稳定的目标函数下，表现优良。</li></ul><h2 id="Adadelta优化器"><a href="#Adadelta优化器" class="headerlink" title="Adadelta优化器"></a>Adadelta优化器</h2><p>Adadelta和RMSProp都在约束项的分母用了梯度平方的指数移动平均数$RMS[g]_t$，不过Adadelta的全局lr不需要自己指定为$\eta$，是更新量的平方的指数加权平均数$RMS[\Delta \theta]_{t-1}$。<br>RMSprop和Adadelta都是为了解决Adagrad的lr急剧下降的问题。</p><script type="math/tex; mode=display">\theta_{t+1} = \theta_t - \frac{RMS[\Delta \theta]_{t-1}}{RMS[g]_t}g_t</script><h2 id="Adam优化器"><a href="#Adam优化器" class="headerlink" title="Adam优化器"></a>Adam优化器</h2><p>这个Adam优化器在目前很多新的通用大语言模型中很常用，很重要。<br>Adam是在RMSProp的基础上，加上了bias-correction，还加上了分子的一阶动量。前面几个优化器只在分母加上了梯度的梯度的二阶动量，下面是Adam的官方算法：<br><img src="/2024/07/08/optimizer02/optimizer0203.png" alt="Adam官方算法实现过程"><br>注意，所有向量操作是element-wise的，比如对梯度的平方，是分别对每个元素平方。  </p><p>分子是一阶动量，也就是梯度$g$的线性组合，$g$的指数移动平均数, $\beta_{1}=0.9$:</p><script type="math/tex; mode=display">m_t=\beta_1 m_{t-1} + (1-\beta_1)g_t</script><p>分母是二阶动量，就是梯度平方的线性组合，是$g$的指数移动平均数,$\beta_{2} = 0.999$:</p><script type="math/tex; mode=display">v_t=\beta_2 v_{t-1} + (1-\beta_2) g_t^2</script><p>得到$m_t$和$v_t$后，需要对其进行偏差纠正，降低偏差对训练初期的影响：</p><script type="math/tex; mode=display">\hat m_t = \frac{m_t}{(1-\beta_1^t)} ,  \ \\hat v_t = \frac{v_t}{1-\beta_2^t}</script><p>最后更新参数，其中默认学习率$\alpha=0.001$，$\epsilon=10^{-8}$避免分母变为0：</p><script type="math/tex; mode=display">\theta_t = \theta_{t} - \frac{\alpha \cdot \hat m_t}{\sqrt{\hat v_t}+\epsilon}</script><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>Adam论文：<a href="https://arxiv.org/pdf/1412.6980">ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION</a></p>]]></content>
    
    
    <categories>
      
      <category>ML基础知识</category>
      
      <category>优化器</category>
      
    </categories>
    
    
    <tags>
      
      <tag>optimizer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch面试题II：梯度更新的代码实现</title>
    <link href="/2024/07/08/pytorch02_GD/"/>
    <url>/2024/07/08/pytorch02_GD/</url>
    
    <content type="html"><![CDATA[<h1 id="pytorch面试题II-梯度更新的代码实现"><a href="#pytorch面试题II-梯度更新的代码实现" class="headerlink" title="pytorch面试题II: 梯度更新的代码实现"></a>pytorch面试题II: 梯度更新的代码实现</h1><h2 id="1-BGD"><a href="#1-BGD" class="headerlink" title="1. BGD"></a>1. BGD</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">batchGradientDescent</span>(<span class="hljs-params">x, y, theta, alpha, m, maxIteration</span>):<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(maxIteration):<br>        hypothesis = np.dot(x, theta)<br>        loss = hypothesis - y<br>        gradient = np.dot(x.transpose(), loss) / m<br>        theta = theta - alpha * gradient              <span class="hljs-comment"># 对所有样本求和</span><br>    <span class="hljs-keyword">return</span> theta<br></code></pre></td></tr></table></figure><h2 id="2-SGD"><a href="#2-SGD" class="headerlink" title="2. SGD"></a>2. SGD</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">StochasticGradientDescent</span>(<span class="hljs-params">x, y, theta, alpha, m, maxIteration</span>):<br>    data = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>        data.append(i)<br>    <span class="hljs-comment"># 这里随便挑选一个进行更新点进行即可（不用想BGD一样全部考虑）</span><br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(maxIteration):<br>        hypothesis = np.dot(x, theta)<br>        loss = hypothesis - y  <span class="hljs-comment"># 这里还是有十个样本</span><br>        index = random.sample(data, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 随机抽取一个样本，得到它的下标</span><br>        gradient = loss[index] * x[index]  <span class="hljs-comment"># 只取一个点进行更新计算</span><br>        theta = theta - alpha * gradient.T<br>    <span class="hljs-keyword">return</span> theta<br></code></pre></td></tr></table></figure><h2 id="3-MBGD"><a href="#3-MBGD" class="headerlink" title="3. MBGD"></a>3. MBGD</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><br></code></pre></td></tr></table></figure><h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><p>(梯度下降的三种形式BGD/SGD/MBGD)[<a href="https://www.cnblogs.com/zongfa/p/9293887.html">https://www.cnblogs.com/zongfa/p/9293887.html</a>]</p>]]></content>
    
    
    <categories>
      
      <category>pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>pytorch</tag>
      
      <tag>coding</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>各种激活函数的介绍</title>
    <link href="/2024/07/05/activation_func/"/>
    <url>/2024/07/05/activation_func/</url>
    
    <content type="html"><![CDATA[<h1 id="常见激活函数的介绍"><a href="#常见激活函数的介绍" class="headerlink" title="常见激活函数的介绍"></a>常见激活函数的介绍</h1><h2 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h2><ol><li>把神经元的输出拉回在一定范围内，</li></ol>]]></content>
    
    
    <categories>
      
      <category>ML基础知识</category>
      
      <category>激活函数</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
      <tag>激活函数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>优化器介绍I——BGD/SGD/MBGD</title>
    <link href="/2024/07/05/optimizer/"/>
    <url>/2024/07/05/optimizer/</url>
    
    <content type="html"><![CDATA[<h1 id="优化器介绍I——BGD-SGD-MBGD"><a href="#优化器介绍I——BGD-SGD-MBGD" class="headerlink" title="优化器介绍I——BGD/SGD/MBGD"></a>优化器介绍I——BGD/SGD/MBGD</h1><p>优化器介绍篇章1，这里介绍了三种梯度更新方法：BGD/SGD/MBGD，并实现公式推导。</p><h2 id="优化器的作用"><a href="#优化器的作用" class="headerlink" title="优化器的作用"></a>优化器的作用</h2><p>优化器的作用是更新模型参数，让损失函数尽可能减小，把参数往正确方向引导，让损失函数不断逼近全局最小值。</p><p>这个优化问题就像下山，损失函数是一座山（真实的损失函数是高维，这里理解为三维），我们要找到全局最小值。当前位置是在当前参数下的损失函数值，下山的方向是损失函数梯度的反方向（梯度是函数上升最快的方向），下山的步长是学习率决定的。</p><p>常见的优化器有：</p><ul><li>BGD: Batch gradient descent</li><li>SGD: Stochastic gradient descent</li><li>MBGD: Mini-batch gradient descent</li><li>使用动量的优化器方法（在<a href="/2024/07/08/optimizer02/" title="优化器介绍II——动量和自适应学习率">优化器篇章2</a>介绍）</li></ul><p>首先区分epoch，batch size，iteration这几个概念：<br>每一个epoch会用到所有训练数据，每一次iteration后更新模型参数，batch size是每次iteration时输入给模型的样本个数。<br>假设现在训练集有1000个样本，会训练epoch=10次，batch size=50，则每个epoch里iteration的次数是$\frac{1000}{50}=20$，因为每次输入50个样本给模型，要把所有数据训练一次，需要输入20次，故一个epoch会更新20次参数。总共会更新$20 \times 10 = 200$次参数。</p><p>梯度更新的公式可以统一这样表示：  </p><script type="math/tex; mode=display">\theta_{t+1} = \theta_{t} - lr \times g_{t}</script><p>其中是在 $ \theta_{t+1}$ 是t+1时刻的参数，$ g_t $ 是t时刻的梯度，lr是学习率。</p><h2 id="BGD-batch-gradient-descent"><a href="#BGD-batch-gradient-descent" class="headerlink" title="BGD batch gradient descent"></a>BGD batch gradient descent</h2><p>BGD方法会让batch size=总样本数，即每次epoch会一次性把所有数据输入给模型，<u>使用所有样本求loss，然后求这些loss的平均值</u>，再反向传播更新参数。<br>也就是说，一个epoch里面只有一次iteration，一个epoch只更新一次参数，但会用到所有样本。<br>优点：能得到全局最优解，很稳定，容易并行实现；<br>缺点：训练很慢，需要很大的显存，因为batch size太大。</p><h2 id="SGD-随机梯度下降"><a href="#SGD-随机梯度下降" class="headerlink" title="SGD 随机梯度下降"></a>SGD 随机梯度下降</h2><p>每一次iteration使用一个样本，即batch size=1，每次epoch更新N次参数，其中N是总样本数。每一个样本更新一次参数，迭代方向很不稳定，不容易收敛，但也不容易陷入局部最优。<br>优点：训练速度快，不容易陷入局部最优；<br>缺点：准确度低，不是每次迭代都朝着整体最优的方向，迭代方向不稳定，不容易并行实现。</p><h2 id="mini-batch-gradient-descent"><a href="#mini-batch-gradient-descent" class="headerlink" title="mini-batch gradient descent"></a>mini-batch gradient descent</h2><p>简称为mini-batch SGD，会设置合适的batch size=n。假设总样本数=N，每次epoch开始前，使用shuffle打乱样本顺序，然后顺序选择n个样本作为一个batch输入给模型。一个epoch有$\frac{N}{n}$次iteration，即一次epoch更新$\frac{N}{n}$次参数，<u>每个iteration计算n个参数的loss，再计算loss的平均</u>，反向求梯度更新参数。</p><p>现在代码中说的SGD一般都是指mini-batch SGD。</p><h2 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h2><p>上面讲的三种方法，BGD使用N个样本点来更新参数，SGD使用1个样本点更新参数，MBGD使用batch size=n个样本点更新参数。这里要思考，使用n个样本点更新参数时，到底是先求这n个loss，求loss的平均值，再计算梯度反向更新；还是先计算每个样本点的loss然后计算梯度，求梯度的平均值来更新参数？</p><p><strong>我认为求loss的平均和求梯度的平均是同一个事情，是等价的</strong>。下面通过公式来说明：（注意下面的公式中$x^i$表示一个向量，表示$x_j$向量中的第j位，是一个值。）  </p><p>假设这个模型是线性函数,其中$\theta_j$是第j个待训练参数，一共有n个待训练参数，即参数向量$\theta$的长度为 $n \times 1$:</p><script type="math/tex; mode=display">H_{\theta}=\sum_{j=0}^{n-1} \theta_j x_j</script><p>下面计算一个样本点的损失，损失函数如下，其中x是输入向量，y是真实向量:</p><script type="math/tex; mode=display">J_{\theta}(x,y) = \frac{1}{2} \times (H_{\theta}(x) - y)^2</script><p>则，如果只使用1个输入样本x来更新参数，参数更新公式如下，其中$\alpha$是学习率，$\theta_j$是第j个参数：</p><script type="math/tex; mode=display">\theta_{j+1}  = \theta_j - \alpha \times \frac{\partial }{\partial \theta_j}J_{\theta}(x,y)</script><p>下面求损失函数$J_{\theta}$相对于的$\theta_j$这个参数的偏导数，注意一共有n个待训练参数，$x_j$表示向量x的第j个位：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial }{\partial \theta_j}J_{\theta}(x,y)  &= \frac{\partial }{\partial \theta_j} \frac{1}{2}(H_{\theta}(x)-y)^2  = (H_{\theta}(x)-y) \times \frac{\partial }{\partial \theta_j}(H_{\theta}(x)-y) \\ &= (H_{\theta}(x)-y) \times \frac{\partial }{\partial \theta_j} (\sum_{i=0}^{n-1} \theta_i x_i - y)  = (H_{\theta}(x)-y) \cdot x_j\end{aligned}</script><p>假设batch size=m，先计算m个loss的平均，再计算梯度更新如下（其中$x^i$表示第i个输入向量）：</p><script type="math/tex; mode=display"> J_{mean}= \frac{1}{m} \sum_{i=1}^m J_{\theta}(x^i,y^i) = \frac{1}{2m} \times \sum_{i=1}^m (H_{\theta}(x^i) - y^i)^2 \\\theta_{j+1}  = \theta_j - \alpha \times \frac{\partial }{\partial \theta_j}J_{mean} = \theta_j - \alpha \cdot \frac{1}{m} \sum_{i=1}^m (H_{\theta}(x^i)-y^i) \cdot x_j</script><p>先计算m个样本对应的梯度，再计算梯度平均，再更新参数，如下：</p><script type="math/tex; mode=display">\frac{1}{m} \cdot \sum_{i=1}^m \frac{\partial }{\partial \theta_j}J_{\theta}(x^i,y) = \frac{1}{m} \sum_{i=1}^m (H_{\theta}(x^i)-y^i) \cdot x_j</script><p><strong>所以先计算loss的平均然后梯度更新，和计算梯度的平均来更新，两个概念是一样的。</strong></p><p>手动实现BGD/SGD/MBGD三种梯度更新的代码：<br><a href="/2024/07/08/pytorch02_GD/" title="pytorch面试题II：梯度更新的代码实现">pytorch面试题II：梯度更新的代码实现</a></p><h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><p>(梯度下降的三种形式BGD/SGD/MBGD)[<a href="https://www.cnblogs.com/zongfa/p/9293887.html">https://www.cnblogs.com/zongfa/p/9293887.html</a>]</p>]]></content>
    
    
    <categories>
      
      <category>ML基础知识</category>
      
      <category>优化器</category>
      
    </categories>
    
    
    <tags>
      
      <tag>optimizer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>详细理解attention的原理</title>
    <link href="/2024/07/05/attention/"/>
    <url>/2024/07/05/attention/</url>
    
    <content type="html"><![CDATA[<h1 id="详细理解attention的原理"><a href="#详细理解attention的原理" class="headerlink" title="详细理解attention的原理"></a>详细理解attention的原理</h1><p>transformer中最重要的就是attention block。这里会详细理解各种类型的attention block：</p><ol><li>注意力机制attention</li><li>自注意力机制 self-attention</li><li>多头注意力机制 multi-head attention</li><li>掩码注意力机制 masked multi-head attention</li><li>交叉注意力机制 cross attention</li></ol><h2 id="1-Attention基本原理"><a href="#1-Attention基本原理" class="headerlink" title="1.Attention基本原理"></a>1.Attention基本原理</h2><p>transformer中的基本结构式self-attention，而在理解自注意力机制self-attention之前，需要理解什么是attention注意力机制。</p><p>Attention会有Source和Target两个集合，Source中的元素想像成一系列的<Key,Value>数据对，而Target中有若干Query数据。<br>通过计算Query和各个元素Key的相似性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，得到最终的Attention值。<br>本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。</p><p><strong>Attention分为三个阶段：</strong>  </p><ul><li>阶段1:Query与Key进行相似度计算得到权值</li><li>阶段2:对上一阶段的计算的权重进行归一化</li><li>阶段3:用归一化的权重与Value加权求和，得到Attention值</li></ul><p><strong>Attention和self-attention的区别：</strong><br>Attention的Source和Target是不同的，比如机器翻译任务中，Source是翻以前的英文句子，Target是翻译后的中文句子。而self-attention的Source和Target是同一个。</p><p>Attention计算的是Source对Target的attention，而self-attention是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。</p><p>简单而言，在self-attention中，Q和K和V是同源的，即这三个张量是通过输入张量X经过线性变化（分别乘以矩阵$W^q, W^k, W^v$）得到的。而attention的Q和{K,V}是不同源的。  </p><p>self.attention就是自己和自己做相识度计算，句子中每个词和句子中其他词计算相识度，利用上下文增强目标词的表达。 </p><h2 id="2-Self-Attention基本原理"><a href="#2-Self-Attention基本原理" class="headerlink" title="2.Self-Attention基本原理"></a>2.Self-Attention基本原理</h2><p>self-attention这个机制是在论文‘attention is all you need’中提出的：<br><img src="/2024/07/05/attention/attention05.png?50*" alt="Scaled Dot-Product Attention"></p><p>NLP任务会输入一个句子，首先会通过tokenizer向量化，然后添加位置编码，得到这组向量：${a_1, a_2,…,a_n}$。 self-attention输入的就是这一组向量:${a_1, a_2,…,a_n}$，然后输出的是上下文关联的向量组:${b_1,b_2…}$.</p><p><strong>为什么需要self-attention：</strong><br>一个句子，每个单词的上下文很重要，因为相同的单词在不同的上下文里的意思不同，如何让每个token向量考虑它的上下文？最简单的是使用sliding window，比如输入的句子是’I saw a saw.’, 滑动窗口的size=3时，每相邻的三个单词（为了简单理解，把每个单词看作一个token）输入给FC层，让每个单词可以学习到相邻的三个单词的信息。<br><img src="/2024/07/05/attention/attention01.png" alt="使用sliding window来考虑上下文"><br>但是滑动窗口有局限性: 如果对某个单词想考虑整个句子的上下文，需要把window size调大覆盖整个句子吗？显然不现实，因为每个句子长度不同。而且window很长时，FC层（fully connected layer）的参数很多，难以训练。  </p><p>为了让每个单词可以考虑上下文信息，这里提出了自注意力机制：输入一组向量${a_1, a_2,…,a_n}$，经过self-attention得到一组输出向量${b_1, b_2,…, b_n}$，为了方便理解假设输入的向量个数和输出的向量个数相等。<br>此刻，${b_i}$向量组中每个向量都是和整个句子上下文相关的，然后经过FC层和softmax得到预测概率。</p><p>自注意力机制如何让每个${b_i}$向量都和${a_i}$向量相关呢？具体实现如下：<br><img src="/2024/07/05/attention/attention02.png" alt="得到Q和K，alpha&#39;=softmax(QK^T/scale)"><br>首先每个$a_i$向量和参数矩阵$W^q, W^k$相乘，得到$q_i, k_i$，然后$\alpha_{ij}=q_i*k_j$,经过softmax函数得到$\alpha’$：</p><script type="math/tex; mode=display">\alpha'_{1,i} = \frac{exp(\alpha_{1,i})}{\sum_{j} exp(\alpha_{1,j})}</script><p>然后$\alpha’_{1,i}$和$v_i$相乘得到$b_1$: $b_1=\sum_i \alpha’_{1,i}v_i$.<br><img src="/2024/07/05/attention/attention03.png" alt="alpha&#39;和v_i相乘之和得到b_1"></p><p>上面是把每个token向量的$q_i, k_i, v_i$向量单独看，但其实代码实现是直接把所有$q_i$向量合并看作$Q=[q_1,q_2,..q_n]$, K和V同理也是整个向量组的合并。<br>从整体张量分析，输入的张量$I=[a_1,a_2,..a_n]$,输出张量$O=[b_1,b_2…b_n]$:<br><img src="/2024/07/05/attention/attention04.png?35" alt="从整体张量来看attention的流程"></p><p>上面通过流程图直观理解了attention的实现原理，然后我们就可以理解论文“attention is all you need”中的这个数学公式：</p><script type="math/tex; mode=display">attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V</script><h2 id="3-多头注意力机制-Multi-Head-Attention"><a href="#3-多头注意力机制-Multi-Head-Attention" class="headerlink" title="3.多头注意力机制 Multi-Head Attention"></a>3.多头注意力机制 Multi-Head Attention</h2><p>多头注意力机制，就是多个自注意力机制的结合。<br>直接看公式和流程图：</p><script type="math/tex; mode=display">Q_i=QW_i^Q, K_i=KW_i^K, V_i=VW_i^V, i=1,2,..8  \\head_i=Attention(Q_i,K_i,V_i)  \\MultiHead(Q,K,V)=Concat(head_1, head_2,...,head_8)W^O</script><p><img src="/2024/07/05/attention/attention06.png?50" alt="多头注意力机制的图示"></p><p>之前每个输入向量$a_i$只会得到一组的${q_i,k_i,v_i}$，但是现在假设$n_head=8$，则每个$a_i$会得到8组的${q_{i,j},k_{i,j},v_{i,j}},j=1,2..8$，每一组q k v计算self-attention，然后得到的8组结果拼接起来乘以一个权重$W^O$.</p><h2 id="4-掩码注意力机制"><a href="#4-掩码注意力机制" class="headerlink" title="4.掩码注意力机制"></a>4.掩码注意力机制</h2><p>在decoder中会使用到掩码注意力机制。在上面的self-attention中，生成的$b_1$向量会考虑所有输入向量$a_i$，生成$b_2$也会考虑所有的$a_i$。<br>但是masked之后，生成$b_1$只考虑$a_1$, $b_2$只考虑$a_1,a_2$，生成$b_k$只考虑$a_1,a_2..a_k$。因为对decoder而言，是先有a1，再有a2 再有a3，一个一个产生的，不是一次性得到所有的ai。</p><h2 id="5-交叉注意力机制"><a href="#5-交叉注意力机制" class="headerlink" title="5.交叉注意力机制"></a>5.交叉注意力机制</h2><p>交叉注意力机制是混合两种不同嵌入序列的注意机制，两个序列必须具有相同的维度，两个序列可以是不同的模式形态（如：文本、声音、图像），一个序列（decoder序列）作为输入的Q，定义了输出的序列长度，另一个序列（encoder序列）提供输入的K和V。</p><p>Cross-attention的输入来自不同的序列，Self-attention的输入来自同序列，除此之外，基本一致。</p><h2 id="Pytorch代码实现"><a href="#Pytorch代码实现" class="headerlink" title="Pytorch代码实现"></a>Pytorch代码实现</h2><p>上面介绍的各种注意力机制的原理，这里是pytorch实现的，结合这个代码可以更好理解注意力机制如何实现：<br><a href="/2024/07/05/pytorch01_attn/" title="pytorch面试题：实现attention结构">pytorch面试题：实现attention结构</a></p><h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><p>吴恩达机器学习网课：<a href="https://speech.ee.ntu.edu.tw/~hylee/ml/2023-spring.php">https://speech.ee.ntu.edu.tw/~hylee/ml/2023-spring.php</a></p>]]></content>
    
    
    <categories>
      
      <category>ML常见模型</category>
      
      <category>transformer</category>
      
    </categories>
    
    
    <tags>
      
      <tag>transformer</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch面试题：实现attention结构</title>
    <link href="/2024/07/05/pytorch01_attn/"/>
    <url>/2024/07/05/pytorch01_attn/</url>
    
    <content type="html"><![CDATA[<h1 id="pytorch面试题I：transformer中重要模块"><a href="#pytorch面试题I：transformer中重要模块" class="headerlink" title="pytorch面试题I：transformer中重要模块"></a>pytorch面试题I：transformer中重要模块</h1><p>transformer中的attention机制很重要，面试中可能会让你手动实现attention。<br>这里记录了transformer架构会考的重要知识点：  </p><ul><li>pytorch手动搭建ScaledDotProduct Attention；</li><li>pytorch搭建multi-head attention；</li><li>pytorch搭建self-attention；</li><li>基于numpy的位置编码的实现；</li></ul><p>首先import所需的库：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br></code></pre></td></tr></table></figure></p><h2 id="1-单头注意力机制"><a href="#1-单头注意力机制" class="headerlink" title="1.单头注意力机制"></a>1.单头注意力机制</h2><p>使用pytorch实现Scaled Dot Product Attention：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ScaledDotProductAttention</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot; Scaled Dot-Product Attention &quot;&quot;&quot;</span><br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, scale</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br> <br>        self.scale = scale<br>        self.softmax = nn.Softmax(dim=<span class="hljs-number">2</span>)<br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, q, k, v, mask=<span class="hljs-literal">None</span></span>):<br>        u = torch.bmm(q, k.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)) <span class="hljs-comment"># 1.Matmul</span><br>        u = u / self.scale <span class="hljs-comment"># 2.Scale</span><br> <br>        <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            u = u.masked_fill(mask, -np.inf) <span class="hljs-comment"># 3.Mask # mask为1的部分设置为-np.inf</span><br> <br>        attn = self.softmax(u) <span class="hljs-comment"># 4.Softmax</span><br>        output = torch.bmm(attn, v) <span class="hljs-comment"># 5.Output</span><br> <br>        <span class="hljs-keyword">return</span> attn, output<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    n_q, n_k, n_v = <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span><br>    d_q, d_k, d_v = <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span><br>    batch = <span class="hljs-number">4</span><br> <br>    q = torch.randn(batch, n_q, d_q)<br>    k = torch.randn(batch, n_k, d_k)<br>    v = torch.randn(batch, n_v, d_v)<br>    mask = torch.zeros(batch, n_q, n_k).<span class="hljs-built_in">bool</span>()<br> <br>    attention = ScaledDotProductAttention(scale=np.power(d_k, <span class="hljs-number">0.5</span>))<br>    attn, output = attention(q, k, v, mask=mask)<br> <br>    <span class="hljs-built_in">print</span>(attn)<br>    <span class="hljs-built_in">print</span>(output)<br></code></pre></td></tr></table></figure></p><h2 id="2-多头注意力机制"><a href="#2-多头注意力机制" class="headerlink" title="2.多头注意力机制"></a>2.多头注意力机制</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadAttention</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot; Multi-Head Attention &quot;&quot;&quot;</span><br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n_head, d_k_, d_v_, d_k, d_v, d_o</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br> <br>        self.n_head = n_head<br>        self.d_k = d_k<br>        self.d_v = d_v<br> <br>        self.fc_q = nn.Linear(d_k_, n_head * d_k) <span class="hljs-comment"># (in_feature, out_feature)</span><br>        self.fc_k = nn.Linear(d_k_, n_head * d_k)<br>        self.fc_v = nn.Linear(d_v_, n_head * d_v)<br> <br>        self.attention = ScaledDotProductAttention(scale=np.power(d_k, <span class="hljs-number">0.5</span>))<br> <br>        self.fc_o = nn.Linear(n_head * d_v, d_o)<br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, q, k, v, mask=<span class="hljs-literal">None</span></span>):<br> <br>        n_head, d_q, d_k, d_v = self.n_head, self.d_k, self.d_k, self.d_v<br> <br>        batch, n_q, d_q_ = q.size() <span class="hljs-comment"># q size=(batch_size, n_q, n_head*d_k) and d_k==d_q</span><br>        batch, n_k, d_k_ = k.size()<br>        batch, n_v, d_v_ = v.size()<br> <br>        q = self.fc_q(q) <span class="hljs-comment"># 1.单头变多头        </span><br>        k = self.fc_k(k)<br>        v = self.fc_v(v)<br>        q = q.view(batch, n_q, n_head, d_q).permute(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>).contiguous().view(-<span class="hljs-number">1</span>, n_q, d_q)<br>        k = k.view(batch, n_k, n_head, d_k).permute(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>).contiguous().view(-<span class="hljs-number">1</span>, n_k, d_k)<br>        v = v.view(batch, n_v, n_head, d_v).permute(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>).contiguous().view(-<span class="hljs-number">1</span>, n_v, d_v)<br>        <span class="hljs-comment"># q.view重构张量维度</span><br>        <span class="hljs-comment"># torch.permute维度换位, permute(2,0,1,3)-&gt;(n_head, batch, n_q, d_q)</span><br>        <span class="hljs-comment"># contiguous: 返回一个在内存中连续的Tensor</span><br>        <br> <br>        <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            mask = mask.repeat(n_head, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>) <span class="hljs-comment"># -&gt; mask.size()=(batch*n_head,n_q, n_k)</span><br>        attn, output = self.attention(q, k, v, mask=mask) <span class="hljs-comment"># 2.当成单头注意力求输出</span><br> <br>        output = output.view(n_head, batch, n_q, d_v).permute(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>).contiguous().view(batch, n_q, -<span class="hljs-number">1</span>) <span class="hljs-comment"># 3.Concat</span><br>        output = self.fc_o(output) <span class="hljs-comment"># 4.仿射变换得到最终输出</span><br>        <span class="hljs-comment"># permute(1,2,0,3)-&gt; (batch, n_q,n_head,d_v)</span><br> <br>        <span class="hljs-keyword">return</span> attn, output<br> <br> <br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    n_q, n_k, n_v = <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span><br>    d_q_, d_k_, d_v_ = <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span><br>    batch=<span class="hljs-number">4</span><br>    <br>    q = torch.randn(batch, n_q, d_q_)<br>    k = torch.randn(batch, n_k, d_k_)<br>    v = torch.randn(batch, n_v, d_v_)    <br>    mask = torch.zeros(batch, n_q, n_k).<span class="hljs-built_in">bool</span>()<br> <br>    mha = MultiHeadAttention(n_head=<span class="hljs-number">8</span>, d_k_=<span class="hljs-number">128</span>, d_v_=<span class="hljs-number">64</span>, d_k=<span class="hljs-number">256</span>, d_v=<span class="hljs-number">128</span>, d_o=<span class="hljs-number">128</span>)<br>    attn, output = mha(q, k, v, mask=mask)<br> <br>    <span class="hljs-built_in">print</span>(attn.size())<br>    <span class="hljs-built_in">print</span>(output.size())<br></code></pre></td></tr></table></figure><h2 id="3-自注意力机制"><a href="#3-自注意力机制" class="headerlink" title="3.自注意力机制"></a>3.自注意力机制</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SelfAttention</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot; Self-Attention &quot;&quot;&quot;</span><br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n_head, d_k, d_v, d_x, d_o</span>):<br>        self.wq = nn.Parameter(torch.Tensor(d_x, d_k))<br>        self.wk = nn.Parameter(torch.Tensor(d_x, d_k))<br>        self.wv = nn.Parameter(torch.Tensor(d_x, d_v))<br> <br>        self.mha = MultiHeadAttention(n_head=n_head, d_k_=d_k, d_v_=d_v, d_k=d_k, d_v=d_v, d_o=d_o)<br> <br>        self.init_parameters()<br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_parameters</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> self.parameters():<br>            stdv = <span class="hljs-number">1.</span> / np.power(param.size(-<span class="hljs-number">1</span>), <span class="hljs-number">0.5</span>)<br>            param.data.uniform_(-stdv, stdv)<br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, mask=<span class="hljs-literal">None</span></span>):<br>        q = torch.matmul(x, self.wq)   <br>        k = torch.matmul(x, self.wk)<br>        v = torch.matmul(x, self.wv)<br> <br>        attn, output = self.mha(q, k, v, mask=mask)<br> <br>        <span class="hljs-keyword">return</span> attn, output<br> <br> <br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    n_x = <span class="hljs-number">4</span><br>    d_x = <span class="hljs-number">80</span><br> <br>    x = torch.randn(batch, n_x, d_x)<br>    mask = torch.zeros(batch, n_x, n_x).<span class="hljs-built_in">bool</span>()<br> <br>    selfattn = SelfAttention(n_head=<span class="hljs-number">8</span>, d_k=<span class="hljs-number">128</span>, d_v=<span class="hljs-number">64</span>, d_x=<span class="hljs-number">80</span>, d_o=<span class="hljs-number">80</span>)<br>    attn, output = selfattn(x, mask=mask)<br> <br>    <span class="hljs-built_in">print</span>(attn.size())<br>    <span class="hljs-built_in">print</span>(output.size())<br>    <br></code></pre></td></tr></table></figure><h2 id="4-位置编码"><a href="#4-位置编码" class="headerlink" title="4.位置编码"></a>4.位置编码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">positional_encoding</span>(<span class="hljs-params">max_position, d_model, min_freq=<span class="hljs-number">1e-4</span></span>):<br>    position = np.arange(max_position)<br>    freqs = min_freq**(<span class="hljs-number">2</span>*(np.arange(d_model)//<span class="hljs-number">2</span>)/d_model)<br>    pos_enc = position.reshape(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)*freqs.reshape(<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>)<br>    pos_enc[:, ::<span class="hljs-number">2</span>] = np.cos(pos_enc[:, ::<span class="hljs-number">2</span>])<br>    pos_enc[:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>] = np.sin(pos_enc[:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>])<br>    <span class="hljs-keyword">return</span> pos_enc<br>    <br><span class="hljs-comment">### Plotting ####</span><br>d_model = <span class="hljs-number">128</span><br>max_pos = <span class="hljs-number">256</span><br>mat = positional_encoding(max_pos, d_model)<br>plt.pcolormesh(mat, cmap=<span class="hljs-string">&#x27;copper&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Depth&#x27;</span>)<br>plt.xlim((<span class="hljs-number">0</span>, d_model))<br>plt.ylabel(<span class="hljs-string">&#x27;Position&#x27;</span>)<br>plt.title(<span class="hljs-string">&quot;PE matrix heat map&quot;</span>)<br>plt.colorbar()<br>plt.show()<br></code></pre></td></tr></table></figure><h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><p><a href="https://www.cnblogs.com/chuqianyu/p/18048501">https://www.cnblogs.com/chuqianyu/p/18048501</a></p>]]></content>
    
    
    <categories>
      
      <category>pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>pytorch</tag>
      
      <tag>coding</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>位置编码的原理详解</title>
    <link href="/2024/07/05/positional_encoding/"/>
    <url>/2024/07/05/positional_encoding/</url>
    
    <content type="html"><![CDATA[<h1 id="位置编码的原理和细节"><a href="#位置编码的原理和细节" class="headerlink" title="位置编码的原理和细节"></a>位置编码的原理和细节</h1><p>位置编码positional encoding就是在tokens中加入位置信息，因为对文本来说，相同单词在不同位置的含义不同，给每个单词在文本中的位置信息对之后的模型训练很重要。  </p><p>这里介绍的是论文’attention is all you need’中的位置编码方法：<br>假设一个句子，经过分词得到L个token，每个token经过embedding得到长度为 $1\times K$的向量，则该句子向量化得到大小为$L\times K$的张量。<br>位置编码公式如下：<br><img src="/2024/07/05/positional_encoding/positional_encoding01.png?70*" alt="位置编码公式"><br>其中pos是token的index，从0～L-1；2i和2i+1表示该token中某个元素是奇数位还是偶数位，范围是0～K-1。$d_{model}$应该是表示embedding的维度，也就是经过embedding之后的token的长度，这里$d_{model}=K$.<br>简单而言，在这个尺寸为$L\times K$的张量中加入位置编码，pos表示行号，2i或者2i+1表示列号。对某一行，偶数列加入sin函数，奇数列加入cos函数。</p><p>可视化位置编码张量，其中纵轴是上面公式中的pos，横轴是2i或者2i+1。即可以把每一行看作给该token添加的位置编码向量，可以发现每个token对应的位置编码都是不一样的，可以表征token在句子当中的位置。<br><img src="/2024/07/05/positional_encoding/positional_encoding02.png" alt="可视化位置编码张量"></p><p>如何自己用代码实现位置编码也是常见考题，在这里看如何用numpy实现位置编码：<br><a href="/2024/07/05/pytorch01_attn/" title="pytorch面试题：实现attention结构">pytorch面试题：实现位置编码</a></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>这个transformer论文中用了上述位置编码原理：<a href="https://arxiv.org/pdf/1706.03762">attention is all you need论文</a></p><p>参考这个博客理解位置编码和代码实现：<a href="https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3">Medium: Positional Encoding Part I</a></p>]]></content>
    
    
    <categories>
      
      <category>ML常见模型</category>
      
      <category>transformer</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>tokenizer的原理</title>
    <link href="/2024/07/05/tokenizer/"/>
    <url>/2024/07/05/tokenizer/</url>
    
    <content type="html"><![CDATA[<h1 id="tokenizer的原理详解"><a href="#tokenizer的原理详解" class="headerlink" title="tokenizer的原理详解"></a>tokenizer的原理详解</h1><h2 id="相关术语"><a href="#相关术语" class="headerlink" title="相关术语"></a>相关术语</h2><ol><li><p>什么是token：<br>token是文本或序列数据中的最小离散单元。在自然语言处理中，一个token可以是一个单词、一个子词（如字母、音节或子词片段），或一个字符，取决于任务和数据的预处理方式。<br>例如，把单词作为最小离散单元，句子”I love deep learning”可以被拆分成4个单词 tokens：[“I”, “love”, “deep”, “learning”]。  </p></li><li><p>什么是tokenization：<br>tokenization的中文意思是分词，就是把文本划分为一个个tokens的过程。  </p></li><li><p>什么是tokenizer：<br>tokenizer是分词器，或者标记器。是模型中将文本转换为tokens的模块。和tokenization差不多。</p></li><li><p>什么是embedding，中文叫嵌入：<br>embedding就是把token映射到连续的向量空间的方法，得到的向量叫做词向量或者词嵌入。<br>神经网络会将文本序列中的每个token编码为向量表示，以便进行后续的计算和处理。现在除了word embedding，还有sentence embedding。这里我只讲如何实现word embedding。我感觉embedding可以是动词：把token变为向量的这个动作；也可以是名词：得到的向量称为embedding。理解它是什么含义即可。</p></li></ol><h2 id="为什么需要tokenizer"><a href="#为什么需要tokenizer" class="headerlink" title="为什么需要tokenizer"></a>为什么需要tokenizer</h2><p>为了方便，下面的文本默认是英文文本。在很多NLP任务中，输入文本，模型无法直接对string类型的文字处理，需要把它们量化为向量。<br>如何把string变为向量？首先，构建词表vocabulary，词表存储的是所有常见的单元。把输入文本切分为一个个token，每个token在词表中有一个对应的int索引，然后把int类型的索引（或者其one-hot向量）变为 $K \times 1$ 的向量。  </p><p>此时涉及到两个问题：  </p><ol><li>如何构建词表的单元是什么，即分词粒度。是每个单词word，还是每个字母character，或者其他单元？下面介绍三种粒度。</li><li>整型索引如何转为向量？下面介绍一种word2vec方法，用于把token转为向量。</li></ol><h3 id="word-based-tokenizer"><a href="#word-based-tokenizer" class="headerlink" title="word-based tokenizer"></a>word-based tokenizer</h3><p>为了方便理解整个过程，以word为基本单元。看看基于word的tokenizer如何实现。<br>首先，构建词表是英文中常见单词，假设词表总共有32000个常见的word。然后，把句子切分为一个个word，每个单词$word_i$对应一个0～31999范围的索引$word_{idx}$。如何把索引转为向量，最基本是使用one-hot方法，每个索引变为大小为32000*1的向量，对应索引位置为1，其余位置为0.比如$word_{idx}=0$对应的向量是$[1,0,0..,0,0]$.</p><p>基于word的tokenizer有缺陷：</p><ul><li>词表vocabulary很大，参数多，难训练。英文中除了常见单词，还有很多生僻词，导致每个word对应的one-hot向量的size很大，后续把向量输入给模型进行训练时，参数矩阵很大，训练难。  </li><li>容易OOV。OOV=out of vocabulary，即某些token在词表中查询不到，例如某些拼写错误的词。</li></ul><h3 id="character-based-tokenizer"><a href="#character-based-tokenizer" class="headerlink" title="character-based tokenizer"></a>character-based tokenizer</h3><p>除了把word作为token单元，还可以把每个字母作为token单元。这样词表的长度=26，每个字母可以表示为一个长度为26*1的one-hot向量。<br>以字母为单元的缺陷很明显：</p><ul><li>每个字母不具备任何意义，只有组合到一起才有意义，故以每个字母作为一个向量输入给模型，模型不一定能训练出这个句子的含义。这是最致命的，导致char-based tokenizer方法不可取。</li><li>一个句子包含很多字母，这样向量化后的sequence很长。</li><li>但是中文是可以进行character-based tokenizer的，因为每个汉字具有意义。</li></ul><h3 id="subword-based-tokenizer"><a href="#subword-based-tokenizer" class="headerlink" title="subword-based tokenizer"></a>subword-based tokenizer</h3><p>选择subword作为基本单元，频率高的word不拆分，罕见词拆分为小的有意义的subword。<br>对于后缀，会在前面加上一个特殊标记，比如把’tokenization’拆分为’token’和’##ization’，其中’##’标记’ization’是一个后缀。  </p><p>优点：</p><ul><li>词表的size适中，减少了OOV问题，模型可以理解没见过的单词，比如对pretrain这个单词，即使模型没见过，但是有pre和train两个token，模型也能理解pretrain的意思。</li></ul><p><strong>如何用subword算法构建词表：</strong><br>最常见的是BPE方法 byte pair encoding：<br>参考这个： <a href="https://www.cnblogs.com/zjuhaohaoxuexi/p/15929039.html">https://www.cnblogs.com/zjuhaohaoxuexi/p/15929039.html</a></p><p>（这里不讲如何构建词表）</p><h2 id="词嵌入-word-embedding"><a href="#词嵌入-word-embedding" class="headerlink" title="词嵌入 word embedding"></a>词嵌入 word embedding</h2><p>上面讲了三种tokenizer方法，分别是基于word，基于character和基于subword。实际代码中一般使用subword方法。<br>在构建好subword词表，并把句子切分为一个个subword后，如何把每个subword变为一个token向量，并且让意思相近的subword对应的向量也距离更近？词嵌入word embedding就是把subword变为词向量的过程。</p><p>假设词表长度=V，即共有V个subword。则每个subword的one-hot向量长度是 $V\times 1$.<br>假设词向量的size是 $K \times 1$ ，即需要把one-hot的 $V \times 1$ 向量转为长度为 $K \times 1$ 的连续向量。一般的，$K=32000,V=768$左右。可见，词向量大大缩小了向量维度，并且词向量是连续向量，不是稀疏的。<br>其中词向量的每个维度表示不同的含义，比如这768个维度中，维度1表示生物，维度2表示会动的物体，维度3表示颜色等等。</p><p>词嵌入的方法：</p><ul><li>predictive基于预测的方法。代表方法：CBoW，Skip-gram</li><li>count-based基于计数的方法。代表方法：Glove vector</li><li>tasked-based基于任务的方法。</li></ul><p>主要介绍CBoW和skip-gram方法。这两个方法都是word2vec方法的具体实现。word2vec方法如字面意思，就是把word转为vector的方法，下面使用的模型很简单，轻量级网络，包含输入层+隐藏层+输出层。</p><h3 id="CBoW方法"><a href="#CBoW方法" class="headerlink" title="CBoW方法"></a>CBoW方法</h3><p>CBoW全称Continuous bag-of-words，CBoW和Skip-gram算法类似，都是基于局部滑动窗口的，利用了局部的上下文特征local context。<br>CBoW是根据上下文预测当前词。假设中心词是$w_t$, 根据其上下文$w_{t-2},w_{t-1},w_{t+1},w_{t+2}$来预测$w_t$。</p><p>模型很简单，输入向量先经过权重矩阵W，再经过权重矩阵W’，然后softmax得到输出。<br>输入：C个上下文单词的one-hot向量；输出：预测中心词的one-hot向量。<br>损失函数：CE，真实label是中心词的真实one-hot向量，预测label是中心词的$V*1$预测向量。</p><p>先看简单的，假设上下文词的个数是1，只输入一个one-hot向量，输出预测向量。<br>输入的 $ V \times 1 $ 向量先经过一个权重矩阵W，W的大小为 $ V \times K $，得到隐藏层向量 $ K \times 1$，再经过权重矩阵W’,size为 $ K \times V$，得到输出向量 $ V \times 1$，再经过softmax，得到概率向量，然后和中心词的真实one-hot计算CE误差。</p><p>如果上下文词的个数是C，即滑动窗口的大小为C+1，一次性输入C个上下文的one-hot，经过W矩阵，得到C个隐藏向量$\{x_1,x_2,..,x_C \}$，<u>计算他们的求和平均值</u>，得到$x_{mean}=\frac{x_1+x_2+..x_C}{C}$,然后经过W’和softmax得到输出。  </p><p>这样，只要用足够的文本训练集，先把文本切分为subword，然后变为一个个one-hot，从头开始移动滑动窗口，每次根据上下文向量来预测中心词向量，从而训练模型权重矩阵W和W’。</p><p>训练好模型后，如果想要知道某一个subword的词向量，只需要把它的one-hot向量经过W矩阵，得到的 $ K \times 1$ 向量就是该subword的词向量。</p><h3 id="skip-gram方法"><a href="#skip-gram方法" class="headerlink" title="skip-gram方法"></a>skip-gram方法</h3><p>上面的CBoW方法是输入上下文向量，预测中心词向量。<br>skip-gram反过来，输入中心词的one-hot向量，经过W矩阵得到 $ K \times 1$的隐藏向量，再经过W’矩阵和softmax得到预测向量 $w_{pred}$, $w_{pred}$会和$w_{t-2},w_{t-1},w_{t+1},w_{t+2}$分别计算CE loss再求和，然后反向传播更新W和W’。</p><h3 id="CBoW和skip-gram对比"><a href="#CBoW和skip-gram对比" class="headerlink" title="CBoW和skip-gram对比"></a>CBoW和skip-gram对比</h3><p>CBoW是输入多个上下文向量，经过hidden层向量后计算平均，再得到输出预测计算loss；<br>skip-gram是输入一个中心词向量，经过hidden层再得到输出后，与所有上下文向量计算loss，loss求和再更新梯度。或者可以理解为，把C个上下文one-hot合并为一个 $ V \times 1$ 向量，其中有C个元素是1，再和输出概率向量计算loss。<br><img src="/2024/07/05/tokenizer/tokenizer01.png" alt="word2vec之CBoW和skip-gram方法"></p><h3 id="查表look-up"><a href="#查表look-up" class="headerlink" title="查表look up"></a>查表look up</h3><p>训练好word2vec模型后，把one-hot向量跟W矩阵相乘就得到这个单词的词向量，这个过程也叫做查表look up。<br>通常语义相近的词出现在上下文的概率更大，word2vec模型会把两个语义相近的词的词向量训练的更接近。</p><h1 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h1><p>CBoW和skip-gram方法的论文：<a href="https://arxiv.org/pdf/1301.3781">Efficient Estimation of Word Representations in Vector Space</a></p>]]></content>
    
    
    <categories>
      
      <category>ML常见模型</category>
      
      <category>transformer</category>
      
    </categories>
    
    
    <tags>
      
      <tag>transformer</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>浅析transformer的结构</title>
    <link href="/2024/07/05/transformer/"/>
    <url>/2024/07/05/transformer/</url>
    
    <content type="html"><![CDATA[<h1 id="浅析transformer结构"><a href="#浅析transformer结构" class="headerlink" title="浅析transformer结构"></a>浅析transformer结构</h1><p>transformer的结构有点复杂，中间涉及很多重要结构。这里会大致理解每个结构的公式原理，详细的解释会在每个细节帖子中展现。</p><p>transformer是NLP模型很常见的结构。<br>大语言模型的处理流程，通俗理解是这样：<br>假设模型现在已经训练好，预测过程：输入文本，经过模型得到预测的下一个字，然后把这个字和之前内容输入，得到下一个字预测。迭代最终得到预测的回答。</p><h2 id="transformer的结构和原理："><a href="#transformer的结构和原理：" class="headerlink" title="transformer的结构和原理："></a>transformer的结构和原理：</h2><p><img src="/2024/07/05/transformer/transformer01.png#pic_center?50*" alt="transformer结构"></p><p>首先输入一段文本或者句子，经过tokenizer把文本进行tokenization，切分为一个个tokens，然后tokens通过embedding向量化，然后添加位置编码，得到向量。</p><p>向量输入给transformer encoder，得到attention block输出的Q和K给到decoder。  </p><p>然后“Outputs”部分的输入本文经过input embedding+positional encoding得到向量，经过attention模块得到的V和之前ecoder的Q和K一起输入cross attention模块。最终得到的预测向量经过linear+softmax得到预测概率向量，概率最大的token为预测token。</p><p>train时，预测token向量会和真实token向量计算loss，更新模型参数。</p><p>transformer可以分为以下几个模块：  </p><ol><li>tokenizer部分：把文本转为向量</li><li>positional encoding：给向量添加位置编码</li><li>attention模块</li><li>训练和预测的细节</li></ol><h2 id="1-tokenizer"><a href="#1-tokenizer" class="headerlink" title="1. tokenizer"></a>1. tokenizer</h2><p>tokenizer的作用就是把文字转换为向量，能方便输入给模型进一步训练处理。  </p><p>tokenizer的细节看这个博客：<a href="/2024/07/05/tokenizer/" title="tokenizer的原理">点击这里查看tokenizer原理介绍</a></p><h2 id="2-positional-encoding"><a href="#2-positional-encoding" class="headerlink" title="2. positional encoding"></a>2. positional encoding</h2><p>位置编码是给文本序列添加位置信息。因为一段文本，同一个单词在不同位置的含义是不一样的，记录单词在文本中的位置信息很重要。</p><p>位置信息的公式，在“attention is all you need”论文中是这样的：<br><img src="/2024/07/05/transformer/positional_encoding01.png?40*" alt="positional encoding">其中，pos是这个token在序列中的位置，例如class token的pos=0；<br>假设这个序列PE总共有N个token，每个token的长度是K，则序列的形状是N*K，pos的范围是0～N-1；2i和2i+1是每个token向量中的index，范围是0～K-1.<br>通俗的，PE是一个矩阵，有N行，每一行表示一个token向量；(pos, 2i)位置就是第pos个token，在index=2i位置的元素。</p><p>positional encoding的细节看这里：<a href="/2024/07/05/positional_encoding/" title="位置编码的原理详解">位置编码的原理</a></p><h2 id="3-Attention结构"><a href="#3-Attention结构" class="headerlink" title="3.Attention结构"></a>3.Attention结构</h2><h3 id="单头注意力机制"><a href="#单头注意力机制" class="headerlink" title="单头注意力机制"></a>单头注意力机制</h3><p>attention结构是transformer当中最重要的部分。论文中attention的公式是这样的：</p><script type="math/tex; mode=display">Attention(Q,K,V)=softmax(\frac{Q\times K^T}{\sqrt{d_k}}V)</script><p>其中，Q,K,V是向量组${a_1,a_2,…a_n}$乘以矩阵$W^q, W^k, W^v$得到的。</p><h3 id="多头注意力机制："><a href="#多头注意力机制：" class="headerlink" title="多头注意力机制："></a>多头注意力机制：</h3><p>多头注意力机制，会对每个Q K V分别乘以$W_i^q,W_i^k,W_i^v$，得到n_head组的${Q_i,K_i,V_i}$,其中n_head是head的个数，一般为8.<br>公式如下：</p><script type="math/tex; mode=display">Q_i=QW_i^Q, K_i=KW_i^K, V_i=VW_i^V, i=1,2,..8  \\head_i=Attention(Q_i,K_i,V_i)  \\MultiHead(Q,K,V)=Concat(head_1, head_2,...,head_8)W^O</script><h3 id="掩码注意力机制"><a href="#掩码注意力机制" class="headerlink" title="掩码注意力机制"></a>掩码注意力机制</h3><p>输入矩阵X先上面的多头注意力机制一样，得到Q K V矩阵，然后$\frac{QK^T}{\sqrt{d)k}}$会和掩码矩阵相乘，再经过softmax，然后和V相乘。公式如下：</p><script type="math/tex; mode=display">masked\_ attn(Q,K,V)=softmax(masked(\frac{QK^T}{\sqrt{d_k}}))\times V</script><p>掩码矩阵就是$M=[a_{ij}], \quad  a_{ij}=0 \quad if \quad  i &lt; j $.</p><h3 id="交叉注意力机制"><a href="#交叉注意力机制" class="headerlink" title="交叉注意力机制"></a>交叉注意力机制</h3><p>cross attention交叉注意力机制。<br>在transformer的decoder block中，第一个是multi-head attention多头注意力机制，其中Q K V都是Outputs向量得到的，但是第二个multi-head attention的K和V是encoder得到的，Q是decoder得到的。</p><p>通过encoder的输出向量C计算得到K和V，再根据decoder block输出的Z计算出Q，后续Q K V的计算和上面一样。<br>好处是：decoder的每位单词可以利用到encoder所有单词信息。</p><p>上看只是浅析了各种attention的公式，具体的详细介绍各种attention结构原理：<br><a href="/2024/07/05/attention/" title="详细理解attention的原理">详细理解attention的原理</a></p><p>关于attention结构的pytorch面试题，看这里，手动pytorch搭建attention结构：<br><a href="/2024/07/05/pytorch01_attn/" title="pytorch面试题：实现attention结构">pytorch面试题：实现attention结构</a></p><h2 id="残差链接-amp-LN"><a href="#残差链接-amp-LN" class="headerlink" title="残差链接&amp;LN"></a>残差链接&amp;LN</h2><p>transformer结构中有残差链接residual connection，并且使用了Layer Norm。</p><p>激活函数是GELU，优化器是Adam。损失函数是Cross entropy loss交叉熵损失函数。</p><p>machine learning中的各种激活函数：<br><a href="/2024/07/05/activation_func/" title="各种激活函数的介绍">激活函数大全</a></p><p>machine learning中的各种优化器optimizer，及其作用：<br><a href="/2024/07/05/optimizer/" title="优化器介绍I——BGD&#x2F;SGD&#x2F;MBGD">优化器介绍I——BGD&#x2F;SGD&#x2F;MBGD</a>, <a href="/2024/07/08/optimizer02/" title="优化器介绍II——动量和自适应学习率">优化器介绍II——动量&amp;自适应学习率</a></p><h1 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h1><p>transformer论文：Attention Is All You Need<br><a href="https://arxiv.org/pdf/1706.03762">https://arxiv.org/pdf/1706.03762</a></p>]]></content>
    
    
    <categories>
      
      <category>ML常见模型</category>
      
      <category>transformer</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
      <tag>transformer</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
